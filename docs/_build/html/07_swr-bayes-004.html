
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Introduction &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '07_swr-bayes-004';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="07b_alternative-with-features.html" />
    <link rel="prev" title="More on mathematical models: Statistical models" href="06_pseudo-swr-analysis.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    models-of-the-mind-notebooks
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_python-jupyter-overview.html">Models of the Mind, 2024</a></li>



<li class="toctree-l1"><a class="reference internal" href="01_game-of-life.html">Conway’s “Game of Life”</a></li>

<li class="toctree-l1"><a class="reference internal" href="02a_math-probability-matching.html"><font color="red">LAB REPORT</font></a></li>


<li class="toctree-l1"><a class="reference internal" href="02b_math-probability-matching.html">PROBABILITY MATCHING, PART 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_agent-probability-matching.html">PROBABILITY MATCHING, PART 3</a></li>




<li class="toctree-l1"><a class="reference internal" href="04_probability-review.html">Preliminaries: Understanding Logical Symbols in Probability</a></li>








<li class="toctree-l1"><a class="reference internal" href="05_bayes-theorem.html">Introduction to Bayes’ theorem</a></li>








<li class="toctree-l1"><a class="reference internal" href="05_bayes-theorem-rev.html">Introduction to Bayes’ theorem</a></li>








<li class="toctree-l1"><a class="reference internal" href="06_pseudo-swr-analysis.html">More on mathematical models: Statistical models</a></li>


<li class="toctree-l1 current active"><a class="current reference internal" href="#">Introduction</a></li>





<li class="toctree-l1"><a class="reference internal" href="08_trace-swr.html">Implementations of the TRACE model</a></li>


<li class="toctree-l1"><a class="reference internal" href="10_netsci-01.html">Network Science Overview</a></li>








<li class="toctree-l1"><a class="reference internal" href="11_nnets-01-linear-functions-and-bias.html">Neural networks, Part 1: Linear logic functions as networks</a></li>


<li class="toctree-l1"><a class="reference internal" href="12_nnets-02-training-linear-networks.html">Neural networks, Part 2 – training networks</a></li>

<li class="toctree-l1"><a class="reference internal" href="13_nnets-03-learning-xor.html">Neural networks, Part 3 – training networks for non-linearly separable mappings</a></li>



<li class="toctree-l1"><a class="reference internal" href="14_nnets-04-srn-statlearn.html">Neural networks, Part 4</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F07_swr-bayes-004.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/07_swr-bayes-004.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1">Part 1</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap-bayes-theorem">Recap: Bayes Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypotheses">Hypotheses</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2">Part 2</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-bayes-theorem-to-multiple-sources-of-evidence">Extending Bayes’ theorem to multiple sources of evidence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario">Scenario</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probabilities">Prior Probabilities:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihoods-probability-of-evidence-given-the-hypotheses">Likelihoods (probability of evidence given the hypotheses):</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-bayes-theorem">Generalizing Bayes’ Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extension-to-many-sources-of-evidence">Extension to many sources of evidence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3">Part 3</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-to-many-sources-of-non-discrete-evidence">Extending to many sources of non-discrete evidence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-cosine-cosine-similarity">Vector cosine (cosine similarity)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#math-behind-vector-cosine">Math behind vector cosine</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-with-3-vectors">Example with 3 vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-cosine-similarity">Interpreting cosine similarity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing">Visualizing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-vector-cosine-for-phonemes">Applying vector cosine for phonemes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-4">Part 4</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-bayesian-approach-to-evaluating-phoneme-inputs">A Bayesian approach to evaluating phoneme inputs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-update-using-cosine-similarity">Bayesian Update Using Cosine Similarity</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-5">Part 5</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-bayesian-approach-to-spoken-word-recognition">A Bayesian approach to spoken word recognition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#phoneme-inputs">Phoneme inputs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bottom-up-phoneme-probabilities-frequencies-priors-etc">Bottom-up phoneme probabilities: frequencies, priors, etc.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#words">Words</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-word-frequency">Prior Probability (Word Frequency)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-words">Likelihood (words)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-probability-words">Posterior Probability (words)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-import-the-lexicon">Step 1: Import the lexicon</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#look-at-frequency-using-histograms">Look at frequency using histograms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trimming-based-on-frequency">Trimming based on frequency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-aside-phonemes">An aside: phonemes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#phoneme-similarity">Phoneme similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#phonetic-similarity">Phonetic similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-similarity">Context similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-counts-to-similarities">Converting counts to similarities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">Cosine Similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-similarity">Visualizing similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-word-recogntion-simulation">Bayesian word recogntion simulation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-report">Lab report</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenge-questions-optional">Challenge questions – optional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-improvement-from-last-year">An example improvement from last year</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h1>
<p>This is a pretty big notebook. You’ll find the lab questions all at the very end. There are 6 questions, and an additional 3 challenge questions.</p>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="part-1">
<h1>Part 1<a class="headerlink" href="#part-1" title="Link to this heading">#</a></h1>
<section id="recap-bayes-theorem">
<h2>Recap: Bayes Theorem<a class="headerlink" href="#recap-bayes-theorem" title="Link to this heading">#</a></h2>
<p>Bayes’ theorem is a fundamental tool in probability theory and statistics that describes the probability of an event based on prior knowledge of conditions that might be related to the event. It is named after Thomas Bayes, who first developed the foundational approach. The theorem provides the mathematically optimal estimates of probabities of outcomes from data (or hypotheses from evidence, such as ‘has disease’ given ‘positive test’, or ‘phoneme /k/ was spoken’ given some speech input [as evidence]).</p>
<p>The theorem is usually stated as follows:</p>
<div class="math notranslate nohighlight">
\[
P(Hypothesis | Evidence) = \frac{P(Evidence | Hypothesis) \times P(Hypothesis)}{P(Evidence)}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(Hypothesis | Evidence)\)</span> is the <strong>posterior probability</strong>: the probability of the hypothesis being true, given the observed evidence.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(Evidence | Hypothesis)\)</span> is the <strong>likelihood</strong>: the probability of observing the evidence, given that the hypothesis is true.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(Hypothesis)\)</span> is the <strong>prior probability</strong>: the initial probability of the hypothesis, before observing any evidence. When we talked about Bayes’ Theorem in the context of statistical reasoning studies, we called this the <em>base rate</em> (the probability of the outcome, e.g., the probability that someone in a given population is infected with a particular disease). This is appropriate when we are talking about something like the prevalence of a disease. <em>Prior probability</em> is more general.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(Evidence)\)</span> is the <strong>marginal likelihood</strong>: the total probability of observing the evidence under all possible hypotheses.</p></li>
</ul>
<p>Again, the <span class="math notranslate nohighlight">\(Hypothesis\)</span> is the outcome under consideration: has the disease, is the phoneme that was said, is the word that was said, etc.</p>
<p>The <span class="math notranslate nohighlight">\(Evidence\)</span> is the data or input: result of a medical test, the speech input, etc.</p>
</section>
<hr class="docutils" />
<section id="hypotheses">
<h2>Hypotheses<a class="headerlink" href="#hypotheses" title="Link to this heading">#</a></h2>
<p>What are <strong>hypotheses</strong> in the context of Bayes’ Theorem? We talked about them as <em>outcomes</em> when we talked about statistical reasoning in class. For statistical reasoning questions, we might ask a seemingly simple question like ‘what is the probability of having a disease given a positive test for the disease?’ This is a case where we have only 2 possible hypotheses: has the disease or does not have the disease, which we might denote as <span class="math notranslate nohighlight">\(D\)</span> vs. <span class="math notranslate nohighlight">\(\neg{D}\)</span> (D or not D). We also only consider 2 possible evidence states: positive test or negative test. This is a case we can depict with a simple diagram.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># play this cell to see the next image</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;rectangles_diagram01.png&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/321041ea1e8ea375c48e09563663b09605bb88bbcd109395bea4bf20727f5000.png" src="_images/321041ea1e8ea375c48e09563663b09605bb88bbcd109395bea4bf20727f5000.png" />
</div>
</div>
<p>In this diagram, the white rectangle represents the full population. The blue rectangle represents individuals with the disease. The red rectangle represents people who test positive, whether they have the disease or not. The part of the red triangle that overlaps with the blue triangle would be true positives: people who have the disease <strong>and</strong> test positive. The rest of the red rectangle represents false positives: people who do <strong>not</strong> have the disease but test positive. The probability of having the disease given a positive test is the the proportion of the red rectangle that overalaps with the blue rectangle – that is, the proportion of positive tests that are true positives.</p>
<p>Recall that to calculate this with Bayes’ Theorem, we need to know the <em>base rate</em> (likelihood of the hypothesis or outcome, that is, having the disease), the hit rate (probability of a positive test when you have the disease), and the fale alarm rate (probability of having a positive test when you do not have the disease. To calculate it, suppose the probability of having the disease is 0.12, the hit rate is 0.1, and the false alarm rate is 0.15. The numerator in the formula is <span class="math notranslate nohighlight">\({P(Evidence | Hypothesis) \times P(Hypothesis)}\)</span>. This is the hit rate (probability of a positive test if you have the disease) multiplied by the base rate. That would give us an answer corresponding to the portion of the red rectangle that overlaps with the blue rectangle. Note that this is the probability that both <em>having the disease</em> <strong>and</strong> <em>having a positive test</em> are true. The denominator is <span class="math notranslate nohighlight">\(P(Evidence)\)</span>. What is that? That’s the probability of a positive test, which would correspond to the entire red rectangle. The numerator already captures the part overlapping with the blue rectangle, so now we need to figure out how big the rest of the red triangle is.</p>
<p>Well, we figured out how big the overlap part was by multiplying the hit rate by the base rate (represented by the blue rectangle). Now we need to multiply the false alarm rate by the size of the white rectangle that does not overlap with the blue one. Those two cannot overlap – they divide the population into has disease and does not have the disease, so the size of the ‘rest’ of the population without the disease is 1 - base rate, or 0.88 in our example. So we would multiply 0.88 by 0.15 (false alarm rate). That gives us the rest of the red rectangle. As a formula, we would have: <span class="math notranslate nohighlight">\(P(Evidence|\neg{Hypothesis}) \times P(\neg{Hypothesis})\)</span>, where <span class="math notranslate nohighlight">\(\neg\)</span> is the ‘not’ symbol. Note that this is the probability that both <em>not having the disease</em> <strong>and</strong> <em>having a positive test</em> are true. We can add the two results to give us <span class="math notranslate nohighlight">\(P(Evidence)\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(Evidence) = [P(Evidence|Hypothesis) \times P(Hypothesis)] + [P(Evidence|\neg{Hypothesis}) \times P(\neg{Hypothesis})]\]</div>
<p>Do you see a pattern here? For each possible hypothesis (here, <span class="math notranslate nohighlight">\(Hypothesis\)</span> or <span class="math notranslate nohighlight">\(\neg{Hypothesis}\)</span>), we multiply the probability of the evidence given the hypothesis by the probability of the hypothesis.</p>
<p>Let’s consider what would happen with a slightly more complex case: let’s suppose that coughing is also associated with the disease. We can depict this by adding another rectangle to our diagram:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># play this cell to see the next image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s1">&#39;rectangles_diagram02.png&#39;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c269b2d2b197242825bf5800479a269da1dd90685ec2ca8fe26692bf0a4068f0.png" src="_images/c269b2d2b197242825bf5800479a269da1dd90685ec2ca8fe26692bf0a4068f0.png" />
</div>
</div>
<p>The yellow rectangle represents having a cough. Now we can ask how likely it is that a patient has the disease if they are coughing, or, more usefully, if a patient is more likely to have the disease if they are coughing <strong>and</strong> have a positive test.</p>
<p>Now how would we determine the probability that someone has the disease given both sources of evidence? We will tackle this formally in the next section, but for now, consider how you could conceptually use the rectangles to work out a solution. In the previous schematic, remember that the probability of having the disease given a positive test would be related to the ratio of the size of the red rectangle overlapping with the blue rectangle to the total size of the red rectangle. Now with the yellow rectangle, there are 8 possible states (2 x 2 x 2, from 2 outcomes or hypotheses [have disease, no disease] x 2 possibilites with respect to the test [positive, negative] x 2 possibilities with the symptom of coughing [cough, no cough]):</p>
<ol class="arabic simple">
<li><p>Have disease, negative test, no cough (only blue)</p></li>
<li><p>Have disease, positive test, no cough (the portion of the red square that only overlaps with blue)</p></li>
<li><p>Have disease, negative test, have cough (only blue and yellow overlap)</p></li>
<li><p>Have disease, positive test, have cough (blue, red, and yellow overlap)</p></li>
<li><p>No disease, negative test, no cough (white only)</p></li>
<li><p>No disease, positive test, no cough (red only)</p></li>
<li><p>No disease, negative test, have cough (yellow only)</p></li>
<li><p>No disease, positive test, cough (red and yellow)</p></li>
</ol>
<p>What ratios would be useful for evaluating the probability of disease given (a) cough but negative test, (b) no cough but positive test, (c) positive test and has cough?</p>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="part-2">
<h1>Part 2<a class="headerlink" href="#part-2" title="Link to this heading">#</a></h1>
<section id="extending-bayes-theorem-to-multiple-sources-of-evidence">
<h2>Extending Bayes’ theorem to multiple sources of evidence<a class="headerlink" href="#extending-bayes-theorem-to-multiple-sources-of-evidence" title="Link to this heading">#</a></h2>
<p>Let’s apply <strong>Bayes’ theorem</strong> to a case with two hypotheses and two sources of evidence. Let’s also move toward a more general notation, where we will enumerate <span class="math notranslate nohighlight">\(n\)</span> hypotheses 1 to <span class="math notranslate nohighlight">\(n\)</span> as <span class="math notranslate nohighlight">\(H_1\)</span>, <span class="math notranslate nohighlight">\(H_2\)</span>, <span class="math notranslate nohighlight">\(\ldots H_n\)</span>, and <span class="math notranslate nohighlight">\(m\)</span> sources of evidence 1 to <span class="math notranslate nohighlight">\(m\)</span> as <span class="math notranslate nohighlight">\(E_1\)</span>, <span class="math notranslate nohighlight">\(E_2\)</span>, <span class="math notranslate nohighlight">\(\ldots E_m\)</span></p>
<section id="scenario">
<h3>Scenario<a class="headerlink" href="#scenario" title="Link to this heading">#</a></h3>
<p>Let’s say we have two hypotheses (<span class="math notranslate nohighlight">\(H_1\)</span>, <span class="math notranslate nohighlight">\(H_2\)</span>) and two sources of evidence (<span class="math notranslate nohighlight">\(E_1\)</span>, <span class="math notranslate nohighlight">\(E_2\)</span>).</p>
<ol class="arabic simple">
<li><p><strong>Hypotheses</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( H_1 \)</span>: The person has the disease.</p></li>
<li><p><span class="math notranslate nohighlight">\( H_2 \)</span>: The person does not have the disease.</p></li>
</ul>
</li>
<li><p><strong>Evidence</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( E_1 \)</span>: The test result is positive.</p></li>
<li><p><span class="math notranslate nohighlight">\( E_2 \)</span>: The person is coughing.</p></li>
</ul>
</li>
</ol>
<section id="prior-probabilities">
<h4>Prior Probabilities:<a class="headerlink" href="#prior-probabilities" title="Link to this heading">#</a></h4>
<p>In more complex cases, we need to know or estimate these for all hypotheses under consideration. For a binary case, it is straightforward.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P(H_1) \)</span>: The prior probability that the person has the disease.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(H_2) = 1 - P(H_1) \)</span>: The prior probability that the person does not have the disease. (We can subtract <span class="math notranslate nohighlight">\(P(H_1)\)</span> from 1 because there are only 2 possible hypotheses, and their summed probability must equal 1. This prior would be useful if we ask what is the probability of <em>not</em> having the disease given a positive or negative test.)</p></li>
</ul>
</section>
<section id="likelihoods-probability-of-evidence-given-the-hypotheses">
<h4>Likelihoods (probability of evidence given the hypotheses):<a class="headerlink" href="#likelihoods-probability-of-evidence-given-the-hypotheses" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P(E_1 \mid H_1) \)</span>: Probability of a positive test if the person has the disease (true positives).</p></li>
<li><p><span class="math notranslate nohighlight">\( P(E_1 \mid H_2) \)</span>: Probability of a positive test if the person does not have the disease (false positives).</p></li>
<li><p><span class="math notranslate nohighlight">\( P(E_2 \mid H_1) \)</span>: Probability of coughing if the person has the disease (true positives – for coughing).</p></li>
<li><p><span class="math notranslate nohighlight">\( P(E_2 \mid H_2) \)</span>: Probability of coughing if the person does not have the disease (false positives – for coughing).</p></li>
</ul>
<!---
#### Probability of the evidence
The denominator for our conditional probabilities will be $P(E)$, where we sum up the relevant evidence. Conceptually, with Bayes' Theorem, when we are asking what is the conditional probabiity of $H$ given $E$, where $E$ may be the joint probability of 2 or more kinds of evidence, we are asking what 
--->
</section>
</section>
<section id="generalizing-bayes-theorem">
<h3>Generalizing Bayes’ Theorem<a class="headerlink" href="#generalizing-bayes-theorem" title="Link to this heading">#</a></h3>
<p>To compute the probability that a person has the disease given a positive test result and coughing, we again use Bayes’ theorem, extended to 2 sources of evidence. Let’s introduce a new convention, where we will specify “<span class="math notranslate nohighlight">\(E_1, E_2\)</span>” to mean the joint probability of <span class="math notranslate nohighlight">\(E_1\)</span> and <span class="math notranslate nohighlight">\(E_2\)</span>, i.e., the probability of <span class="math notranslate nohighlight">\(E_1\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(E_2\)</span>, i.e., that both are true:</p>
<div class="math notranslate nohighlight">
\[
P(H_1 \mid E_1, E_2) = \frac{P(E_1, E_2 \mid H_1) P(H_1)}{P(E_1, E_2)}
\]</div>
<p>So we read this as “the probability of <span class="math notranslate nohighlight">\(H_1\)</span> <em>given</em> the joint probability of <span class="math notranslate nohighlight">\(E_1\)</span> and <span class="math notranslate nohighlight">\(E_2\)</span> is equal to the joint probability of <span class="math notranslate nohighlight">\(E_1\)</span> and <span class="math notranslate nohighlight">\(E_2\)</span> multiplied by the probablility of <span class="math notranslate nohighlight">\(H_1\)</span> – over the probability of the relevant evidence. The relevant evidence is the joint probability of <span class="math notranslate nohighlight">\(E_1\)</span> <em>and</em> <span class="math notranslate nohighlight">\(E_2\)</span>.</p>
<p>How do we get the values we need for this? First, the numerator, <span class="math notranslate nohighlight">\( P(E_1, E_2 \mid H_1)\)</span>,  is the product of the probabilities of each piece of evidence given the hypothesis:</p>
<div class="math notranslate nohighlight">
\[
P(E_1, E_2 \mid H_1) = P(E_1 \mid H_1) P(E_2 \mid H_1)
\]</div>
<!---
Similarly, the posterior probability for $ H_2 $ (the person does not have the disease) is:

$$
P(H_2 \mid E_1, E_2) = \frac{P(E_1, E_2 \mid H_2) P(H_2)}{P(E_1, E_2)}
$$
--->
<p>The denominator is the total probability of the evidence, calculated as:</p>
<div class="math notranslate nohighlight">
\[
P(E_1, E_2) = P(E_1, E_2 \mid H_1) P(H_1) + P(E_1, E_2 \mid H_2) P(H_2)
\]</div>
</section>
<section id="extension-to-many-sources-of-evidence">
<h3>Extension to many sources of evidence<a class="headerlink" href="#extension-to-many-sources-of-evidence" title="Link to this heading">#</a></h3>
<p>We can extend this to arbitrarily many sources of evidence (<span class="math notranslate nohighlight">\(E_1 \ldots E_n\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the nubmer of evidence sources) with a little rewriting:</p>
<div class="math notranslate nohighlight">
\[
P(H_1 \mid E_1, \ldots E_n) = \frac{P(E_1, \ldots E_n \mid H_1) P(H_1)}{P(E_1, \ldots E_n)}
\]</div>
</section>
</section>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h2>
<p>Let’s consider an example with some specific values:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P(H_1) = 0.01 \)</span> (1% of people have the disease)</p></li>
<li><p><span class="math notranslate nohighlight">\( P(E_1 \mid H_1) = 0.95 \)</span> (95% hit rate)</p></li>
<li><p><span class="math notranslate nohighlight">\( P(E_1 \mid H_2) = 0.05 \)</span> (5% false-positive rate)</p></li>
<li><p><span class="math notranslate nohighlight">\( P(E_2 \mid H_1) = 0.8 \)</span> (80% of people with the disease cough)</p></li>
<li><p><span class="math notranslate nohighlight">\( P(E_2 \mid H_2) = 0.1 \)</span> (10% of people without the disease cough)</p></li>
</ul>
<p>To find the posterior probability that the person has the disease given that they tested positive and are coughing:</p>
<p><strong>1. <strong>Numerator</strong> for <span class="math notranslate nohighlight">\( P(H_1 \mid E_1, E_2) \)</span>:</strong></p>
<div class="math notranslate nohighlight">
\[
P(E_1, E_2 \mid H_1) P(H_1) = (0.95 \times 0.8) \times 0.01 = 0.0076
\]</div>
<p><strong>2. <strong>Denominator</strong>:</strong></p>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\( H_1 \)</span>: <span class="math notranslate nohighlight">\( P(E_1, E_2 \mid H_1) P(H_1) = 0.0076 \)</span></p></li>
<li><p>For <span class="math notranslate nohighlight">\( H_2 \)</span>: <span class="math notranslate nohighlight">\( P(E_1, E_2 \mid H_2) P(H_2) = (0.05 \times 0.1) \times 0.99 = 0.00495 \)</span></p></li>
</ul>
<p>So:</p>
<div class="math notranslate nohighlight">
\[
P(E_1, E_2) = 0.0076 + 0.00495 = 0.01255
\]</div>
<p><strong>3. Posterior Probability:</strong></p>
<div class="math notranslate nohighlight">
\[
P(H_1 \mid E_1, E_2) = \frac{0.0076}{0.01255} \approx 0.605
\]</div>
<p>Thus, there is about a 60.5% chance that the person has the disease given both the positive test result and that they are coughing.</p>
<p>Of course, calculating this by hand would be very tedious. We could write code to do this for arbitrarily many hypotheses and evidence sources. <em>Possibly to be added later.</em></p>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="part-3">
<h1>Part 3<a class="headerlink" href="#part-3" title="Link to this heading">#</a></h1>
<section id="extending-to-many-sources-of-non-discrete-evidence">
<h2>Extending to many sources of non-discrete evidence<a class="headerlink" href="#extending-to-many-sources-of-non-discrete-evidence" title="Link to this heading">#</a></h2>
<p>So far we have discussed cases where we have binary sources of evidence: test is positive or negative, a person has a cough or does not. What if we have many sources of evidence? For example, for speech sounds, linguists developed sets of binary features that characterize phonemes. The phoneme /p/, for example, has the features +stop, +bilabial, -alveolar, -voiced, -sonorant, -continuant (among many others). The phoneme /b/ has all of the same feature values as /p/ <em>except</em> it is +voiced. We could convert the +/- notation to a set of values of 1 (has feature) or 0 (does not have feature). So for the 6 features we have so far, pattern for /p/ would be 110<strong>0</strong>00 while the pattern for /b/ would be 110<strong>1</strong>00. We can see that the two vectors are similar, especially if we compare them to the pattern for /s/: -stop, -bilabial, +alveolar, -voiced, -sonorant, +continuant, or 001001. We can compare vectors by doing something like computing a distance between them. For <strong>Euclidean distance</strong>, we are computing the length of a straight line in <span class="math notranslate nohighlight">\(n-\)</span>dimensional space that goes from the ‘address’ of one vector to the address of another. This is easy to visualize in 2 or 3 dimensions, and the math extends to many dimensions.</p>
<p>Another metric that is often used is <strong>cosine similarity</strong>. Recall that the cosine in trigonometry is, for a right angle, the ratio of the length of the adjacent side to the length of the hypotenuse. When the adjacent side is shorter than the hypotenuse, cosine is less than 1. When the adjacent side is longer than the hypotenuse, the cosine is greater than 1. This provides a tool that’s helpful for simplifying measurements involving angles, with many interesting and powerful extensions in math and physics.</p>
<p>For example, cosine can be extended to vectors, which is what we are interested in. Time for a tangent (no pun intended)…</p>
<section id="vector-cosine-cosine-similarity">
<h3>Vector cosine (cosine similarity)<a class="headerlink" href="#vector-cosine-cosine-similarity" title="Link to this heading">#</a></h3>
<p>First, let’s be precise about what a <strong>vector</strong> is. We can think of it just as a series of numbers, but we can do more interesting things when we think of each element in a vector as a <em>dimension</em>. Then we can think of a vector as having both <strong>direction</strong> and <strong>magnitude</strong> (or length). Imagine an arrow pointing somewhere. The length of the arrow is the magnitude, and the direction it points (relative to its originating point) is, well, the direction.</p>
<p>For example, in 2D space, a vector might look like this: <span class="math notranslate nohighlight">\( \mathbf{v} = (x_1, y_1) \)</span>, where <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(y_1\)</span> are the coordinates. So if <span class="math notranslate nohighlight">\(x_1\)</span> is 3 and <span class="math notranslate nohighlight">\(y_1\)</span> is 4, we move out 3 units on the x axis from 0 and then up 4 units on the y axis from 0, and then we consider the vector to describe the straight line that would go from 0,0 to 3,4. It has an angle relative to the x and y axes (which specifies the direction it points) and a length (which is how far it extends along that angle).</p>
<p>Now, <strong>cosine similarity</strong> is a way to measure how similar two vectors are in terms of their direction (<em>not in terms of length at all</em>). The more two vectors point in the same direction, the closer their cosine similarity is to 1. If they point in opposite directions, the cosine similarity will be closer to -1. If they are at a 90-degree angle (perpendicular), the cosine similarity will be 0.</p>
</section>
<section id="math-behind-vector-cosine">
<h3>Math behind vector cosine<a class="headerlink" href="#math-behind-vector-cosine" title="Link to this heading">#</a></h3>
<p><em>Note: you can skip the math – it’s not crucial. The crucial part is conceptually what vector cosine tells us, which I describe after the math.</em></p>
<p>The cosine of the angle between two vectors, <span class="math notranslate nohighlight">\( \mathbf{v} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{w} \)</span>, is calculated using this formula:</p>
<div class="math notranslate nohighlight">
\[
\cos(\theta) = \frac{\mathbf{v} \cdot \mathbf{w}}{\|\mathbf{v}\| \|\mathbf{w}\|}
\]</div>
<p>Here’s what this means:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( \mathbf{v} \cdot \mathbf{w} \)</span> is the <strong>dot product</strong> of the two vectors. This is a special way to multiply vectors, where you multiply their corresponding parts and add them up. If <span class="math notranslate nohighlight">\( \mathbf{v} = (x_1, y_1) \)</span> and <span class="math notranslate nohighlight">\( \mathbf{w} = (x_2, y_2) \)</span>, the dot product is:</p>
<div class="math notranslate nohighlight">
\[
  \mathbf{v} \cdot \mathbf{w} = x_1 \cdot x_2 + y_1 \cdot y_2
  \]</div>
</li>
<li><p><span class="math notranslate nohighlight">\( \|\mathbf{v}\| \)</span> and <span class="math notranslate nohighlight">\( \|\mathbf{w}\| \)</span> are the <strong>magnitudes</strong> (or lengths) of the vectors. To find the magnitude of a vector <span class="math notranslate nohighlight">\( \mathbf{v} = (x_1, y_1) \)</span>, we use:</p>
<div class="math notranslate nohighlight">
\[
  \|\mathbf{v}\| = \sqrt{x_1^2 + y_1^2}
  \]</div>
</li>
</ul>
<p>So, the formula for cosine similarity gives you a value between -1 and 1 that tells you how much the two vectors are pointing in the same direction.</p>
</section>
<section id="example-with-3-vectors">
<h3>Example with 3 vectors<a class="headerlink" href="#example-with-3-vectors" title="Link to this heading">#</a></h3>
<p>Let’s say you have 3 vectors:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v} = (1, 2), \quad \mathbf{w} = (2, 4), \quad \mathbf{u} = (3, 1)
\]</div>
<p><strong>1. Cosine Similarity Between <span class="math notranslate nohighlight">\( \mathbf{v} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{w} \)</span></strong></p>
<p>First, we find the dot product of <span class="math notranslate nohighlight">\( \mathbf{v} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{w} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v} \cdot \mathbf{w} = 1 \cdot 2 + 2 \cdot 4 = 2 + 8 = 10
\]</div>
<p>Next, we find the magnitudes of each vector:</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{v}\| = \sqrt{1^2 + 2^2} = \sqrt{1 + 4} = \sqrt{5}
\]</div>
<div class="math notranslate nohighlight">
\[
\|\mathbf{w}\| = \sqrt{2^2 + 4^2} = \sqrt{4 + 16} = \sqrt{20}
\]</div>
<p>Now we can calculate the cosine similarity:</p>
<div class="math notranslate nohighlight">
\[
\cos(\theta_{\mathbf{v}, \mathbf{w}}) = \frac{10}{\sqrt{5} \times \sqrt{20}} = \frac{10}{\sqrt{100}} = \frac{10}{10} = 1
\]</div>
<p>Since the cosine is 1, these two vectors are pointing in exactly the same direction (we’ll plot them below).</p>
<p><strong>2. Cosine Similarity Between <span class="math notranslate nohighlight">\( \mathbf{v} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{u} \)</span></strong></p>
<p>Now, let’s calculate the cosine similarity between <span class="math notranslate nohighlight">\( \mathbf{v} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{u} \)</span>.</p>
<p>First, we compute the dot product:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v} \cdot \mathbf{u} = 1 \cdot 3 + 2 \cdot 1 = 3 + 2 = 5
\]</div>
<p>Next, we find the magnitude of <span class="math notranslate nohighlight">\( \mathbf{u} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{u}\| = \sqrt{3^2 + 1^2} = \sqrt{9 + 1} = \sqrt{10}
\]</div>
<p>Now, we calculate the cosine similarity, using the magnitude of <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> from before:</p>
<div class="math notranslate nohighlight">
\[
\cos(\theta_{\mathbf{v}, \mathbf{u}}) = \frac{5}{\sqrt{5} \times \sqrt{10}} = \frac{5}{\sqrt{50}} = \frac{5}{7.07} \approx 0.707
\]</div>
<p>Since the cosine is less than 1, the vectors <span class="math notranslate nohighlight">\( \mathbf{v} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{u} \)</span> are not pointing in the same direction but are not extremely different.</p>
<p><strong>3. Cosine Similarity Between <span class="math notranslate nohighlight">\( \mathbf{w} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{u} \)</span></strong></p>
<p>Last, let’s calculate the cosine similarity between <span class="math notranslate nohighlight">\( \mathbf{w} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{u} \)</span>. Can you predict what the outcome will be?</p>
<p>First, we compute the dot product:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w} \cdot \mathbf{u} = 2 \cdot 3 + 4 \cdot 1 = 6 + 4 = 10
\]</div>
<p>Now, we calculate the cosine similarity:</p>
<div class="math notranslate nohighlight">
\[
\cos(\theta_{\mathbf{w}, \mathbf{u}}) = \frac{10}{\sqrt{20} \times \sqrt{10}} = \frac{10}{\sqrt{200}} = \frac{10}{14.14} \approx 0.707
\]</div>
<p>This is the same result as between <span class="math notranslate nohighlight">\( \mathbf{v} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{u} \)</span>, indicating that <span class="math notranslate nohighlight">\( \mathbf{w} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{u} \)</span> also form about a 45-degree angle between them. We could have predicted this since we already knew that the cosine for <span class="math notranslate nohighlight">\( \mathbf{v} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{w} \)</span> was 1.0.</p>
</section>
</section>
<section id="interpreting-cosine-similarity">
<h2>Interpreting cosine similarity<a class="headerlink" href="#interpreting-cosine-similarity" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A value of 1.0 indicates perfect alignment: the two vectors point in the same direction.</p></li>
<li><p>A value of -1.0 indicates that the two angles point in exactly opposite directions.</p></li>
<li><p>A value of 0.0 indicates the vectors are <strong>orthogonal</strong>. In the 2d case, the two vectors would be perpendicular. If we had a vector of 1s and 0s, like [0 1 0], we can create an orthogonal vector by replacing 1s with 0s and vice versa: [1 0 1]. If you calculate the dot product, you can see why…</p></li>
<li><p>Values between 1.0 and 0 or -1.0 and 0 indicate how similar the vectors are in a graded fashion.</p></li>
</ul>
<p>So: higher positive cosines indicate high similarity. More negative cosines indicate something we can think of like mirrored similarity or complementarity. Remember, cosine similarity does not consider length. So just as {1, 2} and {2, 4} have a cosine of 1.0, so will any 2-element vector where the 2nd element is 2 times the first ({0.01, 0.02} or {1005, 2010}). Those points will fall on the same straight line that goes from 0,0 and then passes through 1,2. For this example in 2 dimensions, we can say that the cosine will be 1.0 between any positive 2-element vector <span class="math notranslate nohighlight">\(x,y\)</span> that conforms to the equation <span class="math notranslate nohighlight">\(y = 2x\)</span> (you should recognize the equation for a line here, <span class="math notranslate nohighlight">\(y = ax + b\)</span>; we are setting <span class="math notranslate nohighlight">\(a\)</span> to 2 and assuming <span class="math notranslate nohighlight">\(b\)</span> is 0). The cosine will be -1.0 if the elements conform to that equation but <span class="math notranslate nohighlight">\(x\)</span> is negative. That’s a mildly interesting observation, but it’s just to help make sense of the relationships here. We are going to be working with larger vectors where there will be no immediate utility to working out something like a multiple regression equation that describes the relationship of elements in a large vector.</p>
</section>
<section id="visualizing">
<h2>Visualizing<a class="headerlink" href="#visualizing" title="Link to this heading">#</a></h2>
<p>Let’s plot our 3 example vectors, along with a new vector, <span class="math notranslate nohighlight">\(\mathbf{d}\)</span>, which points away from the others (it’s cosine similarity to <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> is approximately -0.8). The code cell below just defines the vectors ‘by hand’ (they are ‘hard coded’) and makes a plot. The plot may look a little 3-dimensional, but it’s just 2 dimensions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the vectors</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">])</span>  <span class="c1"># A vector that points in the opposite direction of v (cosine ~ -0.8)</span>

<span class="c1"># Origin point for all vectors</span>
<span class="n">origin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># This should match the number of vectors (4 vectors here)</span>

<span class="c1"># Plot the vectors</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>

<span class="c1"># Plot each vector</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> 
           <span class="n">angles</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale_units</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Set limits and labels</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Add labels</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="c1"># plt.title(&quot;Cosine illustration&quot;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b8901efa005c9ef679d6c73aed8ae4a91dcb12d0a9a65a03d45895eeeb197829.png" src="_images/b8901efa005c9ef679d6c73aed8ae4a91dcb12d0a9a65a03d45895eeeb197829.png" />
</div>
</div>
</section>
<section id="applying-vector-cosine-for-phonemes">
<h2>Applying vector cosine for phonemes<a class="headerlink" href="#applying-vector-cosine-for-phonemes" title="Link to this heading">#</a></h2>
<p>Okay, let’s go back to our vectors for phonemes. Recall that we had the features stop, bilabial, alveolar, voiced, sonorant, and continuant for our example, and that the vectors for /p/, /b/, and /s/ would be 110000, 110100 and 001001, respectively. Let’s add one more vector, for the phoneme /z/, which only differs from /s/ in that it is +voiced: 011001. Let’s make a function that calculates vector cosine and calculate the cosines for these 4 vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Function to compute cosine similarity between two vectors</span>
<span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="c1"># Error checking: Ensure vectors have the same length</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Vectors must have the same length&quot;</span><span class="p">)</span>
    
    <span class="c1"># Convert to numpy arrays for easy computation</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
    <span class="c1"># Compute the dot product</span>
    <span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    
    <span class="c1"># Compute the magnitudes (norms) of the vectors</span>
    <span class="n">v_magnitude</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">w_magnitude</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
    <span class="c1"># Error checking: Ensure we do not divide by zero</span>
    <span class="k">if</span> <span class="n">v_magnitude</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">w_magnitude</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;One or both vectors have zero magnitude, cannot compute cosine similarity&quot;</span><span class="p">)</span>
    
    <span class="c1"># Compute cosine similarity</span>
    <span class="n">cosine_sim</span> <span class="o">=</span> <span class="n">dot_product</span> <span class="o">/</span> <span class="p">(</span><span class="n">v_magnitude</span> <span class="o">*</span> <span class="n">w_magnitude</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cosine_sim</span>

<span class="c1"># Function to create a cosine similarity table</span>
<span class="k">def</span> <span class="nf">create_cosine_table</span><span class="p">(</span><span class="n">vectors</span><span class="p">):</span>
    <span class="n">cosine_table</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">name1</span><span class="p">,</span> <span class="n">vec1</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">cosine_table</span><span class="p">[</span><span class="n">name1</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name2</span><span class="p">,</span> <span class="n">vec2</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">cosine_table</span><span class="p">[</span><span class="n">name1</span><span class="p">][</span><span class="n">name2</span><span class="p">]</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
    
    <span class="c1"># Convert the cosine similarity table to a DataFrame</span>
    <span class="n">cosine_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cosine_table</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cosine_df</span>

<span class="c1"># Define the vectors p, b, s, and z</span>
<span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Create a list of vector names and vectors for pairwise comparison</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;p&#39;</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">:</span> <span class="n">z</span><span class="p">}</span>

<span class="c1"># Create and print the cosine similarity table</span>
<span class="n">cosine_df</span> <span class="o">=</span> <span class="n">create_cosine_table</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cosine_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>          p         b         s         z
p  1.000000  0.816497  0.000000  0.000000
b  0.816497  1.000000  0.000000  0.333333
s  0.000000  0.000000  1.000000  0.816497
z  0.000000  0.333333  0.816497  1.000000
</pre></div>
</div>
</div>
</div>
<p>First, of course the cosines comparing a vector to itself are always 1.0; we see those values on the diagonal – these are the <em>identity</em> comparisons. Note also that the matrix is symmetrical: the top row from left to right is identical to the first column from top to bottom, etc. So we can just look at the ‘upper’ or ‘lower’ triangles of values, or whichever one is more convenient when we are examining the table (do you see why the upper and lower triangle values have to be the same?).</p>
<p>We see a high cosine between /p/ and /b/, and an identical cosine between /z/ and /s/. Can you explain why they have the same value?</p>
<p>Also, we see that the cosine between /s/ and /p/ is 0, as is the cosine between /s/ and /b/. This because /s/ has its ‘1’ values only in positions that contain 0 for /p/ and /b/. The cosine for /z/ and /p/ is 0 for the same reason. But the cosine between /z/ and /b/ is greater than 0 because they share the +voiced feature. Now we could extend this approach to a much larger feature set – and we will later.</p>
</section>
<hr class="docutils" />
<section id="part-4">
<h2>Part 4<a class="headerlink" href="#part-4" title="Link to this heading">#</a></h2>
<section id="a-bayesian-approach-to-evaluating-phoneme-inputs">
<h3>A Bayesian approach to evaluating phoneme inputs<a class="headerlink" href="#a-bayesian-approach-to-evaluating-phoneme-inputs" title="Link to this heading">#</a></h3>
<p>We are finally ready to return to thinking about how we would extend Bayes’ theorem to these kinds of cases. Our feature vectors for each phoneme represent the canonical hypothesis for the features that should be present when the input corresponds to that phoneme. So we want to say that the set of hypotheses is the phoneme inventory: we will assume we are only getting speech input, and the only possible sounds correspond to the ~40 phonemes of English.</p>
<p>For now, let’s do an example where we assume that we only have the phonemes /p/, /b/, /s/, and /z/, and suppose we get an input that does not perfectly match any of these, maybe a pattern like this:</p>
<div class="math notranslate nohighlight">
\[[0.8, 0.7, 0.4, 0.5, 0.2, 0.3]\]</div>
<p>What could this vector mean? It might mean that we have evidence that is 80% consistent with the feature stop, 70% consistent with the feature bilabial, 40% consistent with alveolar, 20% consistent with sonorant, and 30% with continuant. Now how do we evaluate how consistent this evidence vector is with our 4 hypotheses (phonemes)? While there are other alternatives, the most straightforward thing we can do is calculate similarity between that input vector and each of our phonemes. Let’s do that in the code block below, by adding a vector called <code class="docutils literal notranslate"><span class="pre">input</span></code>.</p>
<!---
In fact, the file `jstrace-features-no-silence.csv` has 43 features. And they are not just My colleagues and I designed this feature set 
---> <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># everthing else we need was done in previous code blocks;</span>
<span class="c1"># we will add the input vector and then calculate all the </span>
<span class="c1"># pairwise cosines and print the table</span>

<span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>

<span class="c1"># Create a list of vector names and vectors for pairwise comparison</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;p&#39;</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">:</span> <span class="n">z</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">:</span> <span class="nb">input</span><span class="p">}</span>

<span class="c1"># Create and print the cosine similarity table</span>
<span class="n">cosine_df</span> <span class="o">=</span> <span class="n">create_cosine_table</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cosine_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              p         b         s         z     input
p      1.000000  0.816497  0.000000  0.000000  0.820763
b      0.816497  1.000000  0.000000  0.333333  0.893534
s      0.000000  0.000000  1.000000  0.816497  0.383023
z      0.000000  0.333333  0.816497  1.000000  0.536120
input  0.820763  0.893534  0.383023  0.536120  1.000000
</pre></div>
</div>
</div>
</div>
<p>All we care about is the final column (or bottom row, since they have identical values). We see that the input is somewhat similar to all our categories. But if we added together all the similarities, they would sum to more than 1.0, so they cannot be probabilities. We want to work with probabilities. What we need to do is, for each phoneme (i.e., each hypothesis), calculate the probability of that phoneme given the input.</p>
<p>Let’s look again at the formula for Bayes’ theorem for <span class="math notranslate nohighlight">\(n\)</span> evidence elements:</p>
<div class="math notranslate nohighlight">
\[
P(H_1 \mid E_1,\ldots E_n) = \frac{P(E_1,\ldots E_n \mid H_1) P(H_1)}{P(E_1,\ldots E_2)}
\]</div>
<p>But guess what? Instead of considering each <span class="math notranslate nohighlight">\(E\)</span> separately, we can instead replace the <span class="math notranslate nohighlight">\(P(E_1,\ldots E_2)\)</span> terms with the <strong>similarity</strong> between each phoneme’s defined pattern and the input pattern. Let’s walk through the logic of this.</p>
<p><strong>Likelihoods</strong>: Instead of calculating conventional likelihood values (<span class="math notranslate nohighlight">\(P(E|H)\)</span>), we will use <strong>cosine similarity</strong> as a measure of how similar the input is to each phoneme’s canonical vector. That is, we can approximate the joint probability of each element in the input vector via a similarity measure.</p>
<p><strong>Prior</strong>: However, we still need <em>prior probability</em> values (<span class="math notranslate nohighlight">\(P(H)\)</span>) for each phoneme. We will get these from a real language corpus later, but for our example, let’s just make up some values. If we are describing how the probability of each phoneme over all occurrences of any phoneme, then when we add together the priors for all our phonemes, the result must be 1.0. So let’s say that the priors for /p/, /b/, /s/, and /z/, respectively are 0.2, 0.4, 0.1, and 0.3.</p>
<p><strong>Posterior</strong>: Now we are ready to calculate the posterior probabilities of each phoneme using Bayes’ theorem.</p>
<section id="bayesian-update-using-cosine-similarity">
<h4>Bayesian Update Using Cosine Similarity<a class="headerlink" href="#bayesian-update-using-cosine-similarity" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\( H_j \)</span> represent the <span class="math notranslate nohighlight">\(j^{th}\)</span> phoneme.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\( \mathbf{v_j} \)</span> be the canonical vector for phoneme <span class="math notranslate nohighlight">\( H_j \)</span>.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\( \mathbf{i} \)</span> be the input vector.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(H_j) \)</span> is the prior probability of phoneme <span class="math notranslate nohighlight">\( H_j \)</span>.</p></li>
</ul>
<p>Now we substitute the cosine similarity between the input vector and each phoneme’s vector as approximating the likelihood:</p>
<div class="math notranslate nohighlight">
\[
P(\mathbf{i} \mid H_j) \approx \cos({\mathbf{v_j}, \mathbf{i}})
\]</div>
<p>Let’s simplify this and allow for the possibility that we might use another similarity metric, and instead say the following, where <span class="math notranslate nohighlight">\(sim\)</span> is whatever similarity metric we choose:</p>
<div class="math notranslate nohighlight">
\[
P(\mathbf{i} \mid H_j) \approx {sim}({\mathbf{v_j}, \mathbf{i}})
\]</div>
<p>Now, applying Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[
P(H_j \mid \mathbf{i}) = \frac{P(\mathbf{i} \mid H_j) P(H_j)}{P(\mathbf{i})} = \frac{sim(\mathbf{i}, \mathbf{v_j}) P(H_j)}{\sum\limits_{k=1}^{n}sim(\mathbf{i}, \mathbf{v_k}) P(H_k)}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P(H_j \mid \mathbf{i}) \)</span> is the <strong>posterior probability</strong> that the input corresponds to phoneme <span class="math notranslate nohighlight">\( H_j \)</span> (that is, the conditional probability of <span class="math notranslate nohighlight">\(H_j\)</span> given input <span class="math notranslate nohighlight">\(\mathbf{i}\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\( P(\mathbf{i} \mid H_j) \)</span> (the <strong>likelihood</strong> for phoneme <span class="math notranslate nohighlight">\(j\)</span>) is substituted with the (cosine) similarity between the input vector <span class="math notranslate nohighlight">\( \mathbf{i} \)</span> and the phoneme’s defined vector <span class="math notranslate nohighlight">\( \mathbf{v_j} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(H_j) \)</span> is the <strong>prior probability</strong> of the phoneme <span class="math notranslate nohighlight">\( H_j \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(\mathbf{i}) \)</span> is the <strong>total evidence</strong>. This corresponds to the sum of the weighted likelihoods, so we write it using <span class="math notranslate nohighlight">\(\sum\)</span> notation. The sum notation indicates that there are <span class="math notranslate nohighlight">\(n\)</span> phonemes, and we are summing the similarities for each phoneme <span class="math notranslate nohighlight">\(k\)</span> (starting with <span class="math notranslate nohighlight">\(k=1\)</span>, continuing through <span class="math notranslate nohighlight">\(k=n\)</span>). Note that the sum includes all phonemes, including phoneme <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
</ul>
<p>The way to think about this ratio is is how much of the total similarity ‘mass’ does phoneme <span class="math notranslate nohighlight">\(j\)</span> provide? This is directly analogous to thinking about the ratio of true positives over all positives.</p>
<p>Let’s expand on our python code to calculate the posterior probability of each phoneme given the input pattern [0.8, 0.7, 0.4, 0.5, 0.2, 0.3] and prior probabilities for the 4 phonemes listed above (0.2, 0.4, 0.1, 0.3).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Function to compute posterior probabilities using cosine similarity and prior probabilities</span>
<span class="k">def</span> <span class="nf">compute_posterior</span><span class="p">(</span><span class="n">input_vector</span><span class="p">,</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">priors</span><span class="p">):</span>
    <span class="n">posteriors</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">total_weighted_similarity</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Step 1: Calculate the cosine similarity between the input vector and each phoneme vector</span>
    <span class="k">for</span> <span class="n">phoneme</span><span class="p">,</span> <span class="n">phoneme_vector</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">cosine_sim</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">input_vector</span><span class="p">,</span> <span class="n">phoneme_vector</span><span class="p">)</span>
        
        <span class="c1"># Step 2: Multiply the cosine similarity by the prior probability for each phoneme</span>
        <span class="n">weighted_similarity</span> <span class="o">=</span> <span class="n">cosine_sim</span> <span class="o">*</span> <span class="n">priors</span><span class="p">[</span><span class="n">phoneme</span><span class="p">]</span>
        <span class="n">posteriors</span><span class="p">[</span><span class="n">phoneme</span><span class="p">]</span> <span class="o">=</span> <span class="n">weighted_similarity</span>
        
        <span class="c1"># Increment the sum with each new weighted similarity</span>
        <span class="n">total_weighted_similarity</span> <span class="o">+=</span> <span class="n">weighted_similarity</span>

    <span class="c1"># Step 3: Normalize by dividing each prior-weighted similarity by the total weighted similarity</span>
    <span class="k">for</span> <span class="n">phoneme</span> <span class="ow">in</span> <span class="n">posteriors</span><span class="p">:</span>
        <span class="n">posteriors</span><span class="p">[</span><span class="n">phoneme</span><span class="p">]</span> <span class="o">/=</span> <span class="n">total_weighted_similarity</span>

    <span class="k">return</span> <span class="n">posteriors</span>

<span class="c1"># Define the prior probabilities for each phoneme</span>
<span class="n">priors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;p&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
    <span class="s1">&#39;s&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s1">&#39;z&#39;</span><span class="p">:</span> <span class="mf">0.3</span>
<span class="p">}</span>

<span class="c1"># The input vector to be classified</span>
<span class="n">input_vector</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>

<span class="c1"># Create a list of vector names and vectors for pairwise comparison</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;p&#39;</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="n">b</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">:</span> <span class="n">s</span><span class="p">,</span> <span class="s1">&#39;z&#39;</span><span class="p">:</span> <span class="n">z</span><span class="p">}</span>

<span class="c1"># Compute and print the posterior probabilities for each phoneme</span>
<span class="n">posteriors</span> <span class="o">=</span> <span class="n">compute_posterior</span><span class="p">(</span><span class="n">input_vector</span><span class="p">,</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">priors</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Posterior probabilities for each phoneme:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">phoneme</span><span class="p">,</span> <span class="n">probability</span> <span class="ow">in</span> <span class="n">posteriors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">phoneme</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">probability</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Posterior probabilities for each phoneme:
p: 0.2278
b: 0.4959
s: 0.0531
z: 0.2232
</pre></div>
</div>
</div>
</div>
<p>Now we have the posteriors. We see that /b/ is substantially more likely given the input than the other phonemes. What should a phoneme-recognition system do at this point? One possibility would be to maximize (exploit) and ‘recognize’ /b/. Alternatively, these values could be the input to a subsequent module for mapping sequences of phoneme probabilities to words. We will explore a few possibilities below.</p>
</section>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="part-5">
<h1>Part 5<a class="headerlink" href="#part-5" title="Link to this heading">#</a></h1>
<section id="a-bayesian-approach-to-spoken-word-recognition">
<h2>A Bayesian approach to spoken word recognition<a class="headerlink" href="#a-bayesian-approach-to-spoken-word-recognition" title="Link to this heading">#</a></h2>
<p>In a Bayesian framework for word recognition, we must work with probabilities conditioned on observed evidence. Our observed evidence will be a sequence of phonetic features, from which we’ll calculate the probabilities of possible phonemes at each step. Then we need to continuously map that sequence onto possible words.</p>
<p>For our first attempt, we are going to try to follow the straightforward logic of the Shortlist B model (Norris &amp; McQueen, 2008). We will simulate spoken word recognition by processing one phoneme at a time. The idea is to update our beliefs (probabilities) about which word is being spoken as new phonemes are observed.</p>
<p>We will simplify the approach compared to Norris &amp; McQueen in several ways. First, if you have read the Shortlist B paper (which you were not asked to do!), you will know that they actually generate a ‘shortlist’ of words that could start at any position in the input. So if the input is ‘catalog’, at position 1, they consider any word that starts with /k/ aligned with that position (cat, kit, color, etc.). When the next phoneme arrives, the shortlist at position 1 favors words that begin /kæ/. At the same time, they start a shortlist of words that begin with /æ/ aligned at position 2! This is very interesting, and it allows their system to parse sequences of multiple words in an elegant way. (These are called ‘shortlists’ because not all possibilities are considered; shortlist size is a parameter of the model, with, e.g., the 20 most frequent [most likely!] possibilities retained).</p>
<p>However, we are simply going to make a model that does isolated word recognition and therefore only considers words aligned at position 1. In other respects, we will make our model as much like theirs as we can.</p>
<blockquote>
<div><p><em>Note</em>: æ is the phonetic symbol for the vowel sound in cat.</p>
</div></blockquote>
<blockquote>
<div><p><em>Note: Even though I just described this with phrases like ‘/k/ aligned with that position’, they don’t use discrete phoneme inputs. They use distributions of phoneme probabilities, i.e., the probability of each phoneme at each position given the input, similar to what we discussed in the previous part. Instead of phoneme similarities based on phonetic features, they use diphone (phoneme pair) confusion probabilities that were estimated from Dutch speakers listening to real syllables. We can talk more about this in class.</em></p>
</div></blockquote>
</section>
<section id="phoneme-inputs">
<h2>Phoneme inputs<a class="headerlink" href="#phoneme-inputs" title="Link to this heading">#</a></h2>
<section id="bottom-up-phoneme-probabilities-frequencies-priors-etc">
<h3>Bottom-up phoneme probabilities: frequencies, priors, etc.<a class="headerlink" href="#bottom-up-phoneme-probabilities-frequencies-priors-etc" title="Link to this heading">#</a></h3>
<p>An assumption on the Shortlist approach is that we will evaluate the evidence for phonemes purely on the basis of bottom-up input. This means that even at later positions, we will not evaluate the conditional probability of a phoneme at the current position given both the current bottom-up evidence (current phoneme) <em>and</em> the preceding sequence <em>or</em> top-down probabilities based on the words in the lexicon.</p>
<p>So for each phoneme, we will need a way to calculate <span class="math notranslate nohighlight">\(P(Evidence|Phoneme)\)</span> and <span class="math notranslate nohighlight">\(P(Phoneme|Evidence)\)</span> (that is, the probability of the evidence [input signal, i.e., the current phoneme-sized sound pattern] given a specific phoneme, and the probability of that phoneme given the evidence). What is the evidence? It will be the phoneme that is the input at each position. So we could simply say that if the input is /k/, it is 100% likely that /k/ has occurred and 0% evidence for all other phonemes. However, we know that humans typically do not ‘recognize’ phonemes with 100% accuracy – in large part because speakers’ productions are not 100% unambiguous. So when the word is ‘bet’, listeners will be most likely to hear the first sound as /b/, but there will be some probability that they will hear it is /p/ (pet) or /g/ (get), but also very low chance that they will hear it as /s/ (set) or /f/ (fête). We will have graded confusability among phonemes. (A related note: there is evidence that people are sensitive to the distribution of probabilities of <em>all</em> phonemes; it’s not that sometimes we just hear /b/ and sometimes we just hear /p/, when the input was intended to be /b/. Rather, we are attuned to the probabilities of /p/, /b/, /t/, /d/, /m/, etc., given natural speech input.)</p>
<p>We could get these from human perceptual data, like Norris and McQueen did for Dutch. But this would be a massive undertaking. In the Part 4, we already discussed a way to generate theoretically-based similarities, based on what we know about the <em>phonetic features</em> linguists have described for different phonemes. We will use a much larger feature set (43 features, though it’s possible to describe all phonemes in English robustly with 18 features; I’ll explain why we use so many later). Based on the features, we can create a table where each column is a phonetic feature, so each row is the phonetic feature vector for a different phoneme.</p>
<p>At first, we will assume clean, noise-free inputs. So when the current phoneme is /b/, we just take the /b/ row from the dataframe. Because other phonemes share features with /b/, we will not propose that /b/ is 100% likely when the input is the vector for /b/. Instead, we will consult the (cosine) similarities of the vector for /b/ to all other phoneme vectors, weight those similarities by the prior probabilites (relative frequencies) of each phoneme, and normalize (divide each phoneme’s prior-weighted similarity to the input by the sum of all phonemes’ similarities to the input).</p>
<p>This approach is a bit different from that taken by Norris and McQueen (2008). Here is the step where they calculate posterior (conditional) probabilities of phonemes given the current input (their Equation 3):</p>
<div class="math notranslate nohighlight">
\[
P(Phoneme_{j}|Evidence)=\frac{P(Evidence|Phoneme_{j}) \times P(Phoneme_{j})}{\sum\limits_{k=1}^{n}P(Evidence|Phoneme_{k}) \times P(Phoneme_{k})}
\]</div>
<p>where there are <span class="math notranslate nohighlight">\(m\)</span> phonemes in the set.</p>
<p>But recall, we are doing this step like this instead:</p>
<div class="math notranslate nohighlight">
\[
P(H_j \mid \mathbf{i}) = \frac{P(\mathbf{i} \mid H_j) P(H_j)}{P(\mathbf{i})} = \frac{sim(\mathbf{i}, \mathbf{v_j}) P(H_j)}{\sum\limits_{k=1}^{n}sim(\mathbf{i}, \mathbf{v_k}) P(H_k)}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( P(H_j \mid \mathbf{i}) \)</span> is the <strong>posterior probability</strong> that the input corresponds to phoneme <span class="math notranslate nohighlight">\( H_j \)</span> (that is, the conditional probability of <span class="math notranslate nohighlight">\(H_j\)</span> given input <span class="math notranslate nohighlight">\(\mathbf{i}\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\( P(\mathbf{i} \mid H_j) \)</span> (the <strong>likelihood</strong> for phoneme <span class="math notranslate nohighlight">\(j\)</span>) is substituted with the (cosine) similarity between the input vector <span class="math notranslate nohighlight">\( \mathbf{i} \)</span> and the phoneme’s defined vector <span class="math notranslate nohighlight">\( \mathbf{v_j} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(H_j) \)</span> is the <strong>prior probability</strong> of the phoneme <span class="math notranslate nohighlight">\( H_j \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(\mathbf{i}) \)</span> is the <strong>total evidence</strong>. This corresponds to the sum of the weighted likelihoods, so we write it using <span class="math notranslate nohighlight">\(\sum\)</span> notation. The sum notation indicates that there are <span class="math notranslate nohighlight">\(n\)</span> phonemes, and we are summing the similarities for each phoneme <span class="math notranslate nohighlight">\(k\)</span> (starting with <span class="math notranslate nohighlight">\(k=1\)</span>, continuing through <span class="math notranslate nohighlight">\(k=n\)</span>). Note that the sum includes all phonemes, including phoneme <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
</ul>
</section>
</section>
<section id="words">
<h2>Words<a class="headerlink" href="#words" title="Link to this heading">#</a></h2>
<p>Now let’s consider words.</p>
<section id="prior-probability-word-frequency">
<h3>Prior Probability (Word Frequency)<a class="headerlink" href="#prior-probability-word-frequency" title="Link to this heading">#</a></h3>
<p>Before any phonemes are observed, our belief about which word is likely to be spoken would be based on the frequency with which each word occurs in the language. This is our <em>prior distribution</em>,  <span class="math notranslate nohighlight">\( P(\mathbf{W}) \)</span>, where <span class="math notranslate nohighlight">\( \mathbf{W} \)</span> is a vector with frequencies known words (typically expressed as occurrences per 1 million words). In the simplest case, we convert freqencies to probabilities by normalizing them – we divide each word’s frequency by the sum of all words’ frequencies.</p>
</section>
<section id="likelihood-words">
<h3>Likelihood (words)<a class="headerlink" href="#likelihood-words" title="Link to this heading">#</a></h3>
<p>As each new phoneme is observed, we want to update our estimate of how likely each word is to be the current word (that is coming in phoneme-by-phoneme). So we take the <span class="math notranslate nohighlight">\(PhonemeString\)</span> that has emerged so far as the Evidence (think of this as a matrix with 1 column for every time step and 1 row for every phoneme, with each cell containing the probability of the phoneme at that position), and then need to calculate <span class="math notranslate nohighlight">\(P(Word_{j}|Evidence)\)</span> for each word <span class="math notranslate nohighlight">\(j\)</span> (that is, we need to calculate the probability of word <span class="math notranslate nohighlight">\(j\)</span> given the input thus far). Norris &amp; McQueen do this with their Equation 4.</p>
<div class="math notranslate nohighlight">
\[
Likelihood(Word_{j}) = P(Evidence|Word_{j})=P(PhonemeString_{k})=\prod\limits_{k=1}^{n}P(Phoneme_{k}|Evidence)
\]</div>
<p>So this says that we will proceed in phoneme-size steps. At each step we get the probability of each phoneme given the bottom-up input. Now we are going to make a somewhat dubious assumption: that the phonemes at each position are independent of each other. This is an assumption Norris &amp; McQueen make. Of course, the probability of /k/ being followed by /æ/ is probably higher than the probability of /k/ being followed by /s/ (as happens in the word <em>taxes</em>, where the phonemes are /tæks^z/). But we will ignore that for now, and accept their assumption that each phoneme is independent of the others. If the phonemes are independent events, how do we calculate the probability of a sequence of independent events? Recall that for coin flips, we find the probability of 3 heads in a row by multiplying 0.5 x 0.5 x 0.5. So in the equation above, we do the same thing: the right side of the equation is saying we will calculate the product of the phoneme probabilities at each position.</p>
<p>So if we consider whether the input sequence is a good match to the word is ‘cat’, which has phonemes /k/, /æ/, and /t/, we would calculate <span class="math notranslate nohighlight">\(P(k|i_1) \times P(æ|i_2) \times P(t|i_3)\)</span> (where <span class="math notranslate nohighlight">\(i_1, i_2,\)</span> and <span class="math notranslate nohighlight">\(i_3\)</span> are the inputs at positions 1-3). To evaluate the likelihood that the word is /fæt/, we would calcuate <span class="math notranslate nohighlight">\(P(f|i_1) \times P(æ|i_2) \times P(t|i_3)\)</span>. In fact, we do this for every word in the lexicon.</p>
</section>
<section id="posterior-probability-words">
<h3>Posterior Probability (words)<a class="headerlink" href="#posterior-probability-words" title="Link to this heading">#</a></h3>
<p>Then to get the posterior probability for each word at the current position, we use Equation 5 from Norris &amp; McQueen.</p>
<div class="math notranslate nohighlight">
\[
P(Word_{i}|Evidence)=\frac{P(Evidence|Word_{i}) \times P(Word_{i})}
{\sum\limits_{j=1}^{j=n}P(Evidence|Word_{j}) \times P(Word_{j})}
\]</div>
<p>which we can calculate by using <span class="math notranslate nohighlight">\(PhonemeString\)</span> values:</p>
<div class="math notranslate nohighlight">
\[
Posterior = P(Word_{i}|Evidence)=\frac{P(PhonemeString_{i}) \times P(Word_{i})}
{\sum\limits_{j=1}^{j=n}P(PhonemeString_{j}) \times P(Word_{j})}
\]</div>
<p>We will do this at each phoneme position in ‘left-to-right’ fashion (first to last phonemes) in order to generate an ongoing estimate of probabilities. We will try to relate this to some fine-grained measures of human spoken word recognition later.</p>
<p><em>Note: in 2024, I revamped the phoneme section a lot, and I will need to do more work to make the word and phoneme sections more compatible.</em></p>
<hr class="docutils" />
<p>Let’s try to implement this model!</p>
</section>
<section id="step-1-import-the-lexicon">
<h3>Step 1: Import the lexicon<a class="headerlink" href="#step-1-import-the-lexicon" title="Link to this heading">#</a></h3>
<p>The first thing we need is a lexicon that will have the list of words we want to consider, and their <em>prior probabilities</em>. Prior probability for words is usually expressed as frequency [occurrences] per million words. To calculate this, we take a big <em>corpus</em> of text or speech. For example, we could take 10 years worth of the text of the <em>New York Times</em>, everything that has ever been posted on Twitter, or we could use ever-growing databases that compile subtitles from speech transcribed for movies, shows, etc.</p>
<p>We are going to use a lexicon called <em>lemmalex</em> that uses a variety of sources, but mainly subtitle databases. This is from a tool for the R statistical programming language that my lab released called <em>LexFindR</em> (Li, Z., Crinnion, A. M., &amp; Magnuson, J. S. (2021). LexFindR: A fast, simple, and extensible R package for finding similar words in a lexicon. <em>Behavior Research Methods</em>. <a class="reference external" href="https://doi.org/10.3758/s13428-021-01667-6">https://doi.org/10.3758/s13428-021-01667-6</a>).</p>
<p>Let’s read it into a dataframe called <code class="docutils literal notranslate"><span class="pre">lexicon_df</span></code>. We will also create a log frequency column. Then we can look at the first 5 rows by calling <code class="docutils literal notranslate"><span class="pre">lexicon_df.head()</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Function to read in the lexicon file</span>
<span class="k">def</span> <span class="nf">read_lexicon_file</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
    <span class="n">lexicon_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lexicon_df</span>

<span class="c1"># Read the lexicon file</span>
<span class="n">lexicon_df</span> <span class="o">=</span> <span class="n">read_lexicon_file</span><span class="p">(</span><span class="s1">&#39;./lemmalex.csv&#39;</span><span class="p">)</span>

<span class="c1"># Drop any rows with NaN [not a number, here indicating no value] available</span>
<span class="c1"># values in the &#39;Pronunciation&#39; column</span>
<span class="n">lexicon_df</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Pronunciation&#39;</span><span class="p">])</span>

<span class="c1"># add log frequency column</span>
<span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;lfrq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;Frequency&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">lexicon_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Item</th>
      <th>Frequency</th>
      <th>Pronunciation</th>
      <th>lfrq</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>a</td>
      <td>20415.27</td>
      <td>AH</td>
      <td>9.924087</td>
    </tr>
    <tr>
      <th>1</th>
      <td>abandon</td>
      <td>8.10</td>
      <td>AH B AE N D IH N</td>
      <td>2.208274</td>
    </tr>
    <tr>
      <th>2</th>
      <td>abandonment</td>
      <td>0.96</td>
      <td>AH B AE N D AH N M AH N T</td>
      <td>0.672944</td>
    </tr>
    <tr>
      <th>3</th>
      <td>abate</td>
      <td>0.10</td>
      <td>AH B EY T</td>
      <td>0.095310</td>
    </tr>
    <tr>
      <th>4</th>
      <td>abbey</td>
      <td>3.18</td>
      <td>AE B IY</td>
      <td>1.430311</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<hr class="docutils" />
<p>Here are simple ways to get the length (number of rows) in the dataframe and also to summarize its numerical columns. Note that frequencies are occurrences per million words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># this will show that there are 17,750 words in the lexicon</span>
<span class="nb">len</span><span class="p">(</span><span class="n">lexicon_df</span><span class="p">),</span> <span class="n">lexicon_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(17750,
           Frequency          lfrq
 count  17750.000000  17750.000000
 mean      43.262201      1.222613
 std      651.204163      1.398385
 min        0.020000      0.019803
 25%        0.250000      0.223144
 50%        1.020000      0.703098
 75%        4.760000      1.750937
 max    41857.120000     10.642041)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="look-at-frequency-using-histograms">
<h3>Look at frequency using histograms<a class="headerlink" href="#look-at-frequency-using-histograms" title="Link to this heading">#</a></h3>
<p>Wow, those frequency ranges are huge – from 0.02 occurrence per million words to 41,857!</p>
<p>Let’s make a function that will plot histograms and then a ‘cumulative sort plot’. The histogram plots the frequency of different values. The sort plot just sorts the values and plots them in sequence. We will see that there are a very small number of very high frequency items (like the words ‘the’ or ‘a’). This helps illustrate why it’s useful to move the raw frequency counts to log values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">math</span>


<span class="k">def</span> <span class="nf">plot_histograms</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">pointsize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alphaval</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots histograms for the specified columns of the given DataFrame.</span>
<span class="sd">    Adds a subplot below each histogram to show the individual data points.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        df (DataFrame): The DataFrame containing the specified columns.</span>
<span class="sd">        columns (list): List of column names to plot.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Calculate the number of rows needed for the subplots</span>
    <span class="n">n_rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns</span><span class="p">)</span>
    
    <span class="c1"># Create a figure and a set of subplots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">n_rows</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;width_ratios&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]})</span>
    
    <span class="c1"># If there&#39;s only one row, axs is a 1D array and we need to reshape it</span>
    <span class="k">if</span> <span class="n">n_rows</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Flatten the axs array for easy iteration</span>
    <span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="c1"># Plot histograms and individual data points</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">columns</span><span class="p">):</span>
        <span class="c1"># Plot histogram</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alphaval</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1"> Histogram&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
        <span class="c1"># Apply log scale to raw frequency (&#39;Frequency&#39;) histogram</span>
        <span class="c1"># if col == &#39;Frequency&#39;:</span>
        <span class="c1">#     axs[2 * idx].set_xscale(&#39;log&#39;)</span>
        <span class="c1">#     axs[2 * idx].set_yscale(&#39;log&#39;)</span>
        
        <span class="c1"># Plot individual data points sorted by value</span>
        <span class="n">sorted_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sorted_data</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">markersize</span> <span class="o">=</span> <span class="n">pointsize</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1"> Sorted Data Points&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Position&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
        
        <span class="c1"># Apply log scale to raw frequency (&#39;Frequency&#39;) histogram</span>
        <span class="c1"># if col == &#39;Frequency&#39;:</span>
        <span class="c1">#     axs[2 * idx + 1].set_xscale(&#39;log&#39;)</span>
        <span class="c1">#     axs[2 * idx + 1].set_yscale(&#39;log&#39;)</span>

    <span class="c1"># Hide any extra subplots</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">columns</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">axs</span><span class="p">)):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    
    <span class="c1"># Show the plots</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example usage:</span>
<span class="n">plot_histograms</span><span class="p">(</span><span class="n">lexicon_df</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="s1">&#39;lfrq&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/63900558a726582b91289dd8821f3136038309fcf10cd040ce0d05b22d057910.png" src="_images/63900558a726582b91289dd8821f3136038309fcf10cd040ce0d05b22d057910.png" />
</div>
</div>
</section>
<section id="trimming-based-on-frequency">
<h3>Trimming based on frequency<a class="headerlink" href="#trimming-based-on-frequency" title="Link to this heading">#</a></h3>
<p>For reasons we can discuss in class, having small numbers of items with very high frequency is problematic. Let’s just cut items with log frequency more than 3 standard deviations above the mean. (For those of you who care about these details, Berger &amp; Kiefer [2021] compared various methods for trimming outlying values and found that a simple 3SD threshold is a minimally biased approach; they are concerned with reaction time data, but this is a fine way to proceed for our purposes… if this were a research project we intended to publish, we would have to delve a bit deeper and really understand the characteristics of the items we are trimming).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate mean and standard deviation of &#39;lfrq&#39;</span>
<span class="n">mean_lfrq</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;lfrq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">std_lfrq</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;lfrq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># Define the boundaries for acceptable &#39;lfrq&#39; values</span>
<span class="n">lower_bound</span> <span class="o">=</span> <span class="n">mean_lfrq</span> <span class="o">-</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">std_lfrq</span>
<span class="n">upper_bound</span> <span class="o">=</span> <span class="n">mean_lfrq</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">std_lfrq</span>

<span class="c1"># Remove records where &#39;lfrq&#39; is more than 3 standard deviations from the mean</span>
<span class="n">filtered_lexicon_df</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="p">[(</span><span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;lfrq&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">lower_bound</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;lfrq&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">upper_bound</span><span class="p">)]</span>

<span class="c1"># replot histograms</span>
<span class="n">plot_histograms</span><span class="p">(</span><span class="n">filtered_lexicon_df</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="s1">&#39;lfrq&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7478ca09fc3946b2004f26ea5c2c2ea976021c635dd30be91c2f9ceddb31a34a.png" src="_images/7478ca09fc3946b2004f26ea5c2c2ea976021c635dd30be91c2f9ceddb31a34a.png" />
</div>
</div>
<p>The <strong>trimmed data</strong> seems better. We have a smaller range, and fewer points way out at high values. Next code cells describe and then just take the filtered lexicon as our default lexicon.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># much better; let&#39;s redescribe</span>
<span class="n">filtered_lexicon_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Frequency</th>
      <th>lfrq</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17397.000000</td>
      <td>17397.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>7.590574</td>
      <td>1.112872</td>
    </tr>
    <tr>
      <th>std</th>
      <td>21.898453</td>
      <td>1.168418</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.020000</td>
      <td>0.019803</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.240000</td>
      <td>0.215111</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.960000</td>
      <td>0.672944</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.240000</td>
      <td>1.656321</td>
    </tr>
    <tr>
      <th>max</th>
      <td>223.550000</td>
      <td>5.414098</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">lexicon_df</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">filtered_lexicon_df</span><span class="p">),</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">filtered_lexicon_df</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">lexicon_df</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(17750, 17397, 0.980112676056338)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># let&#39;s just replace lexicon_df with the filtered one</span>
<span class="n">lexicon_df</span> <span class="o">=</span> <span class="n">filtered_lexicon_df</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="an-aside-phonemes">
<h3>An aside: phonemes<a class="headerlink" href="#an-aside-phonemes" title="Link to this heading">#</a></h3>
<p>Again, phonemes are roughly the consonants and vowels of a language. Note that there is not a 1-to-1 mapping between phonemes and letters in English. We can get a /k/ sound from the letters C, K, QU, for example. The letter A can link to sounds like <em>ay</em>, <em>eh</em>, <em>uh</em>, <em>ah</em>, <em>ae</em> [as in cat], etc. In North American English dialects, we typically have 39-40 phonemes that we have to represent using 26 letters.</p>
<p>Linguists use the International Phonetic Alphabet to transcribe sounds across languages. The lexicon file we are using uses a symbol system often used in natural language processing computing called <em>Arpabet</em>. Here’s how to map phonemes to IPA and to actual sounds using examples. This data for this table comes from the website of a speech recognition company called Soapbox.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Arpabet (SoapBox)</p></th>
<th class="head"><p>IPA</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AA</p></td>
<td><p>ɑ</p></td>
<td><p>balm, bot</p></td>
</tr>
<tr class="row-odd"><td><p>AE</p></td>
<td><p>æ</p></td>
<td><p>bat</p></td>
</tr>
<tr class="row-even"><td><p>AH</p></td>
<td><p>ʌ</p></td>
<td><p>butt</p></td>
</tr>
<tr class="row-odd"><td><p>AO</p></td>
<td><p>ɔ</p></td>
<td><p>cot</p></td>
</tr>
<tr class="row-even"><td><p>AW</p></td>
<td><p>aʊ</p></td>
<td><p>bout</p></td>
</tr>
<tr class="row-odd"><td><p>AY</p></td>
<td><p>aɪ</p></td>
<td><p>bite</p></td>
</tr>
<tr class="row-even"><td><p>EH</p></td>
<td><p>ɛ</p></td>
<td><p>bet</p></td>
</tr>
<tr class="row-odd"><td><p>ER</p></td>
<td><p>ɝ</p></td>
<td><p>bird</p></td>
</tr>
<tr class="row-even"><td><p>EY</p></td>
<td><p>eɪ</p></td>
<td><p>bait</p></td>
</tr>
<tr class="row-odd"><td><p>IH</p></td>
<td><p>ɪ</p></td>
<td><p>bit</p></td>
</tr>
<tr class="row-even"><td><p>IY</p></td>
<td><p>i</p></td>
<td><p>beat</p></td>
</tr>
<tr class="row-odd"><td><p>OW</p></td>
<td><p>oʊ</p></td>
<td><p>boat</p></td>
</tr>
<tr class="row-even"><td><p>OY</p></td>
<td><p>ɔɪ</p></td>
<td><p>boy</p></td>
</tr>
<tr class="row-odd"><td><p>UH</p></td>
<td><p>ʊ</p></td>
<td><p>book</p></td>
</tr>
<tr class="row-even"><td><p>UW</p></td>
<td><p>u</p></td>
<td><p>boot</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>b</p></td>
<td><p>buy</p></td>
</tr>
<tr class="row-even"><td><p>CH</p></td>
<td><p>tʃ</p></td>
<td><p>china</p></td>
</tr>
<tr class="row-odd"><td><p>D</p></td>
<td><p>d</p></td>
<td><p>die</p></td>
</tr>
<tr class="row-even"><td><p>DH</p></td>
<td><p>ð</p></td>
<td><p>thy, bathe</p></td>
</tr>
<tr class="row-odd"><td><p>F</p></td>
<td><p>f</p></td>
<td><p>fight</p></td>
</tr>
<tr class="row-even"><td><p>G</p></td>
<td><p>g</p></td>
<td><p>guy</p></td>
</tr>
<tr class="row-odd"><td><p>HH</p></td>
<td><p>h</p></td>
<td><p>high</p></td>
</tr>
<tr class="row-even"><td><p>JH</p></td>
<td><p>dʒ</p></td>
<td><p>jump</p></td>
</tr>
<tr class="row-odd"><td><p>K</p></td>
<td><p>k</p></td>
<td><p>kite</p></td>
</tr>
<tr class="row-even"><td><p>L</p></td>
<td><p>l</p></td>
<td><p>lie</p></td>
</tr>
<tr class="row-odd"><td><p>M</p></td>
<td><p>m</p></td>
<td><p>my</p></td>
</tr>
<tr class="row-even"><td><p>N</p></td>
<td><p>n</p></td>
<td><p>night</p></td>
</tr>
<tr class="row-odd"><td><p>NG</p></td>
<td><p>ŋ</p></td>
<td><p>sing</p></td>
</tr>
<tr class="row-even"><td><p>P</p></td>
<td><p>p</p></td>
<td><p>pie</p></td>
</tr>
<tr class="row-odd"><td><p>R</p></td>
<td><p>ɹ</p></td>
<td><p>rye</p></td>
</tr>
<tr class="row-even"><td><p>S</p></td>
<td><p>s</p></td>
<td><p>sigh</p></td>
</tr>
<tr class="row-odd"><td><p>SH</p></td>
<td><p>ʃ</p></td>
<td><p>shy</p></td>
</tr>
<tr class="row-even"><td><p>T</p></td>
<td><p>t</p></td>
<td><p>tie</p></td>
</tr>
<tr class="row-odd"><td><p>TH</p></td>
<td><p>θ</p></td>
<td><p>think</p></td>
</tr>
<tr class="row-even"><td><p>V</p></td>
<td><p>v</p></td>
<td><p>vie</p></td>
</tr>
<tr class="row-odd"><td><p>W</p></td>
<td><p>w</p></td>
<td><p>wise</p></td>
</tr>
<tr class="row-even"><td><p>Y</p></td>
<td><p>j</p></td>
<td><p>yacht, yet</p></td>
</tr>
<tr class="row-odd"><td><p>Z</p></td>
<td><p>z</p></td>
<td><p>zoo</p></td>
</tr>
<tr class="row-even"><td><p>ZH</p></td>
<td><p>ʒ</p></td>
<td><p>pleasure</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="phoneme-similarity">
<h3>Phoneme similarity<a class="headerlink" href="#phoneme-similarity" title="Link to this heading">#</a></h3>
<p>We are going to do simulations by presenting one phoneme at a time. We could use ‘localist’ or ‘one-hot’ representations, where each phoneme has a discrete value and no similarity to other phonemes (so when the input is /k/, the evidence for /k/ would be 100% and evidence for all other phonemes would be 0%). However, we know from a century of research that humans perceive some phonemes (e.g., ‘b’ and ‘p’) as more similar than others (e.g., ‘p’ and ‘a’ are not similar). So we want to include this in our simulation. If the word is ‘bat’, the phoneme /b/ should give us high evidence that /b/ has occurred, but also moderate evidence that /p/ may have occurred, and very low evidence that /o/ occurred (because /b/ is quite similar to /p/ but quite different from /o/, phonetically).</p>
<p>There are various ways to estimate similarity. For example, we can present phonemes in noise to listeners and ask them to transcribe what they hear. Then we can calculate ‘confusability’ or similarity as how often, for example, people answered ‘p’ when the actual input was ‘b’.</p>
<p>There are a <em>lot</em> of potential sources for such ‘confusion’ data, but it is not obvious how to use it. For example, Luce (1986) argued in his dissertation that his confusion data could not generalize beyond the specific conditions under which it was collected – it applied only for the particular level of noise and other conditions under which it was collected.</p>
<p>Let’s figure out a way to do our own similarity estimates from what data is avaialble to us.</p>
</section>
<section id="phonetic-similarity">
<h3>Phonetic similarity<a class="headerlink" href="#phonetic-similarity" title="Link to this heading">#</a></h3>
<p>We could use featural definitions of phonemes. For example, /p/ and /b/ are both bilabial stops. The only difference is that /b/ is ‘voiced’ while /p/ is ‘voiceless’. We could compare phonemes on their featural similarity. We might do this later… Let’s try a method that does not depend on us knowing the featural definitions.</p>
</section>
<section id="context-similarity">
<h3>Context similarity<a class="headerlink" href="#context-similarity" title="Link to this heading">#</a></h3>
<p>It turns out that we can learn a lot about some element in the world or in a system (phoneme, word, object) by the company it keeps. Let’s just go through the lexicon and count how many times any phoneme pair co-occurs adjacently. So for example, the word ‘cat’ has the phonemes /k/, /æ/, and /t/. So we could increment the count for /kæ/ and /æt/, the two pairs of phonemes that occur. To tabulate this, we’ll make a phoneme-by-phoneme matrix. So for cat, we would increment the counts for {k,æ}, {æ, k}, {æ, t}, and {t, æ}. However, we should also adjust this for word frequency. The word cat occurs fairly often, so we would want to boost these counts a bit more than the counts for the same pairs from the word catalytic. We will use word frequency to do this.</p>
<p>What we will end up with is a phoneme-by-phoneme matrix that counts how often each phoneme occurs with every other phoneme. At the end of the code cell, we display a bit of the matrix, and also the calculated prior probability for each phoneme. Note that the matrix is showing how often each phoneme-phoneme pair occurred. So AA,AA will be 0 because that sequence does not occur within words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract unique phonemes</span>
<span class="n">unique_phonemes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="k">for</span> <span class="n">pronunciation</span> <span class="ow">in</span> <span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;Pronunciation&#39;</span><span class="p">]:</span>
    <span class="n">phonemes</span> <span class="o">=</span> <span class="n">pronunciation</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">unique_phonemes</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">phonemes</span><span class="p">)</span>
<span class="n">sorted_unique_phonemes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">unique_phonemes</span><span class="p">))</span>
<span class="n">num_phonemes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sorted_unique_phonemes</span><span class="p">)</span>
<span class="n">phoneme_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">phoneme</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">phoneme</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_unique_phonemes</span><span class="p">)}</span>

<span class="c1"># Create a weighted matrix for phoneme pairs</span>
<span class="n">weighted_phoneme_matrix_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="c1"># create dictionary for frequency-weighted counts of each phoneme</span>
<span class="n">phoneme_count_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>


<span class="c1"># Now let&#39;s fill the matrix</span>
<span class="n">fweight</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># how much to use frequency; when set to 1, just use full value</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">lexicon_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">pronunciation</span><span class="p">,</span> <span class="n">lfrq</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Pronunciation&#39;</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;lfrq&#39;</span><span class="p">]</span>
    <span class="n">phonemes</span> <span class="o">=</span> <span class="n">pronunciation</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    
    <span class="c1"># Update the frequency-weighted count for each individual phoneme</span>
    <span class="k">for</span> <span class="n">phoneme</span> <span class="ow">in</span> <span class="n">phonemes</span><span class="p">:</span>
        <span class="n">phoneme_count_dict</span><span class="p">[</span><span class="n">phoneme</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">lfrq</span> <span class="o">*</span> <span class="n">fweight</span><span class="p">)</span>
 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">phonemes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">first_phoneme</span><span class="p">,</span> <span class="n">second_phoneme</span> <span class="o">=</span> <span class="n">phonemes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">phonemes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">weighted_phoneme_matrix_dict</span><span class="p">[(</span><span class="n">first_phoneme</span><span class="p">,</span> <span class="n">second_phoneme</span><span class="p">)]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">lfrq</span> <span class="o">*</span> <span class="n">fweight</span><span class="p">)</span>
        <span class="n">weighted_phoneme_matrix_dict</span><span class="p">[(</span><span class="n">second_phoneme</span><span class="p">,</span> <span class="n">first_phoneme</span><span class="p">)]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">lfrq</span> <span class="o">*</span> <span class="n">fweight</span><span class="p">)</span>


<span class="c1"># Initialize the weighted matrix</span>
<span class="n">weighted_phoneme_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_phonemes</span><span class="p">,</span> <span class="n">num_phonemes</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="k">for</span> <span class="p">(</span><span class="n">first</span><span class="p">,</span> <span class="n">second</span><span class="p">),</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">weighted_phoneme_matrix_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">phoneme_to_index</span><span class="p">[</span><span class="n">first</span><span class="p">],</span> <span class="n">phoneme_to_index</span><span class="p">[</span><span class="n">second</span><span class="p">]</span>
    <span class="n">weighted_phoneme_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">count</span>

<span class="c1"># Convert the matrix to a DataFrame for better readability</span>
<span class="n">weighted_phoneme_matrix_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">weighted_phoneme_matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sorted_unique_phonemes</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sorted_unique_phonemes</span><span class="p">)</span>

<span class="c1"># Convert the phoneme_count_dict to a DataFrame for better readability</span>
<span class="n">phoneme_count_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">phoneme_count_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Phoneme&#39;</span><span class="p">,</span> <span class="s1">&#39;FrequencyWeightedCount&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;FrequencyWeightedCount&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Calculate the sum of all FrequencyWeightedCount values</span>
<span class="n">total_count</span> <span class="o">=</span> <span class="n">phoneme_count_df</span><span class="p">[</span><span class="s1">&#39;FrequencyWeightedCount&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="c1"># Add a new column for normalized probabilities</span>
<span class="n">phoneme_count_df</span><span class="p">[</span><span class="s1">&#39;phon_prob&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">phoneme_count_df</span><span class="p">[</span><span class="s1">&#39;FrequencyWeightedCount&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">total_count</span>

<span class="c1"># Create a simpler dictionary that just pairs phonemes with their normalized probabilities (phon_prob)</span>
<span class="n">phoneme_prob_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">phoneme_count_df</span><span class="p">[</span><span class="s1">&#39;Phoneme&#39;</span><span class="p">],</span> <span class="n">phoneme_count_df</span><span class="p">[</span><span class="s1">&#39;phon_prob&#39;</span><span class="p">]))</span>
<span class="n">phoneme_prob_dict</span>

<span class="c1"># Display a portion of the weighted matrix and phoneme count matrix for review</span>
<span class="n">weighted_phoneme_matrix_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:</span><span class="mi">5</span><span class="p">],</span> <span class="n">phoneme_prob_dict</span><span class="c1">#,phoneme_count_df.iloc[:100, :10]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(            AA          AE           AH         AO         AW
 AA    0.000000    0.000000     0.000000   0.000000   0.000000
 AE    0.000000    0.000000     0.000000   0.000000   0.000000
 AH    0.000000    0.000000     1.280758   1.350667  39.343166
 AO    0.000000    0.000000     1.350667   0.000000   0.000000
 AW    0.000000    0.000000    39.343166   0.000000   0.000000
 AY    7.207485    5.742495   102.973433   2.414126   0.000000
 B   185.558680  283.949295  1000.629454  54.270354  24.262590
 CH   40.617086  108.511521   210.936280   5.700518  10.245048
 D   134.052686  186.728707   844.436416  55.920047  56.054154
 DH    8.222988   14.214397    90.675537   0.000000   3.154017,
 {&#39;AH&#39;: 0.1041717568844594,
  &#39;R&#39;: 0.08762797978723763,
  &#39;IH&#39;: 0.0799592708972376,
  &#39;T&#39;: 0.06813767731764012,
  &#39;N&#39;: 0.06763029365957593,
  &#39;S&#39;: 0.06098663594854901,
  &#39;L&#39;: 0.05478876040112506,
  &#39;K&#39;: 0.04727856415076621,
  &#39;D&#39;: 0.03389393117466113,
  &#39;IY&#39;: 0.033529270909775345,
  &#39;P&#39;: 0.033084815530742835,
  &#39;M&#39;: 0.030582308953022393,
  &#39;EH&#39;: 0.028678651231096634,
  &#39;AE&#39;: 0.026828697625210646,
  &#39;B&#39;: 0.020968823283849955,
  &#39;EY&#39;: 0.019224852918350188,
  &#39;F&#39;: 0.01825658287079943,
  &#39;AA&#39;: 0.017992744655139652,
  &#39;OW&#39;: 0.015183788981014234,
  &#39;AY&#39;: 0.014523540559943045,
  &#39;V&#39;: 0.013862097241048121,
  &#39;NG&#39;: 0.013816409135661099,
  &#39;SH&#39;: 0.013456726488212731,
  &#39;G&#39;: 0.011597370300915441,
  &#39;UW&#39;: 0.011006863236902724,
  &#39;AO&#39;: 0.009862089327191868,
  &#39;W&#39;: 0.008890755716750316,
  &#39;JH&#39;: 0.008844950873695032,
  &#39;Z&#39;: 0.008484306531619308,
  &#39;HH&#39;: 0.008362809790146416,
  &#39;CH&#39;: 0.007017499600829476,
  &#39;Y&#39;: 0.0059719301052602895,
  &#39;AW&#39;: 0.004176772179213557,
  &#39;UH&#39;: 0.0038225078344894655,
  &#39;TH&#39;: 0.003810499381643429,
  &#39;OY&#39;: 0.001537476163463332,
  &#39;DH&#39;: 0.0010990804003592981,
  &#39;ZH&#39;: 0.0010509079524016942})
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">probability_for_AH</span> <span class="o">=</span> <span class="n">phoneme_prob_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;AH&quot;</span><span class="p">,</span> <span class="s2">&quot;Phoneme not found&quot;</span><span class="p">)</span>
<span class="n">probability_for_AH</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.1041717568844594
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="converting-counts-to-similarities">
<h3>Converting counts to similarities<a class="headerlink" href="#converting-counts-to-similarities" title="Link to this heading">#</a></h3>
<p>Once we have the matrix, now we can take the rows or columns as ‘representations’ of each phoneme, in terms of how often the phoneme co-occurs with other phonemes. To get pairwise similarity, we could compare the Euclidean distance of the vectors. Another approach is to use <em>cosine</em> similarity.</p>
</section>
</section>
<section id="cosine-similarity">
<h2>Cosine Similarity<a class="headerlink" href="#cosine-similarity" title="Link to this heading">#</a></h2>
<p>There is a new (in 2024) section earlier on that discusses cosine simmilarity. I gave code with a function that provides vector cosine. But we can also just get it from the <code class="docutils literal notranslate"><span class="pre">sci-kit-learn</span></code> package, as in the code block below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="c1"># Compute the cosine similarity between each pair of phoneme vectors</span>
<span class="n">cosine_sim_matrix</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">weighted_phoneme_matrix</span><span class="p">)</span>

<span class="c1"># Convert the similarity matrix to a DataFrame for better readability</span>
<span class="n">cosine_sim_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cosine_sim_matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sorted_unique_phonemes</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sorted_unique_phonemes</span><span class="p">)</span>

<span class="c1"># Display a portion of the cosine similarity matrix</span>
<span class="n">cosine_sim_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AA</th>
      <th>AE</th>
      <th>AH</th>
      <th>AO</th>
      <th>AW</th>
      <th>AY</th>
      <th>B</th>
      <th>CH</th>
      <th>D</th>
      <th>DH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AA</th>
      <td>1.000000</td>
      <td>0.966031</td>
      <td>0.931129</td>
      <td>0.848900</td>
      <td>0.757327</td>
      <td>0.785100</td>
      <td>0.258615</td>
      <td>0.311287</td>
      <td>0.359275</td>
      <td>0.116166</td>
    </tr>
    <tr>
      <th>AE</th>
      <td>0.966031</td>
      <td>1.000000</td>
      <td>0.948020</td>
      <td>0.808589</td>
      <td>0.845911</td>
      <td>0.856638</td>
      <td>0.242135</td>
      <td>0.328735</td>
      <td>0.371903</td>
      <td>0.096337</td>
    </tr>
    <tr>
      <th>AH</th>
      <td>0.931129</td>
      <td>0.948020</td>
      <td>1.000000</td>
      <td>0.892759</td>
      <td>0.798543</td>
      <td>0.891911</td>
      <td>0.295744</td>
      <td>0.334902</td>
      <td>0.415630</td>
      <td>0.130527</td>
    </tr>
    <tr>
      <th>AO</th>
      <td>0.848900</td>
      <td>0.808589</td>
      <td>0.892759</td>
      <td>1.000000</td>
      <td>0.566249</td>
      <td>0.766836</td>
      <td>0.306023</td>
      <td>0.243556</td>
      <td>0.328706</td>
      <td>0.143260</td>
    </tr>
    <tr>
      <th>AW</th>
      <td>0.757327</td>
      <td>0.845911</td>
      <td>0.798543</td>
      <td>0.566249</td>
      <td>1.000000</td>
      <td>0.814663</td>
      <td>0.268305</td>
      <td>0.461216</td>
      <td>0.487079</td>
      <td>0.183749</td>
    </tr>
    <tr>
      <th>AY</th>
      <td>0.785100</td>
      <td>0.856638</td>
      <td>0.891911</td>
      <td>0.766836</td>
      <td>0.814663</td>
      <td>1.000000</td>
      <td>0.358534</td>
      <td>0.364591</td>
      <td>0.413384</td>
      <td>0.206859</td>
    </tr>
    <tr>
      <th>B</th>
      <td>0.258615</td>
      <td>0.242135</td>
      <td>0.295744</td>
      <td>0.306023</td>
      <td>0.268305</td>
      <td>0.358534</td>
      <td>1.000000</td>
      <td>0.820980</td>
      <td>0.765612</td>
      <td>0.933101</td>
    </tr>
    <tr>
      <th>CH</th>
      <td>0.311287</td>
      <td>0.328735</td>
      <td>0.334902</td>
      <td>0.243556</td>
      <td>0.461216</td>
      <td>0.364591</td>
      <td>0.820980</td>
      <td>1.000000</td>
      <td>0.950619</td>
      <td>0.757804</td>
    </tr>
    <tr>
      <th>D</th>
      <td>0.359275</td>
      <td>0.371903</td>
      <td>0.415630</td>
      <td>0.328706</td>
      <td>0.487079</td>
      <td>0.413384</td>
      <td>0.765612</td>
      <td>0.950619</td>
      <td>1.000000</td>
      <td>0.692227</td>
    </tr>
    <tr>
      <th>DH</th>
      <td>0.116166</td>
      <td>0.096337</td>
      <td>0.130527</td>
      <td>0.143260</td>
      <td>0.183749</td>
      <td>0.206859</td>
      <td>0.933101</td>
      <td>0.757804</td>
      <td>0.692227</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can see that self-similarity (cosine) is 1.0 for all phonemes. We can also see that some pairs have extremely high cosines – e.g., <span class="math notranslate nohighlight">\([/AA/, /AE/]= 0.966031\)</span>. This is probably too high. Let’s downweight the non-identity similarities by dividing them by a constant.</p>
<p><strong>Note</strong>: This is a very consequential decision (dividing by 5). We may want to try smaller or larger values…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adjust the diagonal entries from 1.0 to 0.9</span>
<span class="c1">#np.fill_diagonal(cosine_sim_matrix, 0.9)</span>

<span class="c1"># Reduce the off-diagonal entries -- divide by 5 for now</span>
<span class="n">off_diagonal_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cosine_sim_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>
<span class="n">adjusted_cosine_sim_matrix</span> <span class="o">=</span> <span class="n">cosine_sim_matrix</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">adjusted_cosine_sim_matrix</span><span class="p">[</span><span class="n">off_diagonal_indices</span><span class="p">]</span> <span class="o">/=</span> <span class="mf">5.0</span>

<span class="c1"># Convert the adjusted similarity matrix to a DataFrame for better readability</span>
<span class="n">adjusted_cosine_sim_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">adjusted_cosine_sim_matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sorted_unique_phonemes</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sorted_unique_phonemes</span><span class="p">)</span>

<span class="c1"># Display a portion of the adjusted cosine similarity matrix</span>
<span class="n">adjusted_cosine_sim_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AA</th>
      <th>AE</th>
      <th>AH</th>
      <th>AO</th>
      <th>AW</th>
      <th>AY</th>
      <th>B</th>
      <th>CH</th>
      <th>D</th>
      <th>DH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AA</th>
      <td>1.000000</td>
      <td>0.193206</td>
      <td>0.186226</td>
      <td>0.169780</td>
      <td>0.151465</td>
      <td>0.157020</td>
      <td>0.051723</td>
      <td>0.062257</td>
      <td>0.071855</td>
      <td>0.023233</td>
    </tr>
    <tr>
      <th>AE</th>
      <td>0.193206</td>
      <td>1.000000</td>
      <td>0.189604</td>
      <td>0.161718</td>
      <td>0.169182</td>
      <td>0.171328</td>
      <td>0.048427</td>
      <td>0.065747</td>
      <td>0.074381</td>
      <td>0.019267</td>
    </tr>
    <tr>
      <th>AH</th>
      <td>0.186226</td>
      <td>0.189604</td>
      <td>1.000000</td>
      <td>0.178552</td>
      <td>0.159709</td>
      <td>0.178382</td>
      <td>0.059149</td>
      <td>0.066980</td>
      <td>0.083126</td>
      <td>0.026105</td>
    </tr>
    <tr>
      <th>AO</th>
      <td>0.169780</td>
      <td>0.161718</td>
      <td>0.178552</td>
      <td>1.000000</td>
      <td>0.113250</td>
      <td>0.153367</td>
      <td>0.061205</td>
      <td>0.048711</td>
      <td>0.065741</td>
      <td>0.028652</td>
    </tr>
    <tr>
      <th>AW</th>
      <td>0.151465</td>
      <td>0.169182</td>
      <td>0.159709</td>
      <td>0.113250</td>
      <td>1.000000</td>
      <td>0.162933</td>
      <td>0.053661</td>
      <td>0.092243</td>
      <td>0.097416</td>
      <td>0.036750</td>
    </tr>
    <tr>
      <th>AY</th>
      <td>0.157020</td>
      <td>0.171328</td>
      <td>0.178382</td>
      <td>0.153367</td>
      <td>0.162933</td>
      <td>1.000000</td>
      <td>0.071707</td>
      <td>0.072918</td>
      <td>0.082677</td>
      <td>0.041372</td>
    </tr>
    <tr>
      <th>B</th>
      <td>0.051723</td>
      <td>0.048427</td>
      <td>0.059149</td>
      <td>0.061205</td>
      <td>0.053661</td>
      <td>0.071707</td>
      <td>1.000000</td>
      <td>0.164196</td>
      <td>0.153122</td>
      <td>0.186620</td>
    </tr>
    <tr>
      <th>CH</th>
      <td>0.062257</td>
      <td>0.065747</td>
      <td>0.066980</td>
      <td>0.048711</td>
      <td>0.092243</td>
      <td>0.072918</td>
      <td>0.164196</td>
      <td>1.000000</td>
      <td>0.190124</td>
      <td>0.151561</td>
    </tr>
    <tr>
      <th>D</th>
      <td>0.071855</td>
      <td>0.074381</td>
      <td>0.083126</td>
      <td>0.065741</td>
      <td>0.097416</td>
      <td>0.082677</td>
      <td>0.153122</td>
      <td>0.190124</td>
      <td>1.000000</td>
      <td>0.138445</td>
    </tr>
    <tr>
      <th>DH</th>
      <td>0.023233</td>
      <td>0.019267</td>
      <td>0.026105</td>
      <td>0.028652</td>
      <td>0.036750</td>
      <td>0.041372</td>
      <td>0.186620</td>
      <td>0.151561</td>
      <td>0.138445</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>This looks better. Let’s normalize the values though to treat them like probabilities.</p>
<p>(<em><strong>Why would we do this?</strong></em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Normalize the rows of the adjusted cosine similarity matrix so they sum to 1</span>
<span class="n">row_sums</span> <span class="o">=</span> <span class="n">adjusted_cosine_sim_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">normalized_cosine_sim_matrix</span> <span class="o">=</span> <span class="n">adjusted_cosine_sim_matrix</span> <span class="o">/</span> <span class="n">row_sums</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

<span class="c1"># Convert the normalized similarity matrix to a DataFrame for better readability</span>
<span class="n">normalized_cosine_sim_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">normalized_cosine_sim_matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sorted_unique_phonemes</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sorted_unique_phonemes</span><span class="p">)</span>

<span class="c1"># Display a portion of the normalized cosine similarity matrix</span>
<span class="n">normalized_cosine_sim_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AA</th>
      <th>AE</th>
      <th>AH</th>
      <th>AO</th>
      <th>AW</th>
      <th>AY</th>
      <th>B</th>
      <th>CH</th>
      <th>D</th>
      <th>DH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>AA</th>
      <td>0.231184</td>
      <td>0.044666</td>
      <td>0.043052</td>
      <td>0.039250</td>
      <td>0.035016</td>
      <td>0.036300</td>
      <td>0.011958</td>
      <td>0.014393</td>
      <td>0.016612</td>
      <td>0.005371</td>
    </tr>
    <tr>
      <th>AE</th>
      <td>0.043542</td>
      <td>0.225367</td>
      <td>0.042730</td>
      <td>0.036446</td>
      <td>0.038128</td>
      <td>0.038612</td>
      <td>0.010914</td>
      <td>0.014817</td>
      <td>0.016763</td>
      <td>0.004342</td>
    </tr>
    <tr>
      <th>AH</th>
      <td>0.041180</td>
      <td>0.041927</td>
      <td>0.221132</td>
      <td>0.039483</td>
      <td>0.035317</td>
      <td>0.039446</td>
      <td>0.013080</td>
      <td>0.014811</td>
      <td>0.018382</td>
      <td>0.005773</td>
    </tr>
    <tr>
      <th>AO</th>
      <td>0.041679</td>
      <td>0.039700</td>
      <td>0.043833</td>
      <td>0.245491</td>
      <td>0.027802</td>
      <td>0.037650</td>
      <td>0.015025</td>
      <td>0.011958</td>
      <td>0.016139</td>
      <td>0.007034</td>
    </tr>
    <tr>
      <th>AW</th>
      <td>0.034274</td>
      <td>0.038283</td>
      <td>0.036140</td>
      <td>0.025627</td>
      <td>0.226284</td>
      <td>0.036869</td>
      <td>0.012143</td>
      <td>0.020873</td>
      <td>0.022044</td>
      <td>0.008316</td>
    </tr>
    <tr>
      <th>AY</th>
      <td>0.032920</td>
      <td>0.035920</td>
      <td>0.037399</td>
      <td>0.032154</td>
      <td>0.034160</td>
      <td>0.209655</td>
      <td>0.015034</td>
      <td>0.015288</td>
      <td>0.017334</td>
      <td>0.008674</td>
    </tr>
    <tr>
      <th>B</th>
      <td>0.009705</td>
      <td>0.009087</td>
      <td>0.011098</td>
      <td>0.011484</td>
      <td>0.010069</td>
      <td>0.013455</td>
      <td>0.187634</td>
      <td>0.030809</td>
      <td>0.028731</td>
      <td>0.035016</td>
    </tr>
    <tr>
      <th>CH</th>
      <td>0.011055</td>
      <td>0.011675</td>
      <td>0.011894</td>
      <td>0.008650</td>
      <td>0.016379</td>
      <td>0.012948</td>
      <td>0.029156</td>
      <td>0.177568</td>
      <td>0.033760</td>
      <td>0.026912</td>
    </tr>
    <tr>
      <th>D</th>
      <td>0.012692</td>
      <td>0.013139</td>
      <td>0.014683</td>
      <td>0.011613</td>
      <td>0.017208</td>
      <td>0.014604</td>
      <td>0.027048</td>
      <td>0.033583</td>
      <td>0.176640</td>
      <td>0.024455</td>
    </tr>
    <tr>
      <th>DH</th>
      <td>0.004986</td>
      <td>0.004135</td>
      <td>0.005602</td>
      <td>0.006149</td>
      <td>0.007886</td>
      <td>0.008878</td>
      <td>0.040048</td>
      <td>0.032524</td>
      <td>0.029710</td>
      <td>0.214597</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>*When we divide non-identity values by 2, the normalized values wind up being around 0.1 for identity and 0.05 for highly similar items. These may be too similar, so I’ve instead divided by 5. This makes the identity values around 0.22 and high sim items 0.04. Might not be perfect, but let’s try this. *</p>
<hr class="docutils" />
<section id="visualizing-similarity">
<h3>Visualizing similarity<a class="headerlink" href="#visualizing-similarity" title="Link to this heading">#</a></h3>
<p>A simple and effective way to visualize similarity when we have a matrix is to use a <em>heatmap</em>, where color indicates value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># let&#39;s visualize it</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Set up the matplotlib figure</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># Draw the heatmap</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;coolwarm&quot;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Add labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Normalized adjusted Cosine Similarity Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Phonemes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Phonemes&quot;</span><span class="p">)</span>

<span class="c1"># Adjust the font size of the tick labels</span>
<span class="c1"># plt.xticks(fontsize=6)</span>
<span class="c1"># plt.yticks(fontsize=8)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/53fbf20dfd0637ecc6b9e02de8b9c5a1e7b9aa3245900a03d6ee2385cf554b04.png" src="_images/53fbf20dfd0637ecc6b9e02de8b9c5a1e7b9aa3245900a03d6ee2385cf554b04.png" />
</div>
</div>
<p>So we can definitely see structure here. But phonemes are sorted alphabetically. Could we resort in some way to put similar things near similar things? Let’s use a standard <em>clustering</em> technique called <em>hierarchical clustering</em> (which will reorder the columns so that similar items are near each other) and then recreate the heatmap.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s cluster first</span>

<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">leaves_list</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Perform hierarchical clustering</span>
<span class="n">link</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;average&#39;</span><span class="p">)</span>

<span class="c1"># Get the order of rows according to the hierarchy</span>
<span class="n">row_order</span> <span class="o">=</span> <span class="n">leaves_list</span><span class="p">(</span><span class="n">link</span><span class="p">)</span>

<span class="c1"># Reorder the DataFrame</span>
<span class="n">sorted_unique_phonemes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sorted_unique_phonemes</span><span class="p">)</span>  <span class="c1"># Assuming it&#39;s a list; convert to NumPy array</span>
<span class="n">sorted_unique_phonemes</span> <span class="o">=</span> <span class="n">sorted_unique_phonemes</span><span class="p">[</span><span class="n">row_order</span><span class="p">]</span>

<span class="c1"># Reorder rows and columns based on hierarchical clustering</span>
<span class="n">reordered_df</span> <span class="o">=</span> <span class="n">normalized_cosine_sim_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">sorted_unique_phonemes</span><span class="p">,</span> <span class="n">sorted_unique_phonemes</span><span class="p">]</span>

<span class="c1"># Create the heatmap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">reordered_df</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Reordered Normalized Adjusted Cosine Similarity Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Phonemes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Phonemes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3ccdb0785961f4b4567530f7582c70b63094d5b9e88b599d68b4c4da763f9619.png" src="_images/3ccdb0785961f4b4567530f7582c70b63094d5b9e88b599d68b4c4da763f9619.png" />
</div>
</div>
<p>Much better! we can now see consonants and vowels tend to be grouped together. However, there’s a big gap between identity (.2-.3) and high cosine values (which we scaled down to less than 0.05). This means we are not using the color scale completely. Let’s leave out the high values and see if we can improve the heatmap.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s leave the identity positions out to make the scale smaller</span>

<span class="c1"># Set the diagonal entries to NaN</span>
<span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">reordered_df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
<span class="c1"># Create the heatmap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">reordered_df</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">reordered_df</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Reordered Normalized Adjusted Cosine Similarity Matrix (Diagonal Excluded)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Phonemes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Phonemes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2e75a000cfae7fd48670986de7bd524d9874062fc211ae804b0352294a1c7a37.png" src="_images/2e75a000cfae7fd48670986de7bd524d9874062fc211ae804b0352294a1c7a37.png" />
</div>
</div>
<p>Much better again; now we can really see the structure (unfortunately, UH is getting grouped between NG and HH rather than after K…). It may be that this is not a good basis for estimating human perceptual similarity, but it seems to get the basics correct.</p>
</section>
<hr class="docutils" />
<section id="bayesian-word-recogntion-simulation">
<h3>Bayesian word recogntion simulation<a class="headerlink" href="#bayesian-word-recogntion-simulation" title="Link to this heading">#</a></h3>
<p>We are now ready to try to simulate words using Bayesian principles.</p>
<p>The function(s) below carry out calculations according to the principles we reviewed at the start of this notebook.</p>
<p>Note the <code class="docutils literal notranslate"><span class="pre">topX</span></code> variable. Norris and McQueen say it is fine to compute posteriors over the whole lexicon, because most words will have low evidence, and they limit to the top 50 words simply for convenience. However, when you have 17,000 words and all have non-zero evidence, even miniscule evidence 17,000 times adds up to a lot. So the <code class="docutils literal notranslate"><span class="pre">topX</span></code> value actually matters a lot. If we don’t make this a pretty small number (like 50 relative to 17,000) our probabilities will be tiny…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">get_phoneme_prob_at_position</span><span class="p">(</span><span class="n">phoneme</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">):</span>
<span class="w">    </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the bottom-up probability for a phoneme at a given position.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        phoneme (str): The current phoneme.</span>
<span class="sd">        normalized_cosine_sim_df (DataFrame): The DataFrame containing the normalized cosine similarities between phonemes.</span>
<span class="sd">        phoneme_prob_dict (dict): The dictionary containing the probabilities of each phoneme.</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        pd.Series: The probabilities of each phoneme at the current position.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">evidence_distribution</span> <span class="o">=</span> <span class="n">normalized_cosine_sim_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">phoneme</span><span class="p">]</span>
    
    <span class="c1"># Instead of multiplying the whole Series with a dictionary, use map for element-wise multiplication</span>
    <span class="n">evidence_distribution</span> <span class="o">=</span> <span class="n">evidence_distribution</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">phoneme_prob_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># normalize (back to probabilities) and return</span>
    <span class="k">return</span> <span class="n">evidence_distribution</span> <span class="o">/</span> <span class="n">evidence_distribution</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">sim_bayes_new</span><span class="p">(</span><span class="n">target_word</span><span class="p">,</span> <span class="n">lexicon_df</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">,</span> <span class="n">topX</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simulate word recognition based on Bayesian inference.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        target_word (str): The target word to recognize.</span>
<span class="sd">        lexicon_df (DataFrame): The DataFrame containing the lexicon.</span>
<span class="sd">        normalized_cosine_sim_df (DataFrame): The DataFrame containing the normalized cosine similarities between phonemes.</span>
<span class="sd">        phoneme_prob_dict (dict): The dictionary containing the probabilities of each phoneme.</span>
<span class="sd">        topX: how many words to retain in the dictionary based on peak evidence values</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        posterior_word_df, posterior_phon_df: 2 dataframes with word and phoneme probabilities by position</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;--- Starting simulation of word </span><span class="si">{</span><span class="n">target_word</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="c1"># let&#39;s get the phonemes for the target word</span>
    <span class="c1"># this just pulls the Pronunciation for the target word</span>
    <span class="n">target_pronunciation</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">target_word</span><span class="p">,</span> <span class="s1">&#39;Pronunciation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">target_phonemes</span> <span class="o">=</span> <span class="n">target_pronunciation</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">result_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">phoneme_prob_list</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># New list to store phoneme probabilities</span>

    <span class="c1"># Print the elapsed time -- nothing essential, just updating the user</span>
    <span class="c1"># part1 = time.time()</span>
    <span class="c1"># elapsed_time = part1 - start_time</span>
    <span class="c1"># print(f&quot;  Part 1a: {elapsed_time:.6f} seconds&quot;)</span>
    
    <span class="c1"># this is wasteful to compute every time, but it takes very little time</span>
    <span class="c1"># we need the sum of all lfrq values to convert to probabilities</span>
    <span class="n">total_frequency</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;lfrq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    
    <span class="c1"># now we divide the lfrq values by total_frequency</span>
    <span class="c1"># we go ahead and do it in word_prob_dict because it has local scope -- we </span>
    <span class="c1"># are not changing the lexicon outside the function, just the &#39;copy&#39; we have</span>
    <span class="c1"># inside the function (could make a version of this outside the function and</span>
    <span class="c1"># pass it inside and then skip this normalization step...)</span>
    <span class="n">word_prob_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]:</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;lfrq&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">total_frequency</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">lexicon_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()}</span>

    <span class="c1"># Print the elapsed time -- just keeping user updated, and trying to figure</span>
    <span class="c1"># out how long each part is taking</span>
    <span class="c1"># part2 = time.time()</span>
    <span class="c1"># elapsed_time = part2 - part1</span>
    <span class="c1"># print(f&quot;  Part 1b: {elapsed_time:.6f} seconds&quot;)</span>

    <span class="c1"># Print the elapsed time from start_time so far</span>
    <span class="c1"># elapsed_time = time.time() - start_time</span>
    <span class="c1"># print(f&quot;  Times so far: {elapsed_time:.6f} seconds&quot;)</span>

    <span class="c1"># Loop through each phoneme in the target word</span>
    <span class="k">for</span> <span class="n">phoneme_pos</span><span class="p">,</span> <span class="n">phoneme</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">target_phonemes</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">phon_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Step 1: Get the bottom-up probability for the phoneme</span>
        <span class="n">phoneme_prob_at_pos</span> <span class="o">=</span> <span class="n">get_phoneme_prob_at_position</span><span class="p">(</span><span class="n">phoneme</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">)</span>
        
        <span class="c1"># Store the phoneme probabilities in the new list</span>
        <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">phoneme_prob_at_pos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">phoneme_prob_list</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;Phoneme Position&#39;</span><span class="p">:</span> <span class="n">phoneme_pos</span><span class="p">,</span> <span class="s1">&#39;Phoneme&#39;</span><span class="p">:</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;Probability&#39;</span><span class="p">:</span> <span class="n">prob</span><span class="p">})</span>
            <span class="c1"># for debugging / inspection -- uncomment next line to see the phoneme probabilities at </span>
            <span class="c1"># each phoneme_pos</span>
            <span class="c1">#print(f&#39;     ##### Phoneme Position {phoneme_pos}, Phoneme {p}, Probability {prob}&#39;)</span>

        <span class="c1"># Step 2: Calculate the probability of each word&#39;s substrings (pseq)</span>
        <span class="n">updated_prob_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">word_pronunciation</span> <span class="ow">in</span> <span class="n">lexicon_df</span><span class="p">[[</span><span class="s1">&#39;Item&#39;</span><span class="p">,</span> <span class="s1">&#39;Pronunciation&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
            <span class="n">word_phonemes</span> <span class="o">=</span> <span class="n">word_pronunciation</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
            
            <span class="c1"># this is brute force way to deal with words that are shorter than </span>
            <span class="c1"># the current word -- we just ignore them once the input is longer </span>
            <span class="c1"># than their length... Maybe this is not the best idea? To look at</span>
            <span class="c1"># later... </span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_phonemes</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">phoneme_pos</span><span class="p">:</span>
                <span class="k">continue</span>
            
            <span class="c1"># now we want pseq to be the product of the phoneme probabilities </span>
            <span class="c1"># for the current word at each position. We set it to 1.0 initially</span>
            <span class="c1"># so we can multiply it by the probabilities... This is the likelihood</span>
            <span class="c1"># step: 𝐿𝑖𝑘𝑒𝑙𝑖ℎ𝑜𝑜𝑑(𝑊𝑜𝑟𝑑𝑖)=𝑃(𝐸𝑣𝑖𝑑𝑒𝑛𝑐𝑒|𝑊𝑜𝑟𝑑𝑖)=𝑃(𝑃ℎ𝑜𝑛𝑒𝑚𝑒𝑆𝑡𝑟𝑖𝑛𝑔𝑖)=∏𝑗=1𝑙𝑃(𝑃ℎ𝑜𝑛𝑒𝑚𝑒𝑗|𝐸𝑣𝑖𝑑𝑒𝑛𝑐𝑒)</span>
            <span class="n">pseq</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">phoneme_pos</span><span class="p">):</span>
                <span class="n">pseq</span> <span class="o">*=</span> <span class="n">phoneme_prob_at_pos</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word_phonemes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
                
            <span class="c1"># Step 3: Multiply by word probability and update</span>
            <span class="c1"># We take the pseq product and multiply by the word&#39;s probability</span>
            <span class="c1"># this is the step of 𝑃(𝐸𝑣𝑖𝑑𝑒𝑛𝑐𝑒|𝑊𝑜𝑟𝑑𝑖)×𝑃(𝑊𝑜𝑟𝑑𝑖) for 𝑊𝑜𝑟𝑑𝑖</span>
            <span class="n">updated_prob_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">pseq</span> <span class="o">*</span> <span class="n">word_prob_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        
        <span class="c1"># Diagnostic prints for phoneme position 1</span>
        <span class="c1"># if phoneme_pos == 1:</span>
        <span class="n">prob_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">updated_prob_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="c1"># print(f&quot;Statistics before normalization at phoneme {phoneme_pos}:&quot;)</span>
        <span class="c1"># print(f&quot;  Max: {max(prob_values):.15f}&quot;)</span>
        <span class="c1"># print(f&quot;  Min: {min(prob_values):.15f}&quot;)</span>
        <span class="c1"># print(f&quot;  Mean: {sum(prob_values) / len(prob_values):.15f}&quot;)</span>
        <span class="c1"># print(f&quot;  Std: {sum((x - sum(prob_values) / len(prob_values))**2 for x in prob_values)**0.5 / len(prob_values):.15f}&quot;)</span>

        <span class="c1"># Sort by probability and take only top X words</span>
        <span class="n">sorted_words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">updated_prob_dict</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="n">topX</span><span class="p">]</span>
        <span class="c1"># Always include the target_word</span>
        <span class="k">if</span> <span class="n">target_word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">sorted_words</span><span class="p">]:</span>
            <span class="n">sorted_words</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">target_word</span><span class="p">,</span> <span class="n">updated_prob_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">target_word</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>

        <span class="n">total_prob</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">prob</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">sorted_words</span><span class="p">])</span>
        
        <span class="c1"># Normalize only top X words</span>
        <span class="n">updated_prob_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">prob</span> <span class="o">/</span> <span class="n">total_prob</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">sorted_words</span><span class="p">}</span>

        <span class="c1"># Update lexicon to only include these top X words for the next phoneme</span>
        <span class="c1"># --- note that this could be highly problematic -- we are only allowing words </span>
        <span class="c1"># --- to be considered at later positions if they were in the topX set at previous</span>
        <span class="c1"># --- positions. Could compare results using a much larger topX or even entire lexicon...</span>
        <span class="n">lexicon_df</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="p">[</span><span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">sorted_words</span><span class="p">])]</span>
        
        <span class="c1"># Normalize the updated probabilities</span>
        <span class="n">total_prob</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">updated_prob_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">updated_prob_dict</span><span class="p">:</span>
            <span class="n">updated_prob_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">/=</span> <span class="n">total_prob</span>
        
        <span class="c1"># Diagnostic prints for phoneme position 1</span>
        <span class="c1"># if phoneme_pos == 1:</span>
        <span class="c1"># prob_values = list(updated_prob_dict.values())</span>
        <span class="c1"># print(f&quot;Statistics POST normalization at phoneme {phoneme_pos}:&quot;)</span>
        <span class="c1"># print(f&quot;  Max: {max(prob_values):.15f}&quot;)</span>
        <span class="c1"># print(f&quot;  Min: {min(prob_values):.15f}&quot;)</span>
        <span class="c1"># print(f&quot;  Mean: {sum(prob_values) / len(prob_values):.15f}&quot;)</span>
        <span class="c1"># print(f&quot;  Std: {sum((x - sum(prob_values) / len(prob_values))**2 for x in prob_values)**0.5 / len(prob_values):.15f}&quot;)</span>

        <span class="c1"># Update word_prob_dict for next iteration</span>
        <span class="n">word_prob_dict</span> <span class="o">=</span> <span class="n">updated_prob_dict</span>
        
        <span class="c1"># Create a DataFrame for this phoneme position and append to the result list</span>
        <span class="n">temp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">word_prob_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">,</span> <span class="s1">&#39;Probability&#39;</span><span class="p">])</span>
        <span class="n">temp_df</span> <span class="o">=</span> <span class="n">temp_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Probability&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">temp_df</span><span class="p">[</span><span class="s1">&#39;Phoneme Position&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">phoneme_pos</span>
        <span class="n">result_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_df</span><span class="p">)</span>
        
        <span class="c1"># Print the elapsed time for this phoneme</span>
        <span class="c1"># elapsed_time = time.time() - phon_start</span>
        <span class="c1"># print(f&quot;      Time elapsed after processing phoneme {phoneme_pos}: {elapsed_time:.6f} seconds&quot;)        </span>
        
    <span class="c1"># New DataFrame for phoneme probabilities</span>
    <span class="n">posterior_phon_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">phoneme_prob_list</span><span class="p">)</span>
    
    <span class="c1"># Combine all the DataFrames into one and return </span>
    <span class="c1"># [ignore_index=True speeds things by disregarding existing index values]</span>
    <span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">result_list</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Calculate peak activations for each word</span>
    <span class="n">peak_activations</span> <span class="o">=</span> <span class="n">result_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Item&#39;</span><span class="p">)[</span><span class="s1">&#39;Probability&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    
    <span class="c1"># Select the target topX words by peak activation</span>
    <span class="n">top_words</span> <span class="o">=</span> <span class="n">peak_activations</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">topX</span><span class="p">)</span>
    
    <span class="c1"># Filter final results to only include these top words</span>
    <span class="n">posterior_word_df</span> <span class="o">=</span> <span class="n">result_df</span><span class="p">[</span><span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">top_words</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">])]</span>
    
    <span class="c1"># Create an ordered list of words based on peak activations</span>
    <span class="n">sorted_words_by_peak</span> <span class="o">=</span> <span class="n">top_words</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Move the target word to the top of the list</span>
    <span class="n">sorted_words_by_peak</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">target_word</span><span class="p">)</span>
    <span class="n">sorted_words_by_peak</span> <span class="o">=</span> <span class="p">[</span><span class="n">target_word</span><span class="p">]</span> <span class="o">+</span> <span class="n">sorted_words_by_peak</span>
    
    <span class="c1"># Create a dictionary to map each word to its index in sorted_words_by_peak, formatted as a string</span>
    <span class="n">sort_order</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">index</span><span class="si">:</span><span class="s2">02d</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_words_by_peak</span><span class="p">)}</span>
    
    <span class="c1"># Add a new column that indicates the sort order based on peak activations</span>
    <span class="n">posterior_word_df</span> <span class="o">=</span> <span class="n">posterior_word_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">posterior_word_df</span><span class="p">[</span><span class="s1">&#39;ItemWithOrder&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">posterior_word_df</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">sort_order</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;99&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Add a target_word column to the final result DataFrames; could help with subsequent analysis</span>
    <span class="n">posterior_word_df</span><span class="p">[</span><span class="s1">&#39;Target Word&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_word</span>
    <span class="n">posterior_phon_df</span><span class="p">[</span><span class="s1">&#39;Target Word&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_word</span>
     
    <span class="c1"># Print the total elapsed time</span>
    <span class="n">total_elapsed_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">... Time for simulation: </span><span class="si">{</span><span class="n">total_elapsed_time</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">posterior_word_df</span><span class="p">,</span> <span class="n">posterior_phon_df</span>  <span class="c1"># Return both DataFrames</span>

<span class="c1"># Uncomment the following lines to test the function once you have all the required DataFrames and dictionaries.</span>
<span class="n">word_result</span><span class="p">,</span> <span class="n">phon_result</span> <span class="o">=</span> <span class="n">sim_bayes_new</span><span class="p">(</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">lexicon_df</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Starting simulation of word cat	... Time for simulation: 0.280275 seconds
</pre></div>
</div>
</div>
</div>
<p>Okay, we have simulated processing of one word! How can we visualize the results? Let’s make a function that will generate a plot from the kind of result we get from our simulation function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>

<span class="k">def</span> <span class="nf">plot_simulation_result</span><span class="p">(</span><span class="n">word_result</span><span class="p">,</span> <span class="n">phon_result</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">full_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">saveplots</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">phonplot</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">wordplot</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots a line graph based on the simulation result DataFrame and phoneme probabilities.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        word_result (DataFrame): The DataFrame containing simulation results.</span>
<span class="sd">        phon_result (DataFrame): The DataFrame containing phoneme probabilities.</span>
<span class="sd">        log_scale (bool): Whether to use a log scale for the Y-axis. Default is False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">unique_words</span> <span class="o">=</span> <span class="n">word_result</span><span class="p">[</span><span class="s1">&#39;Target Word&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">unique_words</span><span class="p">:</span>
        <span class="c1"># Filter data for the current word</span>
        <span class="n">word_data</span> <span class="o">=</span> <span class="n">word_result</span><span class="p">[</span><span class="n">word_result</span><span class="p">[</span><span class="s1">&#39;Target Word&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">word</span><span class="p">]</span>
        <span class="n">phon_data</span> <span class="o">=</span> <span class="n">phon_result</span><span class="p">[</span><span class="n">phon_result</span><span class="p">[</span><span class="s1">&#39;Target Word&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">word</span><span class="p">]</span>
        
        <span class="c1"># Get the phonemes corresponding to the positions in the target word</span>
        <span class="n">target_pronunciation</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">word</span><span class="p">,</span> <span class="s1">&#39;Pronunciation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target_phonemes</span> <span class="o">=</span> <span class="n">target_pronunciation</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        
        <span class="n">marker_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">,</span><span class="s2">&quot;o&quot;</span><span class="p">,</span><span class="s2">&quot;v&quot;</span><span class="p">,</span><span class="s2">&quot;^&quot;</span><span class="p">,</span><span class="s2">&quot;&lt;&quot;</span><span class="p">,</span><span class="s2">&quot;&gt;&quot;</span><span class="p">,</span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="s2">&quot;2&quot;</span><span class="p">,</span><span class="s2">&quot;3&quot;</span><span class="p">,</span><span class="s2">&quot;4&quot;</span><span class="p">,</span><span class="s2">&quot;8&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;s&quot;</span><span class="p">,</span><span class="s2">&quot;p&quot;</span><span class="p">,</span><span class="s2">&quot;P&quot;</span><span class="p">,</span><span class="s2">&quot;*&quot;</span><span class="p">,</span><span class="s2">&quot;h&quot;</span><span class="p">,</span><span class="s2">&quot;H&quot;</span><span class="p">,</span><span class="s2">&quot;+&quot;</span><span class="p">,</span><span class="s2">&quot;x&quot;</span><span class="p">,</span><span class="s2">&quot;X&quot;</span><span class="p">,</span><span class="s2">&quot;D&quot;</span><span class="p">,</span><span class="s2">&quot;d&quot;</span><span class="p">,</span><span class="s2">&quot;|&quot;</span><span class="p">,</span><span class="s2">&quot;_&quot;</span><span class="p">,</span>
                       <span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">]</span>
        
        <span class="n">marker_iter</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">cycle</span><span class="p">(</span><span class="n">marker_list</span><span class="p">)</span>
        
        <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
        
        <span class="c1"># Initialize empty lists for legend labels and handles</span>
        <span class="n">word_legend_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">word_legend_handles</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># First subplot for word probabilities</span>
        <span class="c1"># for name, group in word_data.groupby(&#39;ItemWithOrder&#39;):</span>
        <span class="c1">#     marker = next(marker_iter)</span>
        <span class="c1">#     line, = axs[0].plot(group[&#39;Phoneme Position&#39;], group[&#39;Probability&#39;], marker=marker, markersize=6)</span>
        <span class="c1">#     word_legend_labels.append(name)</span>
        <span class="c1">#     word_legend_handles.append(line)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">word_data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;ItemWithOrder&#39;</span><span class="p">):</span>
            <span class="n">marker</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">marker_iter</span><span class="p">)</span>
            <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;Phoneme Position&#39;</span><span class="p">],</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;Probability&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
            
            <span class="c1"># Get the pronunciation (phoneme string) for the word</span>
            <span class="n">word_spelling</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">word_pronunciation</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">word_spelling</span><span class="p">,</span> <span class="s1">&#39;Pronunciation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            
            <span class="c1"># Update legend label to include phoneme string</span>
            <span class="n">label_with_phonemes</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">word_pronunciation</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">word_legend_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label_with_phonemes</span><span class="p">)</span>
            <span class="n">word_legend_handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        
        <span class="c1"># the wordplot arguments limit the lists to that many items in the legend, even if more lines plot</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">word_legend_handles</span><span class="p">[:</span><span class="n">wordplot</span><span class="p">],</span> <span class="n">word_legend_labels</span><span class="p">[:</span><span class="n">wordplot</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Phoneme Position&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Word probabilities for </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_phonemes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">target_phonemes</span><span class="p">)</span>  <span class="c1"># Update x-axis labels to show phonemes</span>
        
        <span class="c1"># Calculate peak activations for each phoneme</span>
        <span class="n">peak_phoneme_activations</span> <span class="o">=</span> <span class="n">phon_data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Phoneme&#39;</span><span class="p">)[</span><span class="s1">&#39;Probability&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
        
        <span class="c1"># Select the top phonemes by peak activation</span>
        <span class="n">top_phonemes</span> <span class="o">=</span> <span class="n">peak_phoneme_activations</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">phonplot</span><span class="p">)[</span><span class="s1">&#39;Phoneme&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        
        <span class="c1"># Initialize empty lists for legend labels and handles</span>
        <span class="n">phoneme_legend_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">phoneme_legend_handles</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Second subplot for phoneme probabilities</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">phon_data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Phoneme&#39;</span><span class="p">):</span>
            <span class="n">marker</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">marker_iter</span><span class="p">)</span>
            <span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;Phoneme Position&#39;</span><span class="p">],</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;Probability&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
            <span class="c1"># Only add to the legend if the phoneme is among the top phonemes</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">top_phonemes</span><span class="p">:</span>
                <span class="n">phoneme_legend_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
                <span class="n">phoneme_legend_handles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>        
                
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">phoneme_legend_handles</span><span class="p">[:</span><span class="n">phonplot</span><span class="p">],</span> <span class="n">phoneme_legend_labels</span><span class="p">[:</span><span class="n">phonplot</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Phonemes&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Phoneme Position&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Phoneme Probabilities for </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_phonemes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">target_phonemes</span><span class="p">)</span>  <span class="c1"># Update x-axis labels to show phonemes</span>
        
        <span class="k">if</span> <span class="n">log_scale</span><span class="p">:</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

        <span class="c1"># If full_scale is True, set the y-axis to range from 0 to 1</span>
        <span class="k">if</span> <span class="n">full_scale</span><span class="p">:</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">saveplots</span><span class="p">:</span>
            <span class="c1"># Ensure the directory exists</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;plots&#39;</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s1">&#39;plots&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;plots/</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">.png&#39;</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s make a function that will let us simulate many words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">def</span> <span class="nf">run_simulations</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="n">lexicon_df</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">,</span> <span class="n">topX</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs simulations for a list of words and returns aggregated results.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">        word_list (list): List of words to simulate.</span>
<span class="sd">        lexicon_df (DataFrame): The DataFrame containing the lexicon.</span>
<span class="sd">        normalized_cosine_sim_df (DataFrame): The DataFrame containing normalized cosine similarities.</span>
<span class="sd">        phoneme_prob_dict (dict): Dictionary containing the probabilities of each phoneme.</span>
<span class="sd">        topX (int): Number of top words to consider during simulation. Default is 50.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        DataFrame, DataFrame: Aggregated word and phoneme results.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Initialize empty DataFrames to store aggregated results</span>
    <span class="n">aggregated_word_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">aggregated_phoneme_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">:</span>
        <span class="c1">#print(f&quot;--- Running simulation for word: {word}&quot;)</span>
        <span class="c1"># Check if the word exists in the lexicon</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: &#39;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">&#39; is not in the lexicon. Skipping this word.&quot;</span><span class="p">)</span>
            <span class="k">continue</span>

        <span class="n">word_result</span><span class="p">,</span> <span class="n">phon_result</span> <span class="o">=</span> <span class="n">sim_bayes_new</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">lexicon_df</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">,</span> <span class="n">topX</span><span class="p">)</span>
        
        <span class="c1"># Append the results to the aggregated DataFrames</span>
        <span class="n">aggregated_word_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">word_result</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">aggregated_phoneme_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">aggregated_phoneme_results</span><span class="p">,</span> <span class="n">phon_result</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example usage:</span>
<span class="n">word_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;boo&#39;</span><span class="p">,</span> <span class="s1">&#39;boot&#39;</span><span class="p">,</span> <span class="s1">&#39;go&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;abrupt&#39;</span><span class="p">,</span> <span class="s1">&#39;territory&#39;</span><span class="p">,</span> <span class="s1">&#39;abandon&#39;</span><span class="p">,</span> <span class="s1">&#39;clamp&#39;</span><span class="p">,</span> <span class="s1">&#39;intolerable&#39;</span><span class="p">,</span> <span class="s1">&#39;sew&#39;</span><span class="p">]</span>
<span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span> <span class="o">=</span> <span class="n">run_simulations</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="n">lexicon_df</span><span class="p">,</span> 
                                                                      <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">)</span>
<span class="n">plot_simulation_result</span><span class="p">(</span><span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Starting simulation of word boo	... Time for simulation: 0.294764 seconds
--- Starting simulation of word boot	... Time for simulation: 0.260511 seconds
Warning: &#39;go&#39; is not in the lexicon. Skipping this word.
--- Starting simulation of word dog	... Time for simulation: 0.250739 seconds
--- Starting simulation of word abrupt	... Time for simulation: 0.259454 seconds
--- Starting simulation of word territory	... Time for simulation: 0.260331 seconds
--- Starting simulation of word abandon	... Time for simulation: 0.261817 seconds
--- Starting simulation of word clamp	... Time for simulation: 0.257876 seconds
--- Starting simulation of word intolerable	... Time for simulation: 0.316359 seconds
--- Starting simulation of word sew	... Time for simulation: 0.259150 seconds
</pre></div>
</div>
<img alt="_images/87703b39cc5490b74ea11d668326fce38183f43a5fcbfb624efdd9f59de62e84.png" src="_images/87703b39cc5490b74ea11d668326fce38183f43a5fcbfb624efdd9f59de62e84.png" />
<img alt="_images/81371d9d646bedbc0ca388ae51e2c4034d6e146ef5aa536533cbc4904c67e640.png" src="_images/81371d9d646bedbc0ca388ae51e2c4034d6e146ef5aa536533cbc4904c67e640.png" />
<img alt="_images/ab7be14e5cb973d5dcc6da03e73fad8df88055c1003ad85ef24a3d7e75d6e486.png" src="_images/ab7be14e5cb973d5dcc6da03e73fad8df88055c1003ad85ef24a3d7e75d6e486.png" />
<img alt="_images/a660f1eef7ea22e2244d2edd22b83d52cc8972fab97ed9affdfd86373f055b2a.png" src="_images/a660f1eef7ea22e2244d2edd22b83d52cc8972fab97ed9affdfd86373f055b2a.png" />
<img alt="_images/12ea3949575aed1aabc4db78d0ff7f389b73a41cb3c7bae7d198d388fde497fd.png" src="_images/12ea3949575aed1aabc4db78d0ff7f389b73a41cb3c7bae7d198d388fde497fd.png" />
<img alt="_images/0f7e728ae5e2fe87e830f434ee533fdc3aff11b4ca63ac76a9e58b06897b88e1.png" src="_images/0f7e728ae5e2fe87e830f434ee533fdc3aff11b4ca63ac76a9e58b06897b88e1.png" />
<img alt="_images/112fd19bbfa3dca78494095a543b03824d4602052fd1d69e9748fa618284bff3.png" src="_images/112fd19bbfa3dca78494095a543b03824d4602052fd1d69e9748fa618284bff3.png" />
<img alt="_images/ddd4874140a0d62afcd78087cf1065adac033d63c9dd348a3d7e1f883a36c2a7.png" src="_images/ddd4874140a0d62afcd78087cf1065adac033d63c9dd348a3d7e1f883a36c2a7.png" />
<img alt="_images/3699c01d937a62782eea100dd8524ee457f13df14e15bbfd63c8bb5a221fb803.png" src="_images/3699c01d937a62782eea100dd8524ee457f13df14e15bbfd63c8bb5a221fb803.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;tail&#39;</span><span class="p">,</span> <span class="s1">&#39;tale&#39;</span><span class="p">,</span> <span class="s1">&#39;boo&#39;</span><span class="p">,</span> <span class="s1">&#39;boat&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;category&#39;</span><span class="p">,</span> <span class="s1">&#39;vernacular&#39;</span><span class="p">,</span> <span class="s1">&#39;rain&#39;</span><span class="p">,</span> <span class="s1">&#39;rainy&#39;</span><span class="p">,</span> <span class="s1">&#39;chew&#39;</span><span class="p">,</span> <span class="s1">&#39;cardinal&#39;</span><span class="p">,</span> <span class="s1">&#39;zodiacal&#39;</span><span class="p">]</span>
<span class="n">word_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bickering&#39;</span><span class="p">,</span> <span class="s1">&#39;tail&#39;</span><span class="p">,</span> <span class="s1">&#39;tale&#39;</span><span class="p">,</span> <span class="s1">&#39;boo&#39;</span><span class="p">,</span> <span class="s1">&#39;sew&#39;</span><span class="p">,</span> <span class="s1">&#39;so&#39;</span><span class="p">]</span>
<span class="c1"># Example usage:</span>
<span class="c1"># word_list = [&#39;cat&#39;, &#39;dog&#39;, &#39;abrupt&#39;, &#39;territory&#39;, &#39;abandon&#39;, &#39;clamp&#39;, &#39;intolerable&#39;, &#39;go&#39;]</span>
<span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span> <span class="o">=</span> <span class="n">run_simulations</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="n">lexicon_df</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> 
                                                                      <span class="n">phoneme_prob_dict</span><span class="p">)</span>
<span class="n">plot_simulation_result</span><span class="p">(</span><span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span><span class="p">,</span> <span class="n">full_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Starting simulation of word bickering	... Time for simulation: 0.335023 seconds
--- Starting simulation of word tail	... Time for simulation: 0.251786 seconds
--- Starting simulation of word tale	... Time for simulation: 0.253768 seconds
--- Starting simulation of word boo	... Time for simulation: 0.259923 seconds
--- Starting simulation of word sew	... Time for simulation: 0.256054 seconds
Warning: &#39;so&#39; is not in the lexicon. Skipping this word.
</pre></div>
</div>
<img alt="_images/daf68cbd6ce83378b538861dc84bb62245abdf60d7889a6599a55bd374210a44.png" src="_images/daf68cbd6ce83378b538861dc84bb62245abdf60d7889a6599a55bd374210a44.png" />
<img alt="_images/61a2e459d66f95f997ecb1a8a096866a67d934b3a6c22a8284cf4d45f6db3279.png" src="_images/61a2e459d66f95f997ecb1a8a096866a67d934b3a6c22a8284cf4d45f6db3279.png" />
<img alt="_images/a754a08b9e941cfe1bf01449006408ae26c6dba27a4da3d013b6777f936c02c8.png" src="_images/a754a08b9e941cfe1bf01449006408ae26c6dba27a4da3d013b6777f936c02c8.png" />
<img alt="_images/87703b39cc5490b74ea11d668326fce38183f43a5fcbfb624efdd9f59de62e84.png" src="_images/87703b39cc5490b74ea11d668326fce38183f43a5fcbfb624efdd9f59de62e84.png" />
<img alt="_images/3699c01d937a62782eea100dd8524ee457f13df14e15bbfd63c8bb5a221fb803.png" src="_images/3699c01d937a62782eea100dd8524ee457f13df14e15bbfd63c8bb5a221fb803.png" />
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="lab-report">
<h1>Lab report<a class="headerlink" href="#lab-report" title="Link to this heading">#</a></h1>
<ol class="arabic simple">
<li><p>Write a brief summary of the Bayesian approach we are using here (3-4 sentences).</p></li>
<li><p>Examine the plots above. What observations can you make? Does it seem the system works well?</p></li>
<li><p>Replot, but set <code class="docutils literal notranslate"><span class="pre">full_scale=True</span></code>, which will make the Y axes go from 0 to 1 (see code below, which you can enter in a new code cell). Now review the new plots. What kinds of changes do you see? Are there any things that seem problematic?</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plot_simulation_result</span><span class="p">(</span><span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span><span class="p">,</span> <span class="n">full_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>What are some other words you think might be interesting to examine? Run new simulations with your words. Replace the word list below with the words you want to examine and copy the 3 lines of code into a new code cell. Run it. Include the code lines in your report. Explain why you chose the words you did, and report some observations about the results. (Also, if you scroll to the bottom of this notebook, there’s a code snippet a student developed last year that will randomly pick 8 words – this is a nice way to explore words you might not have thought to test.)</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># change the words in word list; you can make the list longer or shorter. Try at least 4 words.</span>
<span class="n">word_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;abrupt&#39;</span><span class="p">,</span> <span class="s1">&#39;territory&#39;</span><span class="p">,</span> <span class="s1">&#39;abandon&#39;</span><span class="p">,</span> <span class="s1">&#39;clamp&#39;</span><span class="p">,</span> <span class="s1">&#39;intolerable&#39;</span><span class="p">,</span> <span class="s1">&#39;go&#39;</span><span class="p">]</span>
<span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span> <span class="o">=</span> <span class="n">run_simulations</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="n">lexicon_df</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">)</span>
<span class="n">plot_simulation_result</span><span class="p">(</span><span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span><span class="p">,</span> <span class="n">full_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>How could we decide if a simulation should be considered successful or not? That is, how can we assess whether the word has been correctly recognized?</p></li>
<li><p>Do you see any problems with the results so far? List them, but also try to pair each problem with a possible solution – something you could change in the simulations that might improve results.</p></li>
</ol>
<section id="challenge-questions-optional">
<h2>Challenge questions – optional<a class="headerlink" href="#challenge-questions-optional" title="Link to this heading">#</a></h2>
<p><em>PhD and honors students: do at least one of these. Everyone else – you can do them for extra credit.</em></p>
<ol class="arabic simple" start="7">
<li><p><em>Programming challenge.</em> Write a function that takes a dataframe like <code class="docutils literal notranslate"><span class="pre">aggregated_word_results</span></code> and assesses whether the word was recognized correctly or not, using the approach you came up with for question 5.</p></li>
</ol>
<!---8. *Programming challenge.* Write code (does not have to be a function) that takes a random sample of words from the lemmalex lexicon and then run simulations on that list of words and make plots. Maybe just try getting this to work for 5 words initially. --->
<ol class="arabic simple" start="8">
<li><p><em>Theoretical challenge.</em> What are some aspects of spoken word recognition that are missing from our current approach? Consider the challenges from the Magnuson &amp; Crinnion review.</p></li>
</ol>
<hr class="docutils" />
</section>
<section id="an-example-improvement-from-last-year">
<h2>An example improvement from last year<a class="headerlink" href="#an-example-improvement-from-last-year" title="Link to this heading">#</a></h2>
<p>Choosing a random sample of words. The code below will run simulations with a different set of 8 randomly chosen words every time you run it.</p>
<p><em>Note that this occasionally throws an error; you can ignore it and just ‘play’ the cell again.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Submitted by Jacob Johnson</span>

<span class="kn">import</span> <span class="nn">random</span>
<span class="c1"># Extract the &quot;Item&quot; column as a list of words</span>
<span class="n">lexicon_words</span> <span class="o">=</span> <span class="n">lexicon_df</span><span class="p">[</span><span class="s1">&#39;Item&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># Use random.sample to select random keys</span>
<span class="c1"># Can change the # after the comma to select the desired sample size</span>
<span class="n">word_list</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">lexicon_words</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span> <span class="o">=</span> <span class="n">run_simulations</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> 
<span class="n">lexicon_df</span><span class="p">,</span> <span class="n">normalized_cosine_sim_df</span><span class="p">,</span> <span class="n">phoneme_prob_dict</span><span class="p">)</span>
<span class="n">plot_simulation_result</span><span class="p">(</span><span class="n">aggregated_word_results</span><span class="p">,</span> <span class="n">aggregated_phoneme_results</span><span class="p">,</span> <span class="n">full_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Starting simulation of word edit	... Time for simulation: 0.271260 seconds
--- Starting simulation of word boxer	... Time for simulation: 0.263491 seconds
--- Starting simulation of word opportune	... Time for simulation: 0.256966 seconds
--- Starting simulation of word ambition	... Time for simulation: 0.262576 seconds
--- Starting simulation of word suppose	... Time for simulation: 0.259973 seconds
--- Starting simulation of word falcon	... Time for simulation: 0.268831 seconds
--- Starting simulation of word growing	... Time for simulation: 0.275229 seconds
--- Starting simulation of word divisional	... Time for simulation: 0.309686 seconds
</pre></div>
</div>
<img alt="_images/f1f0666feb40b1059025218a89a07407728a5da220c28cd0a8c784c6ce35f0d1.png" src="_images/f1f0666feb40b1059025218a89a07407728a5da220c28cd0a8c784c6ce35f0d1.png" />
<img alt="_images/12fe3acbeadf89eca78046da5c2b288a2c9dfb1092a958d4b7c88fc649509b9c.png" src="_images/12fe3acbeadf89eca78046da5c2b288a2c9dfb1092a958d4b7c88fc649509b9c.png" />
<img alt="_images/6bc156af396b7a0b07b76e61273ce5e1a01bb7455139ae622b2e1473df586b15.png" src="_images/6bc156af396b7a0b07b76e61273ce5e1a01bb7455139ae622b2e1473df586b15.png" />
<img alt="_images/d5c82a4299d1c1da3677f8226af8d65ebf4a8980b473bf46180a618d07e513c4.png" src="_images/d5c82a4299d1c1da3677f8226af8d65ebf4a8980b473bf46180a618d07e513c4.png" />
<img alt="_images/75ae43ed3d705d2bc649ad5a04988b7920ed76fc82c7c570a741baeea7c91777.png" src="_images/75ae43ed3d705d2bc649ad5a04988b7920ed76fc82c7c570a741baeea7c91777.png" />
<img alt="_images/2ac9c12248af093ee482595eeddeff11d0ac4af3ded97211237ba826f3d321c7.png" src="_images/2ac9c12248af093ee482595eeddeff11d0ac4af3ded97211237ba826f3d321c7.png" />
<img alt="_images/07bafb0d606e1ab006f4ff295618897bad7e1699f3dc82764cb5a373b068c8d5.png" src="_images/07bafb0d606e1ab006f4ff295618897bad7e1699f3dc82764cb5a373b068c8d5.png" />
<img alt="_images/8b452ff03ca16470d154de16f35509bdc477a9587fcb12f076dcfffee6b077d4.png" src="_images/8b452ff03ca16470d154de16f35509bdc477a9587fcb12f076dcfffee6b077d4.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="06_pseudo-swr-analysis.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">More on mathematical models: Statistical models</p>
      </div>
    </a>
    <a class="right-next"
       href="07b_alternative-with-features.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">&lt;no title&gt;</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1">Part 1</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#recap-bayes-theorem">Recap: Bayes Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypotheses">Hypotheses</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2">Part 2</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-bayes-theorem-to-multiple-sources-of-evidence">Extending Bayes’ theorem to multiple sources of evidence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario">Scenario</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probabilities">Prior Probabilities:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihoods-probability-of-evidence-given-the-hypotheses">Likelihoods (probability of evidence given the hypotheses):</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalizing-bayes-theorem">Generalizing Bayes’ Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extension-to-many-sources-of-evidence">Extension to many sources of evidence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3">Part 3</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extending-to-many-sources-of-non-discrete-evidence">Extending to many sources of non-discrete evidence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-cosine-cosine-similarity">Vector cosine (cosine similarity)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#math-behind-vector-cosine">Math behind vector cosine</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-with-3-vectors">Example with 3 vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-cosine-similarity">Interpreting cosine similarity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing">Visualizing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-vector-cosine-for-phonemes">Applying vector cosine for phonemes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-4">Part 4</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-bayesian-approach-to-evaluating-phoneme-inputs">A Bayesian approach to evaluating phoneme inputs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-update-using-cosine-similarity">Bayesian Update Using Cosine Similarity</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#part-5">Part 5</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-bayesian-approach-to-spoken-word-recognition">A Bayesian approach to spoken word recognition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#phoneme-inputs">Phoneme inputs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bottom-up-phoneme-probabilities-frequencies-priors-etc">Bottom-up phoneme probabilities: frequencies, priors, etc.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#words">Words</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probability-word-frequency">Prior Probability (Word Frequency)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-words">Likelihood (words)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-probability-words">Posterior Probability (words)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-import-the-lexicon">Step 1: Import the lexicon</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#look-at-frequency-using-histograms">Look at frequency using histograms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trimming-based-on-frequency">Trimming based on frequency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-aside-phonemes">An aside: phonemes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#phoneme-similarity">Phoneme similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#phonetic-similarity">Phonetic similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-similarity">Context similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#converting-counts-to-similarities">Converting counts to similarities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-similarity">Cosine Similarity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-similarity">Visualizing similarity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-word-recogntion-simulation">Bayesian word recogntion simulation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-report">Lab report</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenge-questions-optional">Challenge questions – optional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-improvement-from-last-year">An example improvement from last year</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>