
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Neural networks, Part 3 – training networks for non-linearly separable mappings &#8212; Models of the Mind</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '13_nnets-03-learning-xor';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Neural networks, Part 4" href="14_nnets-04-srn-statlearn.html" />
    <link rel="prev" title="Neural networks, Part 2 – training networks" href="12_nnets-02-training-linear-networks.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Models of the Mind - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="Models of the Mind - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="README.html">
                    models-of-the-mind-notebooks
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_python-jupyter-overview.html">Models of the Mind, 2024</a></li>



<li class="toctree-l1"><a class="reference internal" href="01_game-of-life.html">Conway’s “Game of Life”</a></li>

<li class="toctree-l1"><a class="reference internal" href="02a_math-probability-matching.html"><font color="red">LAB REPORT</font></a></li>


<li class="toctree-l1"><a class="reference internal" href="02b_math-probability-matching.html">PROBABILITY MATCHING, PART 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_agent-probability-matching.html">PROBABILITY MATCHING, PART 3</a></li>




<li class="toctree-l1"><a class="reference internal" href="04_probability-review.html">Preliminaries: Understanding Logical Symbols in Probability</a></li>








<li class="toctree-l1"><a class="reference internal" href="05_bayes-theorem.html">Introduction to Bayes’ theorem</a></li>








<li class="toctree-l1"><a class="reference internal" href="05_bayes-theorem-rev.html">Introduction to Bayes’ theorem</a></li>








<li class="toctree-l1"><a class="reference internal" href="06_pseudo-swr-analysis.html">More on mathematical models: Statistical models</a></li>


<li class="toctree-l1"><a class="reference internal" href="07_swr-bayes-004.html">Introduction</a></li>





<li class="toctree-l1"><a class="reference internal" href="08_trace-swr.html">Implementations of the TRACE model</a></li>


<li class="toctree-l1"><a class="reference internal" href="10_netsci-01.html">Network Science Overview</a></li>








<li class="toctree-l1"><a class="reference internal" href="11_nnets-01-linear-functions-and-bias.html">Neural networks, Part 1: Linear logic functions as networks</a></li>


<li class="toctree-l1"><a class="reference internal" href="12_nnets-02-training-linear-networks.html">Neural networks, Part 2 – training networks</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Neural networks, Part 3 – training networks for non-linearly separable mappings</a></li>



<li class="toctree-l1"><a class="reference internal" href="14_nnets-04-srn-statlearn.html">Neural networks, Part 4</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F13_nnets-03-learning-xor.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/13_nnets-03-learning-xor.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Neural networks, Part 3 – training networks for non-linearly separable mappings</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Neural networks, Part 3 – training networks for non-linearly separable mappings</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-and-backpropagation">Gradient Descent and Backpropagation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#concept">Concept</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#error-surface-in-machine-learning">Error Surface in Machine Learning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics">Characteristics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-in-neural-networks">Importance in neural networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics">Mathematics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-redux">Learning rate redux</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Concept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Mathematics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-derivatives">Understanding Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-idea">Basic Idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-representation">Mathematical Representation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-and-applications">Interpretation and Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backprop-update-rule">Backprop update rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives-of-sigmoid-and-tanh-functions">Derivatives of Sigmoid and Tanh Functions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-the-sigmoid-function">Derivative of the sigmoid function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-the-tanh-function">Derivative of the tanh function</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-backpropagation-in-python">Implementing Backpropagation in Python</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-forward-pass">Step 1: Forward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-compute-loss-error">Step 2: Compute loss (error)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-backward-pass">Step 3: Backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-update-weights">Step 4: Update Weights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-code">Example code</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-aside-random-seeds-in-programming">An aside – random seeds in programming</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-random-seeds">Why use random seeds?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-set-a-random-seed-in-python">How to set a random seed in Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-issues">Other issues</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-finally-train-a-network-for-xor">Let’s (finally!) train a network for XOR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-train-the-network">Let’s train the network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-questions-part-1"><font color="red">Lab questions, part 1</font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-the-network">Plotting the network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-questions-part-2"><font color="red">Lab questions, part 2</font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#late-addition-that-will-help-with-lab-questions-visualizing-the-transformations"><font color="purple">Late addition that will help with lab questions: visualizing the transformations</font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-questions-part-3"><font color="red">Lab questions, part 3</font></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="alert alert-block alert-info">
This tutorial includes 9 lab questions. The question sections have headers like <font color='red'><b>Lab questions, part 1</b></font> in red font (there are 3 parts). Some are challenge questions, and so are required for grad students and honors students. See directions about which ones are required, and which are ones where you choose just one...  Submit your answers via WebCT and remember to paste in the question text (that really helps me grade faster!).  
</div><section class="tex2jax_ignore mathjax_ignore" id="neural-networks-part-3-training-networks-for-non-linearly-separable-mappings">
<h1>Neural networks, Part 3 – training networks for non-linearly separable mappings<a class="headerlink" href="#neural-networks-part-3-training-networks-for-non-linearly-separable-mappings" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<p>In the previous notebook, we saws that the functions OR, AND, and NAND are linearly separable (LS) and we can find weights for them in single-layer (input-output) networks (perceptrons) using a simple training algorithm (the perceptron learning rule). However, XOR is not LS and we saw that we could not find effective weights using the perceptron learning rule. However, we did see that we could ‘hand-wire’ a 2-layer network (inputs to hidden and hidden to output) and set the weights such that the hidden layer nodes were the results of tests for OR and NAND, and we could combine them (testing for OR <strong>AND</strong> NAND) to implement XOR (when both OR and NAND are true, those are cases where XOR is true).</p>
<p>In this notebook, we will move on to training methods that <em>can</em> find weights for non-linearly separable (NLS) mappings. This is going to require us to move beyond the simple perceptron learning rule.</p>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="gradient-descent-and-backpropagation">
<h1>Gradient Descent and Backpropagation<a class="headerlink" href="#gradient-descent-and-backpropagation" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Gradient descent and backpropagation are fundamental concepts in machine learning and neural networks. They are tools for <em>minimizing the error in predictions</em> by iteratively adjusting the weights of the network. Let’s start with metaphorical/conceptual treatment of these ideas.</p>
</section>
<section id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h2>
<section id="concept">
<h3>Concept<a class="headerlink" href="#concept" title="Link to this heading">#</a></h3>
<p>Imagine you’re on a mountain and you are challenged to move to the lowest point of the valley. You don’t have a topographic map, so what can you do? From where you are now, you move in the direction where the slope descends the most. Once you reach the nearest low point, you re-evaluate and aim for the lowest point you can reach from your new location. You keep doing this until you get to the lowest point there seems to be.</p>
<p>This process is akin to what gradient descent does. To complete the analogy, we can think of the errors as forming a surface.</p>
<section id="error-surface-in-machine-learning">
<h4>Error Surface in Machine Learning<a class="headerlink" href="#error-surface-in-machine-learning" title="Link to this heading">#</a></h4>
<p><em><strong>Error or Loss Function</strong></em>. This is a mathematical function that quantifies the difference between the predicted outputs of the model and the actual target values. Common examples include mean squared error for regression tasks and cross-entropy loss for classification tasks (google it if you are curious!).</p>
<p><em><strong>Parameters or Weights</strong></em>. These are the variables within the model (such as a neural network) that are adjusted during training. The goal of training is to find the set of parameters that minimizes the error (that is, that yield outputs that are as close as possible to the desired outputs).</p>
<p><em><strong>Error Surface</strong></em>. This surface represents how the error changes with respect to the model’s parameters. In a simple model with two parameters, this surface can be visualized in three dimensions: two axes represent the parameters (the weights), and the third axis represents the error. In more complex models, the error surface exists in high-dimensional space. Let’s look at an example using 2d (2 weights) plus error (3d).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span>

<span class="c1"># Sample data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>  <span class="c1"># Feature set</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>                     <span class="c1"># Target values</span>

<span class="c1"># Mean Squared Error function</span>
<span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Model prediction</span>
<span class="k">def</span> <span class="nf">model_prediction</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Generating a range of weight values</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>

<span class="c1"># Creating a meshgrid for the weights</span>
<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>

<span class="c1"># Calculating the error for each combination of weights</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mse</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model_prediction</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">))</span> <span class="k">for</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">W1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">W2</span><span class="p">))])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_surface</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Weight 1&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Weight 2&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Error Surface&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="n">elev</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="n">azim</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">interact</span><span class="p">(</span><span class="n">plot_surface</span><span class="p">,</span> <span class="n">elev</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">),</span> <span class="n">azim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "5bf6bd0ee8954cb18dbbf06f0c6b68ac", "version_major": 2, "version_minor": 0}</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function __main__.plot_surface(elev=30, azim=30)&gt;
</pre></div>
</div>
</div>
</div>
<p>So the idea here is that at different weight combinations, we get different amounts of error. If we had a way to detect the gradient in every direction from our current location (our address in terms of weight 1 and weight 2), we could use the strategy of moving down the surface until we can go no lower. In this particular surface, things are very smooth, and there’s a large region that is smoothly near the lowest point. So the goal is to change the weights until we are in that region. If we start at a random position, we don’t want to move up in the error dimension. Here’s another example that generates a more complex surface.</p>
</section>
</section>
</section>
<section id="characteristics">
<h2>Characteristics<a class="headerlink" href="#characteristics" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Shape</strong>: The shape of the error surface depends on the model architecture and the data. It can have flat areas, steep slopes, valleys, and <em>local minima</em> (think of this as a little valley on a plateau that might seem like the lowest point, but if we could see off the plateau, we’d know that there are lower points farther away).</p></li>
<li><p><strong>Global vs. Local Minima</strong>: The very lowest point on the error surface represents the <em>global minimum</em>. However, there can also be local minima, which are not the optimal set of parameters but are lower than the surrounding points.</p></li>
<li><p><strong>Gradient Descent</strong>: Optimization algorithms like gradient descent navigate this surface by iteratively moving in the direction that reduces the error, using the gradient to determine the direction.</p></li>
</ol>
<p>Let’s create a more realistic surface with 1 global minimum but also some local minima.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span>

<span class="c1"># Sample data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>  <span class="c1"># Feature set</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>                     <span class="c1"># Target values</span>

<span class="c1"># Modified Mean Squared Error function to introduce stronger local minima</span>
<span class="k">def</span> <span class="nf">mse_with_minima</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="n">base_mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Amplify the oscillations to create more distinct local minima</span>
    <span class="n">local_minima</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">w1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.3</span> <span class="o">*</span> <span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">w1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">w2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">base_mse</span> <span class="o">+</span> <span class="n">local_minima</span>

<span class="c1"># Model prediction</span>
<span class="k">def</span> <span class="nf">model_prediction</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Generating a range of weight values</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>

<span class="c1"># Creating a meshgrid for the weights</span>
<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>

<span class="c1"># Calculating the error for each combination of weights</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mse_with_minima</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model_prediction</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span> 
                   <span class="k">for</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">W1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">W2</span><span class="p">))])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">errors</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_surface</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
    <span class="n">surf</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Weight 1&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Weight 2&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Error&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Error Surface with Local Minima&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="n">elev</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="n">azim</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">interact</span><span class="p">(</span><span class="n">plot_surface</span><span class="p">,</span> <span class="n">elev</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">),</span> <span class="n">azim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e2af83faac6041198cd2fb315a75233e", "version_major": 2, "version_minor": 0}</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function __main__.plot_surface(elev=30, azim=30)&gt;
</pre></div>
</div>
</div>
</div>
</section>
<section id="importance-in-neural-networks">
<h2>Importance in neural networks<a class="headerlink" href="#importance-in-neural-networks" title="Link to this heading">#</a></h2>
<p>In neural networks, the error surface can be very complex due to the high number of parameters and non-linear transformations. Understanding this surface helps in designing optimization algorithms and model architectures, like selecting activation functions and initialization methods that lead to more favorable error surfaces. We will need an algorithm that can avoid getting stuck in local minima.</p>
<p>The <em>learning rate</em> (<span class="math notranslate nohighlight">\(\eta\)</span>, ‘eta’) determines how far we move on the error surface at any point because it governs how much we change the weights. Think of it as the <em>step size</em>: how far you step as you look for the lowest point (if it’s hard to think about this because we could obviously adjust how far we step, imagine you are in a machine that hops, and there is a minimum hop size). If we set it too low, it may take a long time to explore the surface, or we might get stuck in a local minimum. If we set it too large, we might repeatedly jump over lower points.</p>
<section id="mathematics">
<h3>Mathematics<a class="headerlink" href="#mathematics" title="Link to this heading">#</a></h3>
<p>Consider a function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> that we want to minimize. Here, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> represents the parameters of our model. The gradient (denoted by <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x})\)</span>) points in the direction of the steepest ascent. To find the minimum, we update <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in the opposite direction:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_{\text{new}} = \mathbf{x} - \eta \nabla f(\mathbf{x})
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate, a small positive value that determines the size of each step.</p>
</section>
<section id="learning-rate-redux">
<h3>Learning rate redux<a class="headerlink" href="#learning-rate-redux" title="Link to this heading">#</a></h3>
<p>The learning rate is crucial: too small, and the algorithm takes too long to converge; too large, and it might overshoot the minimum.</p>
</section>
</section>
<hr class="docutils" />
<section id="backpropagation">
<h2>Backpropagation<a class="headerlink" href="#backpropagation" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Concept<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Backpropagation is a method for calculating the gradient of the error function in neural networks. It’s like solving a complex puzzle by working backwards. After comparing the output of the network with the desired output, the algorithm calculates how much each neuron’s activity contributed to the error and adjusts the weights accordingly. The algorithm figures out how much each weight contributed to the ultimate error, and changes each weight slightly in the direction that would lead to lower error if the same pattern were input again.</p>
</section>
<section id="id2">
<h3>Mathematics<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>Consider a neural network with weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> and a loss function <span class="math notranslate nohighlight">\(L\)</span>. Backpropagation involves computing the derivative of the loss with respect to the weights, denoted as <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}}\)</span>. This derivative tells us how to update the weights to reduce the error.</p>
<div class="alert alert-block alert-info">
</section>
</section>
<section id="understanding-derivatives">
<h2>Understanding Derivatives<a class="headerlink" href="#understanding-derivatives" title="Link to this heading">#</a></h2>
<p>A derivative is a fundamental concept in calculus that describes how a function changes as its input changes. It is a measure of the rate at which a function’s value changes at a certain point.</p>
<section id="basic-idea">
<h3>Basic Idea<a class="headerlink" href="#basic-idea" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Instantaneous Rate of Change</strong>: The derivative of a function at a point is the instantaneous rate of change of the function’s value with respect to changes in its input at that point. It’s like measuring the slope of the function at a particular point.</p></li>
<li><p><strong>Slope of the Tangent Line</strong>: Geometrically, the derivative at a point corresponds to the slope of the tangent line to the function at that point. If you graph the function, the tangent line at a point is a straight line that just “touches” the curve at that point without crossing it. The slope of this tangent line is the derivative.</p></li>
</ul>
<p><em>The next code block makes a plot to demonstrate this.</em></p>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the function and its derivative</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">df</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>

<span class="c1"># Define the point at which we calculate the derivative</span>
<span class="n">x0</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">df</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>

<span class="c1"># Define the tangent line at x0</span>
<span class="k">def</span> <span class="nf">tangent_line</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span> <span class="o">+</span> <span class="n">y0</span>

<span class="c1"># Generate x values</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>

<span class="c1"># Plot the function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;f(x) = x^2&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>

<span class="c1"># Plot the tangent line</span>
<span class="n">x_tangent</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_tangent</span><span class="p">,</span> <span class="n">tangent_line</span><span class="p">(</span><span class="n">x_tangent</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Tangent at x = 2&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>

<span class="c1"># Highlight the point of tangency</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">x0</span><span class="p">],</span> <span class="p">[</span><span class="n">y0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;  (</span><span class="si">{</span><span class="n">x0</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">y0</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>

<span class="c1"># Add labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Illustration of Instantaneous Rate of Change&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cbe5a0628c6bcdf112f470e7b813f1fffed3353f3fea892df05d8d25f76c696f.png" src="_images/cbe5a0628c6bcdf112f470e7b813f1fffed3353f3fea892df05d8d25f76c696f.png" />
</div>
</div>
<div class="alert alert-block alert-info">
</section>
<section id="mathematical-representation">
<h3>Mathematical Representation<a class="headerlink" href="#mathematical-representation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Notation</strong>: If <span class="math notranslate nohighlight">\(f(x)\)</span> is a function, its derivative is commonly denoted as <span class="math notranslate nohighlight">\(f'(x)\)</span>, <span class="math notranslate nohighlight">\(\frac{df}{dx}\)</span>, or <span class="math notranslate nohighlight">\(Df(x)\)</span>.</p></li>
<li><p><strong>Limit Definition</strong>: Mathematically, the derivative of <span class="math notranslate nohighlight">\(f\)</span> at a point <span class="math notranslate nohighlight">\(x\)</span> is defined as the limit of the average rate of change of the function over an interval as the interval becomes infinitesimally small:
$<span class="math notranslate nohighlight">\(
f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
\)</span>$</p></li>
</ul>
</section>
<section id="interpretation-and-applications">
<h3>Interpretation and Applications<a class="headerlink" href="#interpretation-and-applications" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Physical Interpretation</strong>: In physics, the derivative of the position of an object with respect to time is its velocity, representing how quickly the position changes over time.</p></li>
<li><p><strong>General Applications</strong>: Derivatives are used in a wide range of fields, from physics and engineering to economics and biology and psychology, for modeling and understanding how changes in one quantity lead to changes in another.</p></li>
<li><p><strong>Simple Example</strong>: For the function <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>, the derivative <span class="math notranslate nohighlight">\(f'(x)\)</span> is <span class="math notranslate nohighlight">\(2x\)</span>. This means that at any point <span class="math notranslate nohighlight">\(x\)</span>, the rate at which <span class="math notranslate nohighlight">\(f(x)\)</span> changes is <span class="math notranslate nohighlight">\(2x\)</span>.</p></li>
</ul>
<p>In summary, derivatives provide a powerful way to analyze and predict the behavior of functions, particularly in understanding and quantifying how changes in variables affect the outcome of the function.</p>
</div></section>
<section id="backprop-update-rule">
<h3>Backprop update rule<a class="headerlink" href="#backprop-update-rule" title="Link to this heading">#</a></h3>
<p>The update rule is similar to gradient descent:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{W}_{\text{new}} = \mathbf{W} - \eta \frac{\partial L}{\partial \mathbf{W}}
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate.</p>
</section>
<section id="intuition">
<h3>Intuition<a class="headerlink" href="#intuition" title="Link to this heading">#</a></h3>
<p>The power of backpropagation lies in its ability to handle complex networks. By breaking the problem into smaller parts (chain rule in calculus), it calculates the gradient for each weight efficiently.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In summary, gradient descent is like finding the best path downhill, and backpropagation is about understanding each step’s contribution to a misstep. Together, they form the backbone of training neural networks.</p>
<div class="alert alert-block alert-info">
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="derivatives-of-sigmoid-and-tanh-functions">
<h1>Derivatives of Sigmoid and Tanh Functions<a class="headerlink" href="#derivatives-of-sigmoid-and-tanh-functions" title="Link to this heading">#</a></h1>
<p><em>You can skip this section if you are not interested in the mathematical details!</em></p>
<p>The functions <code class="docutils literal notranslate"><span class="pre">sigmoid_derivative</span></code> and <code class="docutils literal notranslate"><span class="pre">tanh_derivative</span></code> in code blocks below are used to calculate the derivatives of the sigmoid and hyperbolic tangent (tanh) functions, respectively. These derivatives are crucial in neural networks, especially during backpropagation. Let’s explore how these derivatives are calculated. First, here’s the code – it’s suprisingly simple. (Note that <code class="docutils literal notranslate"><span class="pre">np</span></code> here is a <code class="docutils literal notranslate"><span class="pre">numpy</span></code> function, since we will <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">numpy</span> <span class="pre">as</span> <span class="pre">np</span></code> later.)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># need the derivatives for backpropogation </span>
    <span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">tanh_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<section id="derivative-of-the-sigmoid-function">
<h2>Derivative of the sigmoid function<a class="headerlink" href="#derivative-of-the-sigmoid-function" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The sigmoid function is defined as <span class="math notranslate nohighlight">\( \sigma(x) = \frac{1}{1 + e^{-x}} \)</span>.</p></li>
<li><p>The derivative of the sigmoid function, <span class="math notranslate nohighlight">\( \sigma'(x) \)</span>, is <span class="math notranslate nohighlight">\( \sigma(x) \cdot (1 - \sigma(x)) \)</span>. This derivative is derived using the chain rule in calculus.</p></li>
<li><p>In practice, when implementing backpropagation, we often already have the sigmoid output computed during the forward pass. Hence, instead of recalculating <span class="math notranslate nohighlight">\( \sigma(x) \)</span>, the derivative is computed as <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">x)</span></code>, where <code class="docutils literal notranslate"><span class="pre">x</span></code> is actually <span class="math notranslate nohighlight">\( \sigma(x) \)</span>.</p></li>
</ul>
</section>
<section id="derivative-of-the-tanh-function">
<h2>Derivative of the tanh function<a class="headerlink" href="#derivative-of-the-tanh-function" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The hyperbolic tangent function is defined as <span class="math notranslate nohighlight">\( \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \)</span>.</p></li>
<li><p>The derivative of the tanh function, <span class="math notranslate nohighlight">\( \tanh'(x) \)</span>, is <span class="math notranslate nohighlight">\( 1 - \tanh(x)^2 \)</span>. This also results from applying the chain rule.</p></li>
<li><p>Similar to the sigmoid case, if we have the value of <span class="math notranslate nohighlight">\( \tanh(x) \)</span> from the forward pass, the derivative at that point can be computed efficiently as <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">np.tanh(x)**2</span></code>.</p></li>
</ul>
</div></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="implementing-backpropagation-in-python">
<h1>Implementing Backpropagation in Python<a class="headerlink" href="#implementing-backpropagation-in-python" title="Link to this heading">#</a></h1>
<p>Backpropagation is a key method used in training artificial neural networks. It involves a forward pass to compute the output, and a backward pass to update the network’s weights based on the error. Below is a step-by-step guide to implement backpropagation in Python:</p>
<section id="step-1-forward-pass">
<h2>Step 1: Forward pass<a class="headerlink" href="#step-1-forward-pass" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Initialize Network</strong>: Create the structure of the neural network with initialized weights and biases.</p></li>
<li><p><strong>Input Layer</strong>: Feed the input data into the network.</p></li>
<li><p><strong>Hidden Layers</strong>: Calculate the output of each neuron in the hidden layers, using a weighted sum of inputs and an activation function. For instance, with a sigmoid activation function:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Output Layer</strong>: Compute the network’s final output.</p></li>
</ul>
</section>
<section id="step-2-compute-loss-error">
<h2>Step 2: Compute loss (error)<a class="headerlink" href="#step-2-compute-loss-error" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Calculate Error</strong>: Determine the network’s error using a loss function, like mean squared error (MSE) for regression problems:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">target</span><span class="o">-</span><span class="n">output</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="step-3-backward-pass">
<h2>Step 3: Backward pass<a class="headerlink" href="#step-3-backward-pass" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Output Layer Error</strong>: Determine the error contribution of the output layer.</p></li>
<li><p><strong>Backpropagate Error</strong>: For each layer, starting from the output and moving backwards, calculate:</p></li>
<li><p>The error contribution of each neuron.</p></li>
<li><p>The gradient of the activation function. For a sigmoid, it’s:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Update weights and biases based on the error and derivative:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">weights</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">error</span> <span class="o">*</span> <span class="n">derivative</span>
</pre></div>
</div>
</section>
<section id="step-4-update-weights">
<h2>Step 4: Update Weights<a class="headerlink" href="#step-4-update-weights" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Apply Gradient Descent</strong>: Adjust the network’s weights using the gradients calculated during backpropagation.</p></li>
<li><p><strong>Learning Rate</strong>: Control the weight adjustment magnitude with a learning rate.</p></li>
</ul>
</section>
<section id="example-code">
<h2>Example code<a class="headerlink" href="#example-code" title="Link to this heading">#</a></h2>
<p>Here’s a simplified example of backpropagation for a single neuron:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">backpropagate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="c1"># Calculate error</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">expected_output</span> <span class="o">-</span> <span class="n">output</span>
    
        <span class="c1"># Derivative for sigmoid activation</span>
        <span class="n">derivative</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
    
        <span class="c1"># Update weight</span>
        <span class="n">weight</span> <span class="o">+=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">error</span> <span class="o">*</span> <span class="n">derivative</span>
    
        <span class="k">return</span> <span class="n">weight</span>
</pre></div>
</div>
<hr class="docutils" />
<section id="activation-functions">
<h3>Activation functions<a class="headerlink" href="#activation-functions" title="Link to this heading">#</a></h3>
<p>A subtle but important consideration is the activation function used for nodes at various levels within a network. The next code block defines a function that generates figures to demonstrate different activation functions. Run it now so that the function is defined, and we will call it (execute it) later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_activation_plots</span><span class="p">():</span>

    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

    <span class="c1"># Activation functions</span>
    <span class="k">def</span> <span class="nf">linear_activation</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">step_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sigmoid_activation</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">tanh_activation</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">relu_activation</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="c1"># Generating values from -10 to 10</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

    <span class="c1"># Plotting</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="c1"># Function to add annotations</span>
    <span class="k">def</span> <span class="nf">add_annotation</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">formula</span><span class="p">,</span> <span class="n">xpos</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ypos</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">xpos</span><span class="p">,</span> <span class="n">ypos</span><span class="p">,</span> <span class="n">formula</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> 
                <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> 
                <span class="n">fontsize</span><span class="o">=</span><span class="n">fontsize</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>

    <span class="c1"># Function to add grey dashed lines</span>
    <span class="k">def</span> <span class="nf">add_zero_lines</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Linear</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">linear_activation</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Linear Activation&#39;</span><span class="p">)</span>
    <span class="n">add_annotation</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;f(x) = x&#39;</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>

    <span class="c1"># Step</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">step_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Step Activation (0.5)&#39;</span><span class="p">)</span>
    <span class="n">add_annotation</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;f(x) = 1 if x &gt; 0.5</span><span class="se">\n</span><span class="s1">       else 0&#39;</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="c1"># Sigmoid</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid_activation</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Sigmoid Activation&#39;</span><span class="p">)</span>
    <span class="c1">#add_annotation(axes[0,2], &#39;f(x) = 1 / (1 + exp(-x))&#39;, 0.65, 0.15)</span>
    <span class="n">add_annotation</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;f(x) = $</span><span class="se">\\</span><span class="s1">frac</span><span class="si">{1}</span><span class="s1">{1 + e^{-x}}$&#39;</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="c1"># ReLU</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">relu_activation</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;ReLU Activation&#39;</span><span class="p">)</span>
    <span class="n">add_annotation</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;f(x) = max(0, x)&#39;</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">)</span>

    <span class="c1"># Step</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">step_activation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Step Activation (-1.5)&#39;</span><span class="p">)</span>
    <span class="n">add_annotation</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;f(x) = 1 if x &gt; -1.5</span><span class="se">\n</span><span class="s1">       else 0&#39;</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Tanh</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tanh_activation</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Tanh Activation&#39;</span><span class="p">)</span>
    <span class="n">add_annotation</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;f(x) = $</span><span class="se">\\</span><span class="s1">frac{e^</span><span class="si">{x}</span><span class="s1"> - e^{-x}}{e^</span><span class="si">{x}</span><span class="s1"> + e^{-x}}$&#39;</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">ax_x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">ax_y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
            <span class="n">add_zero_lines</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">ax_x</span><span class="p">,</span><span class="n">ax_y</span><span class="p">])</span>


    <span class="c1"># Adjusting layout</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The choice of activation functions in neural networks is crucial as they determine how the network processes inputs and learns. Different activation functions have characteristics that may make them more or less suitable for specific tasks or layers in a neural network. For example, backprop depends on calculating derivatives, so whether an activation function is <em>differentiable</em> (yields valid derivatives) throughout its entire range, only part of its range, or not at all has important implications.</p>
<p>Here’s an overview of why we might use different activation functions, including linear, threshold, ReLU, sigmoid, or tanh, for hidden and/or output units:</p>
<ul class="simple">
<li><p><strong>Linear Activation Function</strong>:</p>
<ul>
<li><p><strong>Use</strong>: Often used in the output layer for regression tasks where the goal is to predict continuous values.</p></li>
<li><p><strong>Characteristics</strong>: It’s a simple identity function that doesn’t alter the weighted sum of the input. This function is useful when the range of output is not confined to a specific range.</p></li>
<li><p><strong>Considerations</strong>: This function is differentiable. Its derivative is constant, which allows for backpropagation. However, its use can lead to issues in deep networks due to lack of non-linearity.</p></li>
</ul>
</li>
<li><p><strong>Step (threshold) Activation Function</strong>:</p>
<ul>
<li><p><strong>Use</strong>: Primarily used in binary classification tasks or perceptrons. It’s less common in modern deep learning models.</p></li>
<li><p><strong>Characteristics</strong>: It’s a simple function that outputs a binary result based on a threshold. This function can create clear, binary decisions, but it lacks the ability to capture the nuances in data due to its non-continuous and non-differentiable nature.</p></li>
<li><p><strong>Considerations</strong>: Not differentiable, and hence, not typically used in networks trained with backpropagation. Modern neural networks rely on differentiable activation functions like ReLU, sigmoid, and tanh, which allow for effective gradient-based optimization during training.</p></li>
</ul>
</li>
<li><p><strong>ReLU (Rectified Linear Unit) Activation Function</strong>:</p>
<ul>
<li><p><strong>Use</strong>: Widely used in hidden layers of neural networks.</p></li>
<li><p><strong>Characteristics</strong>: ReLU is defined as the positive part of its argument. It helps with the vanishing gradient problem, allowing models to learn faster and perform better. However, it can suffer from the “dying ReLU” problem, where neurons stop responding to variations due to negative input values.</p></li>
<li><p><strong>Considerations</strong>: ReLU is ‘piecewise’ differentiable. It can be used in backpropagation, although it has a derivative of zero for all negative inputs, which can lead to neurons that never activate (the dying ReLU problem).</p></li>
</ul>
</li>
<li><p><strong>Sigmoid Activation Function</strong>:</p>
<ul>
<li><p><strong>Use</strong>: Commonly used in the output layer for binary classification problems.</p></li>
<li><p><strong>Characteristics</strong>: It maps the input values to a range between 0 and 1, making it a good choice for probabilities. However, it suffers from the vanishing gradient problem when dealing with deep networks or values that fall off the range of the linear region of the function.</p></li>
<li><p><strong>Considerations</strong>: The sigmoid function is differentiable across its entire domain. Its smooth gradient prevents sharp jumps during learning.</p></li>
</ul>
</li>
<li><p><strong>Tanh (Hyperbolic Tangent) Activation Function</strong>:</p>
<ul>
<li><p><strong>Use</strong>: Often used in hidden layers.</p></li>
<li><p><strong>Characteristics</strong>: It’s similar to the sigmoid function but maps the input values to a range between -1 and 1. This characteristic means that the tanh function is zero-centered, making it easier to model inputs that have strongly negative, neutral, and strongly positive values. Like the sigmoid, it also suffers from the vanishing gradient problem for extreme values.</p></li>
<li><p><strong>Considerations</strong>: Like the sigmoid, the tanh function is also differentiable across its entire input range and provides a smooth gradient. In some cases, tanh may be superior to sigmoid because it <em>tends</em> (in my experience) to lead to fewer weights with values close to zero.</p></li>
</ul>
</li>
</ul>
<p>The choice of activation function depends on the specific requirements of the neural network task. Linear functions are suitable for regression, threshold functions for simple binary decisions, ReLU for faster learning in hidden layers, sigmoid for binary classification, and tanh for cases where zero-centered outputs are beneficial. The key is to match the function’s properties to the task’s needs, considering factors such as output range, learning speed, and the ability to handle gradients effectively.</p>
<p><em>To see the functions, execute</em> <code class="docutils literal notranslate"><span class="pre">make_activation_plots()</span></code> <em>in the next cell.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">make_activation_plots</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4fd04912203ad20fce434d4d932cd29cc9d5ca73744d6e9ebda775e054de873e.png" src="_images/4fd04912203ad20fce434d4d932cd29cc9d5ca73744d6e9ebda775e054de873e.png" />
</div>
</div>
<hr class="docutils" />
<div class="alert alert-block alert-info">
</section>
</section>
<section id="an-aside-random-seeds-in-programming">
<h2>An aside – random seeds in programming<a class="headerlink" href="#an-aside-random-seeds-in-programming" title="Link to this heading">#</a></h2>
<p>Random seeds are an essential concept in Python and other programming languages, particularly when working with random number generation. They play a crucial role in ensuring the reproducibility of random processes.</p>
<p>A random seed is a starting point in generating a sequence of random numbers. In Python, the random number generators (RNGs) are <em>deterministic</em>, meaning if you provide the same seed, you will get the same sequence of numbers (i.e., the first random number you request will be the same from run to run, and so will the 100th).</p>
<section id="why-use-random-seeds">
<h3>Why use random seeds?<a class="headerlink" href="#why-use-random-seeds" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Reproducibility</strong>: The most common reason to use a random seed is to ensure that results are reproducible. This is crucial in scientific experiments or simulations where the results need to be verifiable.</p></li>
<li><p><strong>Debugging</strong>: It is easier to debug a program if you know it will always follow the same execution path with the same set of pseudo-random numbers.</p></li>
</ul>
</section>
<section id="how-to-set-a-random-seed-in-python">
<h3>How to set a random seed in Python<a class="headerlink" href="#how-to-set-a-random-seed-in-python" title="Link to this heading">#</a></h3>
<p>Python’s <code class="docutils literal notranslate"><span class="pre">random</span></code> module, as well as other libraries like <code class="docutils literal notranslate"><span class="pre">numpy</span></code>, allow you to set a random seed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># using random module</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Setting the seed</span>

<span class="c1"># using numpy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Setting the seed</span>
</pre></div>
</div>
</section>
<section id="other-issues">
<h3>Other issues<a class="headerlink" href="#other-issues" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>You <em>can</em> get different results with the same random seed on different systems depending on operating system, etc., but within a system you will get identical results with the same random seed</p></li>
<li><p>Why <em>wouldn’t</em> we use a random seed?</p>
<ul>
<li><p>Exploration: when testing a new system, such as a neural network trained on the XOR problem, using the same random seed could make it <em>harder</em> to discover good solutions (if, e.g., that random seed generates values that give you initial random weights that are hard for the network to ‘overcome’ to find a solution)</p></li>
<li><p>Variability: when you <em>want</em> a lot of variation between runs – we can discuss what some of these cases might be in class.</p></li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<hr class="docutils" />
<section id="let-s-finally-train-a-network-for-xor">
<h2>Let’s (finally!) train a network for XOR<a class="headerlink" href="#let-s-finally-train-a-network-for-xor" title="Link to this heading">#</a></h2>
<p>We will use tanh for hidden layer activations and sigmoid for outputs.</p>
<p>The next cell defines everything we need for training. The cell after actually trains a network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span>
<span class="kn">import</span> <span class="nn">time</span> 

<span class="c1"># Activation functions and their derivatives</span>

<span class="c1"># sigmoid: translates - and + values to range 0,1; </span>
<span class="c1">#         -6 --&gt; 0.002</span>
<span class="c1">#         -4 --&gt; 0.02</span>
<span class="c1">#         -2 --&gt; 0.12</span>
<span class="c1">#          0 --&gt; 0.50</span>
<span class="c1">#          2 --&gt; 0.88</span>
<span class="c1">#          4 --&gt; 0.98</span>
<span class="c1">#          6 --&gt; 0.998</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># sigmoid: translates - and + values to range -1,1; </span>
<span class="c1">#         -6 --&gt; -0.9999...</span>
<span class="c1">#         -4 --&gt; -0.9993...</span>
<span class="c1">#         -2 --&gt; -0.964</span>
<span class="c1">#         -1 --&gt; -0.762</span>
<span class="c1">#       -0.5 --&gt; -0.462</span>
<span class="c1">#          0 --&gt;  0.000</span>
<span class="c1">#        0.5 --&gt;  0.462</span>
<span class="c1">#          1 --&gt;  0.762</span>
<span class="c1">#          2 --&gt;  0.964</span>
<span class="c1">#          4 --&gt;  0.9993...</span>
<span class="c1">#          6 --&gt;  0.99999..</span>
<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># need the derivatives for backpropogation </span>
<span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tanh_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Mean Squared Error loss</span>
<span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># training function</span>
<span class="k">def</span> <span class="nf">train_xor_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lr_initial</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">lr_reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">plot_interval</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
                    <span class="n">show_wtval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wtsd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                   <span class="n">weight_init_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight_init_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stop_criterion</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">tries</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Function to train the XOR network. </span>
<span class="sd">    &#39;&#39;&#39;</span>
        
    <span class="c1"># User can set a random seed for reproducibility</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">rseed</span><span class="p">)</span>
    
    <span class="c1"># initialize weights in range specified by weight_init_min and _max</span>
    
    <span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">num_hidden</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">num_output</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">num_bias_hid</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">num_bias_out</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="c1"># you can go back to the old way by specifying weight_init_min and weight_init_max values, </span>
    <span class="c1"># but default is to use wtsd (see else)</span>
    <span class="k">if</span> <span class="n">weight_init_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight_init_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> 
        <span class="c1"># 2 input and 2 hidden nodes, hence weights_input_hidden has shape 2, 2</span>
        <span class="n">weights_input_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">weight_init_min</span><span class="p">,</span> <span class="n">weight_init_max</span><span class="p">,</span> <span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">))</span>
        <span class="c1"># 2 hidden and 1 output nodes, hence weights_hidden_output has shape 2, 1</span>
        <span class="n">weights_hidden_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">weight_init_min</span><span class="p">,</span> <span class="n">weight_init_max</span><span class="p">,</span> <span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_output</span><span class="p">))</span>
        <span class="c1"># 1 bias to 2 hidden has shape 1, 2</span>
        <span class="n">bias_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_bias_hid</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">))</span>
        <span class="c1"># 1 bias to 1 output has shape 1, 1</span>
        <span class="n">bias_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_bias_out</span><span class="p">,</span> <span class="n">num_output</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># initialize weights</span>
        <span class="n">weights_input_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">wtsd</span><span class="p">,</span> <span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">))</span>
        <span class="n">weights_hidden_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">wtsd</span><span class="p">,</span> <span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_output</span><span class="p">))</span>
        <span class="n">bias_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">wtsd</span><span class="p">,</span> <span class="p">(</span><span class="n">num_bias_hid</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">))</span>
        <span class="n">bias_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">wtsd</span><span class="p">,</span> <span class="p">(</span><span class="n">num_bias_out</span><span class="p">,</span> <span class="n">num_output</span><span class="p">))</span>
    
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># list to store loss (error) values</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">lr_initial</span>  <span class="c1"># Initialize learning rate</span>
    <span class="n">lr_values</span> <span class="o">=</span> <span class="p">[]</span>
    

    <span class="k">def</span> <span class="nf">calculate_reverse_sigmoid_lr</span><span class="p">(</span><span class="n">lr_initial</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lr_reduce</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate the learning rate following a reverse sigmoid curve.</span>

<span class="sd">        :param lr_initial: Initial learning rate.</span>
<span class="sd">        :param epoch: Current epoch number.</span>
<span class="sd">        :param epochs: Total number of epochs.</span>
<span class="sd">        :param final_lr_fraction: Fraction of the initial learning rate at the final epoch.</span>
<span class="sd">        :param k: Steepness of the sigmoid curve.</span>
<span class="sd">        :return: Adjusted learning rate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Scale the epoch number to [0, 1]</span>
        
        <span class="n">min_lr</span> <span class="o">=</span> <span class="n">lr_reduce</span> <span class="o">*</span> <span class="n">lr_initial</span>
        <span class="n">red_lr</span> <span class="o">=</span> <span class="n">lr_initial</span> <span class="o">-</span> <span class="n">min_lr</span>
        <span class="n">proportion_epoch</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">scaled_epoch</span> <span class="o">=</span> <span class="n">proportion_epoch</span> <span class="o">*</span> <span class="n">red_lr</span>
        

        <span class="c1"># Apply the reverse sigmoid function</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">lr_initial</span> <span class="o">*</span> <span class="p">(</span><span class="n">lr_reduce</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lr_reduce</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">proportion_epoch</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)))))</span>

        <span class="k">return</span> <span class="n">lr</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Forward pass</span>
        
        <span class="c1"># input to the hidden nodes is the dot product of the input values and</span>
        <span class="c1"># the weights_input_hidden matrix plus the value of the bias weight </span>
        <span class="c1"># (since bias input is always 1)</span>
        <span class="n">hidden_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_hidden</span>
        <span class="n">hidden_output</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">hidden_input</span><span class="p">)</span> <span class="c1"># transform using tanh function</span>
        
        <span class="c1"># hidden to output is dot product of hidden values and </span>
        <span class="c1"># weights_hidden_output plus the value of the bias weight </span>
        <span class="c1"># (since bias input is always 1)</span>
        <span class="n">output_input</span> <span class="o">=</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_output</span>
        <span class="n">final_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">output_input</span><span class="p">)</span> <span class="c1"># transform using sigmoid function</span>

        <span class="c1"># Calculate loss -- MSE of expected output and observed output</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">mse_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">final_output</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="c1"># append to list</span>
        
        <span class="c1"># Adjust learning rate based on loss (adaptive learning rate)</span>
        <span class="k">if</span> <span class="n">lr_reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1">#lr = lr_initial * (lr_reduce * loss) - (0.01 * (1/(epoch + 1)))</span>
            <span class="c1">#lr -= lr * (lr_reduce)</span>
            <span class="c1"># lr *= 1 - (lr_reduce *  ((epoch + 1) / (epochs + 1)))</span>
            <span class="c1">#lr = calculate_exponential_lr(lr_initial, epoch, epochs)</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">calculate_reverse_sigmoid_lr</span><span class="p">(</span><span class="n">lr_initial</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">epochs</span><span class="p">)</span>
        <span class="n">lr_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>  <span class="c1"># Store current learning rate</span>
        
        <span class="c1"># Backward pass: compute gradients and update weights and biases</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">output</span> <span class="o">-</span> <span class="n">final_output</span> <span class="c1"># raw difference</span>
        <span class="n">d_final_output</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">final_output</span><span class="p">)</span> <span class="c1"># Loss derivative wrt final output</span>
        <span class="n">error_hidden</span> <span class="o">=</span> <span class="n">d_final_output</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># Error propagated to hidden layer</span>
        <span class="n">d_hidden_output</span> <span class="o">=</span> <span class="n">error_hidden</span> <span class="o">*</span> <span class="n">tanh_derivative</span><span class="p">(</span><span class="n">hidden_output</span><span class="p">)</span> <span class="c1"># Loss deriv wrt hidden output</span>

        <span class="c1"># Update weights and biases using gradient descent</span>
        <span class="c1"># d_ objects are the loss derivatives; lr is learning rate</span>
        <span class="n">weights_hidden_output</span> <span class="o">+=</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_final_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>
        <span class="n">bias_output</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_final_output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>
        <span class="n">weights_input_hidden</span> <span class="o">+=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d_hidden_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>
        <span class="n">bias_hidden</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">d_hidden_output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr</span>

        <span class="c1"># Make plots if epoch modulus plot_interval is plot_interval - 1 -- gives us </span>
        <span class="c1"># epochs that are at the interval plus 1</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="n">plot_interval</span> <span class="o">==</span> <span class="n">plot_interval</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">plot_training_results</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> \
                                  <span class="n">bias_output</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">lr_values</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">show_wtval</span><span class="o">=</span><span class="n">show_wtval</span><span class="p">,</span> <span class="n">tries</span><span class="o">=</span><span class="n">tries</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="n">rseed</span><span class="p">)</span>

        <span class="c1"># Check stopping criterion (optional)</span>
        <span class="k">if</span> <span class="n">stop_criterion</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">((</span><span class="n">final_output</span><span class="p">[</span><span class="n">output</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">stop_criterion</span><span class="p">))</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="ow">and</span> <span class="p">(</span><span class="n">final_output</span><span class="p">[</span><span class="n">output</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">stop_criterion</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()):</span>
                <span class="n">plot_training_results</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> \
                                     <span class="n">bias_output</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">lr_values</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">show_wtval</span><span class="o">=</span><span class="n">show_wtval</span><span class="p">,</span> <span class="n">tries</span><span class="o">=</span><span class="n">tries</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="n">rseed</span><span class="p">)</span>
                <span class="c1"># print(f&quot;Stopping criterion met at epoch {epoch+1}&quot;)</span>
                <span class="k">break</span>

    <span class="k">return</span> <span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">epoch</span>



<span class="k">def</span> <span class="nf">plot_training_results</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">,</span> 
                          <span class="n">losses</span><span class="p">,</span> <span class="n">lr_values</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">show_wtval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>  <span class="c1"># Increase subplot grid size</span>

    <span class="c1"># Loss plot</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tries</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">titletext</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Try </span><span class="si">{</span><span class="n">tries</span><span class="si">}</span><span class="s1">, Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="k">else</span><span class="p">:</span> 
        <span class="n">titletext</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="k">if</span> <span class="n">rseed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">titletext</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;, rseed </span><span class="si">{</span><span class="n">rseed</span><span class="si">}</span><span class="s2">&quot;</span>
        
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">titletext</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    
    <span class="c1"># Decision boundary plot</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
    <span class="n">grid_hidden</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_hidden</span><span class="p">)</span>
    <span class="n">grid_final</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">grid_hidden</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_output</span><span class="p">)</span>
    <span class="n">zz</span> <span class="o">=</span> <span class="n">grid_final</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">zz</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pink&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Decision Boundary&#39;</span><span class="p">)</span>

    <span class="c1"># Weight heatmaps including biases</span>
    <span class="n">weights_input_hidden_with_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">weights_hidden_output_with_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>

    <span class="c1"># Finding the maximum and minimum values</span>
    <span class="n">min_weight</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
    <span class="n">max_weight</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">min_weight</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">weights_input_hidden_with_bias</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">weights_hidden_output_with_bias</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">min_weight</span><span class="p">)</span>
    <span class="n">max_weight</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">weights_input_hidden_with_bias</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">weights_hidden_output_with_bias</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">max_weight</span><span class="p">)</span>

    <span class="n">im1</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">weights_input_hidden_with_bias</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">min_weight</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">max_weight</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Input-Hidden Weights Heatmap&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_wtval</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weights_input_hidden_with_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weights_input_hidden_with_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">text</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">weights_input_hidden_with_bias</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;yellow&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Hidden nodes&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Inputs + Bias&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">im2</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">weights_hidden_output_with_bias</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">min_weight</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">max_weight</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Hidden-Output Weights Heatmap&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show_wtval</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weights_hidden_output_with_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">weights_hidden_output_with_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">text</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">weights_hidden_output_with_bias</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;yellow&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Hidden nodes + Bias&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Output&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

    <span class="c1"># Learning rate plot</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr_values</span><span class="p">)</span>  <span class="c1"># Plot learning rate values</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Learning Rate Over Epochs&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Learning Rate&quot;</span><span class="p">)</span>

    <span class="c1"># hide the unused bottom right panel</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="n">display</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">ioff</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="let-s-train-the-network">
<h2>Let’s train the network<a class="headerlink" href="#let-s-train-the-network" title="Link to this heading">#</a></h2>
<p>Examine the next block. It tries to train a network with pretty simple architecture to solve XOR. We give it <code class="docutils literal notranslate"><span class="pre">stop_criterion=0.2</span></code> which means that it will treat outputs &lt; 0.2 as 0 and outputs &gt; 0.8 as 1. If the network reaches the output criterion (giving an output &lt; 0.2 when the answer is 0 and an output &gt; 0.8 when the answer is 1), it stops training. If it does not reach criterion within <code class="docutils literal notranslate"><span class="pre">max_tries</span></code> attempts, it gives up.</p>
<p>Note that an <strong>epoch</strong> is one pass through the training examples. Here, they are not shuffled, but presented in the same order every epoch. Arguably, this is okay for a small set of input-output mappings, but we will return to this issue later.</p>
<p>Note that the ‘learning rate over epochs’ plot will not change because the learning rate is currently fixed.</p>
<p>If you want to see a case where the network definitely learns (at least on my mac), set <code class="docutils literal notranslate"><span class="pre">rseed=37</span></code> instead of <code class="docutils literal notranslate"><span class="pre">rseed=None</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># XOR Data</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Training parameters</span>
<span class="c1"># epochs = 300000</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">8000</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">rseed</span> <span class="o">=</span> <span class="kc">None</span>
<span class="c1"># rseed = 1</span>

<span class="c1"># Training</span>
<span class="n">max_tries</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">tries</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">last_epoch</span> <span class="o">=</span> <span class="n">epochs</span>
<span class="k">while</span> <span class="n">last_epoch</span> <span class="o">&gt;=</span> <span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">tries</span> <span class="o">&lt;</span> <span class="n">max_tries</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">tries</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">rseed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># change the random seed or it will just repeat the same simulation over and over</span>
        <span class="n">rseed</span> <span class="o">+=</span> <span class="n">tries</span>
        
    <span class="n">tries</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> \
        <span class="n">last_epoch</span> <span class="o">=</span> <span class="n">train_xor_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lr_reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> \
                                     <span class="n">lr_initial</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">plot_interval</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tries</span><span class="o">=</span><span class="n">tries</span><span class="p">,</span> <span class="n">rseed</span><span class="o">=</span><span class="n">rseed</span><span class="p">,</span> <span class="n">stop_criterion</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">last_epoch</span> <span class="o">==</span> <span class="p">(</span><span class="n">epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tries</span> <span class="o">&lt;</span> <span class="n">max_tries</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;### Try </span><span class="si">{</span><span class="n">tries</span><span class="si">}</span><span class="s1">: Failed to meet convergence criteria by epoch </span><span class="si">{</span><span class="n">last_epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">. RESTARTING.&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;### Try </span><span class="si">{</span><span class="n">tries</span><span class="si">}</span><span class="s1">: Failed to meet convergence criteria by epoch </span><span class="si">{</span><span class="n">last_epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">. GIVING UP.&#39;</span><span class="p">)</span>
        <span class="c1"># Test predictions</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)):</span>
            <span class="n">hidden_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_hidden</span>
            <span class="n">hidden_output</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">hidden_input</span><span class="p">)</span>
            <span class="n">final_input</span> <span class="o">=</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_output</span>
            <span class="n">final_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">final_input</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;# input: </span><span class="si">{</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, predicted output: </span><span class="si">{</span><span class="n">final_output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;### Try </span><span class="si">{</span><span class="n">tries</span><span class="si">}</span><span class="s1">: Met convergence criteria at epoch </span><span class="si">{</span><span class="n">last_epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
        
<span class="c1"># # Test predictions</span>
<span class="c1"># hidden_states = []</span>
<span class="c1"># for i in range(len(inputs)):</span>
<span class="c1">#     hidden_input = inputs[i].dot(weights_input_hidden) + bias_hidden</span>
<span class="c1">#     hidden_output = tanh(hidden_input)</span>
<span class="c1">#     hidden_states.append(hidden_output)</span>
<span class="c1">#     final_input = hidden_output.dot(weights_hidden_output.flatten()) + bias_output</span>
<span class="c1">#     final_output = sigmoid(final_input)</span>
<span class="c1">#     # print(f&quot;Input: {inputs[i]}, Predicted Output: {final_output[0][0]:.4f}&quot;)</span>
<span class="c1">#     print(f&quot;Input: {inputs[i]}, Hidden States: {hidden_output}, Predicted Output: {final_output[0][0]:.4f}&quot;)</span>
<span class="c1"># # Convert hidden_states to a 2x4 matrix</span>
<span class="c1"># # Convert hidden_states to a 4x2 matrix</span>
<span class="c1"># hidden_states_matrix = np.array(hidden_states)</span>

<span class="c1"># Test predictions</span>
<span class="n">hidden_states</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">hid_inputs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)):</span>
    <span class="n">hidden_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_hidden</span>
    <span class="n">hid_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hidden_input</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
    <span class="n">hidden_output</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">hidden_input</span><span class="p">)</span>
    <span class="c1"># Flatten the hidden_output to a 1D array and append</span>
    <span class="n">hidden_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">hidden_output</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
    <span class="n">final_input</span> <span class="o">=</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias_output</span>
    <span class="n">final_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">final_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input: </span><span class="si">{</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, Hidden States: </span><span class="si">{</span><span class="n">hidden_output</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="si">}</span><span class="s2">, Predicted Output: </span><span class="si">{</span><span class="n">final_output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Convert hidden_states to a 4x2 matrix</span>
<span class="n">hidden_states_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="n">hidden_inputs_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hid_inputs</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">hidden_states_matrix</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d8baeb28738daeb2c1f40151962556ff7dafcdfcf9086410d268face9e8ba077.png" src="_images/d8baeb28738daeb2c1f40151962556ff7dafcdfcf9086410d268face9e8ba077.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>### Try 1: Met convergence criteria at epoch 1889.
Input: [0 0], Hidden States: [-0.97185181  0.69576136], Predicted Output: 0.1888
Input: [0 1], Hidden States: [-0.53864131 -0.66093599], Predicted Output: 0.8334
Input: [1 0], Hidden States: [-0.53765805 -0.66191962], Predicted Output: 0.8334
Input: [1 1], Hidden States: [ 0.72661923 -0.9852091 ], Predicted Output: 0.1998
[[-0.97185181  0.69576136]
 [-0.53864131 -0.66093599]
 [-0.53765805 -0.66191962]
 [ 0.72661923 -0.9852091 ]]
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="lab-questions-part-1">
<h2><font color='red'>Lab questions, part 1</font><a class="headerlink" href="#lab-questions-part-1" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Run the cell above multiple times, and make notes about what you observe in the top left panel (loss) and the top right panel (decision boundary). Keep running the block until you get at least 5 or 6 cases where the model meets the convergence criterion before epoch 8000. (You can of course go deeper and also make observations about patterns in the weight matrices, but this is not required for this question.) Describe the decision boundary plot when the model succeeds.</p></li>
<li><p>What systematicities do you observe when the model cconverges in loss and decision boundary plots (and weight matrices, if you like, but again, not required)?</p></li>
<li><p>If the model has tended to succeed on all of your 5-6 tries, run a few more until you can observe at least 3 cases where the model fails to converge (if you do more, of course, you will be more likely to detect systematicities). What systematicities (if any) do you observe when the model fails to converge in the loss plot and decision boundary plot (again, you can also address weights, but this is not required)?</p></li>
</ol>
<p><strong>Challenge question (required for grad students and honors students, optional for others)</strong></p>
<ol class="arabic simple" start="4">
<li><p>Change <code class="docutils literal notranslate"><span class="pre">epochs</span></code> to a really large number (start with 50000). Run until you get a case where the model does not converge by 8000 epochs. Does it ever converge if it has not round a solution by 8000 epochs? (On my mac, the first try does not converge if I set rseed to 1.) Do the weights and how they change give you any insights into why (or why not) the model can get stuck? <em>Now try it again, but set epochs to 300000; what do you observe?</em></p></li>
</ol>
</section>
<hr class="docutils" />
<section id="plotting-the-network">
<h2>Plotting the network<a class="headerlink" href="#plotting-the-network" title="Link to this heading">#</a></h2>
<p>When you execute the next block, you will get a plot of the network. (I was having trouble doing this with the <code class="docutils literal notranslate"><span class="pre">network_ploter.py</span></code> file, which is not general enough, so I finally gave up and just made the plot directly here with a slightly sleeker / simpler format.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot_network_architecture_with_all_weights_and_biases</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> 
                                                          <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">,</span>
                                                         <span class="n">fsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    
    <span class="c1"># Normalize weights for alpha values</span>
    <span class="n">fc1_weights_normalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">))</span>
    <span class="n">fc2_weights_normalized</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">weights_hidden_output</span><span class="p">))</span>

    <span class="c1"># Neuron positions with aligned bias nodes</span>
    <span class="n">input_neurons</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">hidden_neurons</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">output_neurons</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
    <span class="n">bias_neurons</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>  <span class="c1"># Aligned bias nodes with input and hidden nodes</span>

    <span class="c1"># Plot neurons</span>
    <span class="k">for</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="n">input_neurons</span> <span class="o">+</span> <span class="n">hidden_neurons</span> <span class="o">+</span> <span class="n">output_neurons</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neuron</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">neuron</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span> <span class="k">if</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="n">input_neurons</span> <span class="k">else</span> <span class="s1">&#39;green&#39;</span> <span class="k">if</span> <span class="n">neuron</span> <span class="ow">in</span> <span class="n">hidden_neurons</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span>

    <span class="c1"># Connect input to hidden layer with weights</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">input_neuron</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_neurons</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">hidden_neuron</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_neurons</span><span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">weights_input_hidden</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">fc1_weights_normalized</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">hidden_neuron</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="n">input_neuron</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>
            <span class="n">text_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_neuron</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hidden_neuron</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_neuron</span><span class="p">))</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">*</span><span class="n">text_pos</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">weight</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fsize</span><span class="p">)</span>

    <span class="c1"># Connect hidden to output layer with weights</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hidden_neuron</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_neurons</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weights_hidden_output</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">fc2_weights_normalized</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">output_neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xytext</span><span class="o">=</span><span class="n">hidden_neuron</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">))</span>
        <span class="n">text_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hidden_neuron</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output_neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hidden_neuron</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">*</span><span class="n">text_pos</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">weight</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fsize</span><span class="p">)</span>

    <span class="c1"># Plot and connect bias nodes for hidden layer, include weights for bias to hidden neurons</span>
    <span class="n">bias_neuron</span> <span class="o">=</span> <span class="n">bias_neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">bias_neuron</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">hidden_neuron</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_neurons</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">hidden_neuron</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="n">bias_neuron</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">))</span>
        <span class="n">text_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bias_neuron</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hidden_neuron</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bias_neuron</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">*</span><span class="n">text_pos</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">bias_hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fsize</span><span class="p">)</span>

    <span class="c1"># Plot and connect bias node for output layer</span>
    <span class="n">output_bias</span> <span class="o">=</span> <span class="n">bias_neurons</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">*</span><span class="n">output_bias</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">output_neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xytext</span><span class="o">=</span><span class="n">output_bias</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">))</span>
    <span class="n">text_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output_bias</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output_neurons</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">output_bias</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">*</span><span class="n">text_pos</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">bias_output</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fsize</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;XOR network (bias nodes &amp; connections in red)&quot;</span><span class="p">)</span>

    <span class="c1"># Print the weight matrices and bias vectors</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First Layer Weights:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">weights_input_hidden</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First Layer Biases:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Second Layer Weights:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Second Layer Biases:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">fig</span>

<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
<span class="c1"># To use the function, pass your network&#39;s weights and biases</span>
<span class="n">network_plot</span> <span class="o">=</span> <span class="n">plot_network_architecture_with_all_weights_and_biases</span><span class="p">(</span><span class="n">weights_input_hidden</span><span class="p">,</span> <span class="n">bias_hidden</span><span class="p">,</span> <span class="n">weights_hidden_output</span><span class="p">,</span> <span class="n">bias_output</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First Layer Weights:
 [[ 1.52376711 -1.65525983]
 [ 1.522383   -1.6535112 ]]
First Layer Biases:
 [[-2.12462262  0.85903736]]
Second Layer Weights:
 [[-3.21159587]
 [-3.28657277]]
Second Layer Biases:
 [[-2.29216133]]
</pre></div>
</div>
<img alt="_images/1934256e62ccc03a45a3757ede4558525689bd2c9164bb91bdbf3768cc0724ee.png" src="_images/1934256e62ccc03a45a3757ede4558525689bd2c9164bb91bdbf3768cc0724ee.png" />
</div>
</div>
</section>
<hr class="docutils" />
<section id="lab-questions-part-2">
<h2><font color='red'>Lab questions, part 2</font><a class="headerlink" href="#lab-questions-part-2" title="Link to this heading">#</a></h2>
<ol class="arabic" start="5">
<li><p>Consider the weights you see in the network plot. Compare them to the weights we used when made the network that combines OR and NAND to implement XOR in the previous notebook. Note that the network is <em>possibly</em> but <em>not necessarily</em> implementing the same function. Keep in mind as well that at the hidden unit stage, it is applying the tanh activation function, and for the output node, it is applying the sigmoid activation function.</p>
<p>a. Try to work out what answer the network should give for the pairs [0,0], [0,1], [1,0], and [1,1]. Multiply those values by the input-to-hidden weights, add the bias-to-hidden weight, then approximately apply tanh. Do the same with the output node, but using sigmoid. Can you get approximately the values that the previous function reported for these weights? (I’ve now added a printout of hidden unit state values after the network trains. Think about what these imply…)</p>
<p>b. Does the network seem to implement anything like simpler logical functions (AND, OR, NAND) at the hidden level and then at the output level? If so, what functions do you think it is approximating?</p>
</li>
</ol>
<p><strong>Challenge questions (required for grad students and honors students, optional for others) – if these are required for you, do at least one.</strong></p>
<ol class="arabic simple" start="6">
<li><p>In the network training cell above, I had started trying to create an adaptive learning rate. I wanted the learning rate to start where the user specified it, and then decrease over training. What we really want is a kind of reverse sigmoid function, where the change will be very slow for a while and then change rapidly, and then slow down again and plateau at 10% of the original learning rate. Another way to go might be to do something that would be kind of like a reverse ReLu: keep the learning rate high for the first half of epochs and the decrease linearly down to 10% of the orignal value by the last epoch. I ran out of time and did not get this work. <strong>Programming challenge</strong>: Can you get an adaptive learning rate going? Look for this line as the place to start making changes. <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">lr_reduce</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None:</span></code>. If you can get one implemented, how does the network behave subsequently? Is it less likely to get stuck?</p></li>
<li><p><strong>Programming challenge</strong>: The network plotting code above uses a lot of ‘hard-coded’ values. The numbers of nodes in each layer are stipulated. Can you modify the function to instead determine the number of nodes in each part of the network based on the shapes of the weight matrices?</p></li>
<li><p><strong>Parameter exploration</strong>. See what happens if you change the number of hidden nodes in the network. Just change the line <code class="docutils literal notranslate"><span class="pre">num_hidden</span> <span class="pre">=</span> <span class="pre">2</span></code> to something like <code class="docutils literal notranslate"><span class="pre">num_hidden</span> <span class="pre">=</span> <span class="pre">4</span></code>. Train the network a few times. What differences do you observe? Note that you will not be able to plot the resulting network unless you do #7.</p></li>
</ol>
<hr class="docutils" />
</section>
<hr class="docutils" />
<section id="late-addition-that-will-help-with-lab-questions-visualizing-the-transformations">
<h2><font color='purple'>Late addition that will help with lab questions: visualizing the transformations</font><a class="headerlink" href="#late-addition-that-will-help-with-lab-questions-visualizing-the-transformations" title="Link to this heading">#</a></h2>
<p>Run the cell below. It makes 3 plots.</p>
<p>Left: the XOR problem in the original input space. X = False, circle = True.</p>
<p>Middle: what raw inputs to the hidden units look like.</p>
<p>Right: tanh-transformed values.</p>
<p>This shows how the network translates the XOR problem from a non-linearly separable form to a linearly separable one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">plot_hidden_states_and_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">hidden_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">ptitle</span><span class="o">=</span><span class="s1">&#39;Hidden Unit States&#39;</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots the input values, raw hidden inputs, and the hidden unit states for given inputs and outputs.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    inputs (np.array): Array of input points.</span>
<span class="sd">    hidden_states (np.array): Array of hidden states corresponding to each input point.</span>
<span class="sd">    hidden_inputs (np.array): Array of raw hidden inputs corresponding to each input point.</span>
<span class="sd">    outputs (np.array): Array of output values corresponding to each input point.</span>
<span class="sd">    jitter (float): Amount of jitter to apply to the points.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>  <span class="c1"># Three panels side by side</span>

    <span class="c1"># Plotting the input values</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">point</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span> <span class="k">if</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span> <span class="k">if</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;x&#39;</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">point</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Input Values&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Input 1&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Input 2&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">)</span>

    <span class="c1"># Plotting the raw hidden inputs</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">point</span><span class="p">,</span> <span class="n">hidden_input</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span> <span class="k">if</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span> <span class="k">if</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;x&#39;</span>
        <span class="n">jittered_state</span> <span class="o">=</span> <span class="n">hidden_input</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">jitter</span><span class="p">,</span> <span class="n">hidden_input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">jittered_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">jittered_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Raw Hidden Inputs&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Hidden Node 1 Input&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Hidden Node 2 Input&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">)</span>

    <span class="c1"># Plotting the activated hidden states</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">point</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span> <span class="k">if</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;red&#39;</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span> <span class="k">if</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">&#39;x&#39;</span>
        <span class="n">jittered_state</span> <span class="o">=</span> <span class="n">hidden_state</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">jitter</span><span class="p">,</span> <span class="n">hidden_state</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">jittered_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">jittered_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">ptitle</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Hidden Node 1 Activation&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Hidden Node 2 Activation&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">)</span>

    
    <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span>  <span class="c1"># Default font size for all text</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Example usage</span>
<span class="n">plot_hidden_states_and_inputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_states_matrix</span><span class="p">,</span> <span class="n">hidden_inputs_matrix</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">ptitle</span><span class="o">=</span><span class="s1">&#39;Hidden Unit States for XOR&#39;</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="mf">0.009</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fdaba5b07a9338d852e210a1dfb0f9e51922f83aac5b4fe91c0623cb2bbc9a8a.png" src="_images/fdaba5b07a9338d852e210a1dfb0f9e51922f83aac5b4fe91c0623cb2bbc9a8a.png" />
</div>
</div>
</section>
<section id="lab-questions-part-3">
<h2><font color='red'>Lab questions, part 3</font><a class="headerlink" href="#lab-questions-part-3" title="Link to this heading">#</a></h2>
<ol class="arabic simple" start="9">
<li><p>Train a few models until you get fairly different final weights. Paste in the network plots showing the weights. Compare the outcomes in terms of the Input vs. Raw hidden vs. (tanh) Hidden states plot directly above. Describe what you observe, and paste in screenshots of the state plots above for the 2 networks.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="12_nnets-02-training-linear-networks.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Neural networks, Part 2 – training networks</p>
      </div>
    </a>
    <a class="right-next"
       href="14_nnets-04-srn-statlearn.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Neural networks, Part 4</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Neural networks, Part 3 – training networks for non-linearly separable mappings</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-and-backpropagation">Gradient Descent and Backpropagation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#concept">Concept</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#error-surface-in-machine-learning">Error Surface in Machine Learning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics">Characteristics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#importance-in-neural-networks">Importance in neural networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics">Mathematics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-redux">Learning rate redux</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Concept</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Mathematics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-derivatives">Understanding Derivatives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-idea">Basic Idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-representation">Mathematical Representation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-and-applications">Interpretation and Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backprop-update-rule">Backprop update rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives-of-sigmoid-and-tanh-functions">Derivatives of Sigmoid and Tanh Functions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-the-sigmoid-function">Derivative of the sigmoid function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-the-tanh-function">Derivative of the tanh function</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-backpropagation-in-python">Implementing Backpropagation in Python</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-forward-pass">Step 1: Forward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-compute-loss-error">Step 2: Compute loss (error)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-backward-pass">Step 3: Backward pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-update-weights">Step 4: Update Weights</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-code">Example code</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-aside-random-seeds-in-programming">An aside – random seeds in programming</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-random-seeds">Why use random seeds?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-set-a-random-seed-in-python">How to set a random seed in Python</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-issues">Other issues</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-finally-train-a-network-for-xor">Let’s (finally!) train a network for XOR</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-train-the-network">Let’s train the network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-questions-part-1"><font color="red">Lab questions, part 1</font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plotting-the-network">Plotting the network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-questions-part-2"><font color="red">Lab questions, part 2</font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#late-addition-that-will-help-with-lab-questions-visualizing-the-transformations"><font color="purple">Late addition that will help with lab questions: visualizing the transformations</font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lab-questions-part-3"><font color="red">Lab questions, part 3</font></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By James Magnuson
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>